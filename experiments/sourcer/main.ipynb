{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea038603",
   "metadata": {},
   "source": [
    "# Script to analyze GitHub trend data and upload results to Dynamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d4a51efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… VC Sourcing Pipeline initialized successfully\n",
      "ðŸ”‘ API clients configured for OpenAI, Exa, and GitHub\n",
      "ðŸ“Š Ready to analyze DevOps repositories across 4 time periods\n"
     ]
    }
   ],
   "source": [
    "# VC Sourcing Pipeline - Complete Setup and Configuration\n",
    "import aiohttp  # type: ignore\n",
    "import asyncio\n",
    "import os\n",
    "import math\n",
    "from typing import Dict, List, Optional, TypeGuard\n",
    "from dotenv import load_dotenv  # type: ignore\n",
    "from openai import AsyncOpenAI, ChatCompletion  # type: ignore\n",
    "from pydantic import BaseModel, Field  # type: ignore\n",
    "from exa_py import Exa  # type: ignore\n",
    "from github import Github, UnknownObjectException  # type: ignore\n",
    "from github.Repository import Repository  # type: ignore\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API Configuration\n",
    "BASE_OSS_INSIGHT_URL = \"https://api.ossinsight.io/v1\"\n",
    "TRENDING_REPOS_URL = f\"{BASE_OSS_INSIGHT_URL}/trends/repos\"\n",
    "COLLECTIONS_URL = f\"{BASE_OSS_INSIGHT_URL}/collections\"\n",
    "PERIOD_VALUES = [\"past_24_hours\", \"past_week\", \"past_month\", \"past_3_months\"]\n",
    "\n",
    "# Initialize API Clients\n",
    "openai_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_PROJECT_KEY\"))\n",
    "exa_client = Exa(api_key=os.getenv(\"EXA_API_KEY\"))\n",
    "github_client = Github(os.getenv(\"GITHUB_TOKEN\"))\n",
    "\n",
    "# LLM Model Configuration\n",
    "REASONING_MODELS = [\n",
    "    \"o1\", \"o1-2024-12-17\", \"o3-mini-2025-01-31\", \"o3-mini\",\n",
    "    \"o4-mini\", \"o4-mini-2025-04-16\", \"o3\", \"o3-2025-04-16\"\n",
    "]\n",
    "\n",
    "LLM_MODEL_COSTS = {\n",
    "    \"o3\": {\"input\": 2 / 10 ** 6, \"cached_input\": 0.6 / 10 ** 6, \"output\": 8 / 10 ** 6},\n",
    "    \"o3-2025-04-16\": {\"input\": 10.0 / 10 ** 6, \"cached_input\": 2.5 / 10 ** 6, \"output\": 40.0 / 10 ** 6},\n",
    "    \"o4-mini\": {\"input\": 1.1 / 10 ** 6, \"cached_input\": 0.275 / 10 ** 6, \"output\": 4.4 / 10 ** 6},\n",
    "    \"o4-mini-2025-04-16\": {\"input\": 1.1 / 10 ** 6, \"cached_input\": 0.275 / 10 ** 6, \"output\": 4.4 / 10 ** 6},\n",
    "    \"gpt-4.1-mini\": {\"input\": 0.40 / 10 ** 6, \"cached_input\": 0.10 / 10 ** 6, \"output\": 1.60 / 10 ** 6},\n",
    "    \"gpt-4.1\": {\"input\": 2.00 / 10 ** 6, \"cached_input\": 0.50 / 10 ** 6, \"output\": 8.00 / 10 ** 6},\n",
    "    \"gpt-4.1-nano\": {\"input\": 0.10 / 10 ** 6, \"cached_input\": 0.025 / 10 ** 6, \"output\": 0.40 / 10 ** 6},\n",
    "    \"gpt-4o\": {\"input\": 2.5 / 10 ** 6, \"cached_input\": 1.25 / 10 ** 6, \"output\": 10 / 10 ** 6},\n",
    "    \"gpt-4o-2024-11-20\": {\"input\": 2.5 / 10 ** 6, \"cached_input\": 1.25 / 10 ** 6, \"output\": 10 / 10 ** 6},\n",
    "    \"gpt-4o-2024-08-06\": {\"input\": 2.5 / 10 ** 6, \"cached_input\": 1.25 / 10 ** 6, \"output\": 10 / 10 ** 6},\n",
    "    \"gpt-4o-mini\": {\"input\": 0.15 / 10 ** 6, \"cached_input\": 0.075 / 10 ** 6, \"output\": 0.6 / 10 ** 6},\n",
    "    \"text-embedding-3-small\": {\"input\": 0.02 / 10 ** 6},\n",
    "    \"text-embedding-3-large\": {\"input\": 0.13 / 10 ** 6},\n",
    "    \"text-embedding-ada-002\": {\"input\": 0.1 / 10 ** 6}\n",
    "}\n",
    "\n",
    "# Pydantic Models for VC Analysis\n",
    "class Collection(BaseModel):\n",
    "    id: str\n",
    "    name: str\n",
    "\n",
    "class DevOpsCollectionsResponse(BaseModel):\n",
    "    collections: List[Collection]\n",
    "\n",
    "class DevOpsClassification(BaseModel):\n",
    "    is_devops_related: bool  # True if this repository is DevOps/infrastructure related\n",
    "    confidence_score: int  # Score from 1-5, how confident the classification is\n",
    "    reasoning: str  # Brief explanation of why it's classified as DevOps or not\n",
    "    devops_categories: List[str]  # Categories like \"CI/CD\", \"Container\", \"Monitoring\", etc.\n",
    "\n",
    "class BigTechClassification(BaseModel):\n",
    "    is_big_tech: bool = Field(\n",
    "        ...,\n",
    "        description=\"True if this repository appears to be from a large/established company unsuitable for early-stage VC investment\"\n",
    "    )\n",
    "    confidence_score: int = Field(\n",
    "        ...,\n",
    "        description=\"Confidence level (1-5, where 5 is very confident)\",\n",
    "        ge=1, le=5\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"Brief explanation of the classification decision\"\n",
    "    )\n",
    "    company_indicators: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"List of indicators found (e.g., 'Enterprise features', 'Large team size', 'Corporate backing')\"\n",
    "    )\n",
    "\n",
    "class RepoRubricAnalysis(BaseModel):\n",
    "    \"\"\"A Pydantic model for the structured output of the README rubric analysis.\"\"\"\n",
    "    problem_clarity_score: int = Field(\n",
    "        ...,\n",
    "        description=\"Score (1-5) for how clearly the project states the problem it solves.\",\n",
    "        ge=1,\n",
    "        le=5\n",
    "    )\n",
    "    adoption_ease_score: int = Field(\n",
    "        ...,\n",
    "        description=\"Score (1-5) for how easy it is to get started based on the README.\",\n",
    "        ge=1,\n",
    "        le=5\n",
    "    )\n",
    "    maturity_health_score: int = Field(\n",
    "        ...,\n",
    "        description=\"Score (1-5) for project maturity as inferred from README links and content.\",\n",
    "        ge=1,\n",
    "        le=5\n",
    "    )\n",
    "    problem_solved: str = Field(\n",
    "        ...,\n",
    "        description=\"In one sentence, what specific, frustrating problem does this project solve?\"\n",
    "    )\n",
    "\n",
    "class CommunitySentimentAnalysis(BaseModel):\n",
    "    \"\"\"Structured output for community sentiment analysis.\"\"\"\n",
    "    excitement_score: int = Field(\n",
    "        ...,\n",
    "        description=\"Score (1-5) for the community's emotional energy and hype.\",\n",
    "        ge=1, le=5\n",
    "    )\n",
    "    problem_solution_fit_score: int = Field(\n",
    "        ...,\n",
    "        description=\"Score (1-5) for how well the project's value proposition resonates with the community.\",\n",
    "        ge=1, le=5\n",
    "    )\n",
    "    credibility_adoption_score: int = Field(\n",
    "        ...,\n",
    "        description=\"Score (1-5) based on the authority of commenters and signs of real-world adoption.\",\n",
    "        ge=1, le=5\n",
    "    )\n",
    "    key_praise_quote: str = Field(\n",
    "        ...,\n",
    "        description=\"A single, direct quote that best captures the community's core praise.\"\n",
    "    )\n",
    "    main_criticism: str = Field(\n",
    "        ...,\n",
    "        description=\"A one-sentence summary of the most common or significant criticism.\"\n",
    "    )\n",
    "\n",
    "class FinalAnalysis(BaseModel):\n",
    "    repo_name: str\n",
    "    oss_insight_data: dict\n",
    "    repo_rubric_analysis: Optional[RepoRubricAnalysis] = None\n",
    "    community_sentiment_analysis: Optional[CommunitySentimentAnalysis] = None\n",
    "    score: Optional[float] = None  # Calculated final score\n",
    "\n",
    "print(\"âœ… VC Sourcing Pipeline initialized successfully\")\n",
    "print(f\"ðŸ”‘ API clients configured for OpenAI, Exa, and GitHub\")\n",
    "print(f\"ðŸ“Š Ready to analyze DevOps repositories across {len(PERIOD_VALUES)} time periods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e5e1fdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost Tracking for LLM Usage\n",
    "class ModelUsageAsync:\n",
    "    \"\"\"Track LLM API costs throughout the VC analysis pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_costs=LLM_MODEL_COSTS, web_search_costs=None):\n",
    "        self.token_usage = {}\n",
    "        self.web_search_usage = {}\n",
    "        self.model_costs = model_costs\n",
    "        self.web_search_costs = web_search_costs\n",
    "        self.logs = []\n",
    "        self.lock = asyncio.Lock()\n",
    "\n",
    "    async def add_tokens(\n",
    "        self, \n",
    "        model: str, \n",
    "        input_tokens: int = 0, \n",
    "        output_tokens: int = 0, \n",
    "        cached_tokens: int = 0, \n",
    "        reasoning_tokens: int = 0, \n",
    "        label: Optional[str] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Add token usage for cost tracking.\"\"\"\n",
    "        async with self.lock:\n",
    "            if model not in self.token_usage:\n",
    "                self.token_usage[model] = {\"input\": 0, \"output\": 0, \"cached\": 0, \"reasoning\": 0}\n",
    "            \n",
    "            self.token_usage[model][\"input\"] += input_tokens\n",
    "            self.token_usage[model][\"output\"] += output_tokens\n",
    "            self.token_usage[model][\"cached\"] += cached_tokens\n",
    "            self.token_usage[model][\"reasoning\"] += reasoning_tokens\n",
    "            \n",
    "            if label is not None:\n",
    "                self.logs.append({\n",
    "                    \"model\": model, \"input_tokens\": input_tokens, \"output_tokens\": output_tokens,\n",
    "                    \"cached_tokens\": cached_tokens, \"reasoning_tokens\": reasoning_tokens, \"label\": label\n",
    "                })\n",
    "\n",
    "    async def add_web_search_usage(self, model: str, search_context_size: str) -> None:\n",
    "        \"\"\"Add web search usage for cost tracking.\"\"\"\n",
    "        if self.web_search_costs is not None:\n",
    "            if model not in self.web_search_usage:\n",
    "                self.web_search_usage[model] = {}\n",
    "            if search_context_size not in self.web_search_usage[model]:\n",
    "                self.web_search_usage[model][search_context_size] = 0\n",
    "            self.web_search_usage[model][search_context_size] += 1\n",
    "        \n",
    "    async def get_cost(self) -> float:\n",
    "        \"\"\"Calculate total cost of LLM usage.\"\"\"\n",
    "        async with self.lock:\n",
    "            for model in self.token_usage:\n",
    "                if model not in self.model_costs:\n",
    "                    raise ValueError(f\"Model {model} is not supported.\")\n",
    "            \n",
    "            cost_of_input_output = sum([\n",
    "                (self.token_usage[model][\"input\"] - self.token_usage[model][\"cached\"]) * self.model_costs[model].get(\"input\", 0) +\n",
    "                self.token_usage[model][\"output\"] * self.model_costs[model].get(\"output\", 0) +\n",
    "                self.token_usage[model][\"cached\"] * self.model_costs[model].get(\"cached_input\", 0)\n",
    "                for model in self.token_usage \n",
    "            ]) \n",
    "\n",
    "            cost_of_web_searches = 0\n",
    "            if self.web_search_costs is not None:\n",
    "                for model in self.web_search_usage:\n",
    "                    for search_context_size in self.web_search_usage[model]:\n",
    "                        cost_of_web_searches += self.web_search_usage[model][search_context_size] * self.web_search_costs[model].get(search_context_size, 0)\n",
    "\n",
    "            return cost_of_input_output + cost_of_web_searches\n",
    "\n",
    "    async def get_tokens_used(self) -> int:\n",
    "        \"\"\"Get total number of tokens used.\"\"\"\n",
    "        async with self.lock:\n",
    "            return sum([\n",
    "                self.token_usage[model][\"input\"] + self.token_usage[model][\"output\"]\n",
    "                for model in self.token_usage\n",
    "            ])\n",
    "\n",
    "    async def get_web_searches_performed(self) -> int:\n",
    "        \"\"\"Get total number of web searches performed.\"\"\"\n",
    "        async with self.lock:\n",
    "            total_searches = 0\n",
    "            for model in self.web_search_usage:\n",
    "                for search_context_size in self.web_search_usage[model]:\n",
    "                    total_searches += self.web_search_usage[model][search_context_size]\n",
    "            return total_searches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0b485e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API Helper with Structured Output\n",
    "async def call_openai_structured(\n",
    "    openai_client: AsyncOpenAI,\n",
    "    model: str,\n",
    "    llm_usage: Optional[ModelUsageAsync] = None,\n",
    "    llm_usage_label: Optional[str] = None,\n",
    "    max_retries: int = 3,\n",
    "    base_delay: int = 9,\n",
    "    completion_timeout: int = 120,\n",
    "    web_search_bool: bool = False,\n",
    "    web_search_context_size: str = \"high\",\n",
    "    **kwargs\n",
    ") -> Optional[ChatCompletion]:\n",
    "    \"\"\"Call OpenAI API with structured output and cost tracking.\"\"\"\n",
    "    \n",
    "    # Handle model-specific parameters\n",
    "    if model in REASONING_MODELS:\n",
    "        kwargs.pop(\"temperature\", None)  # Reasoning models don't support temperature\n",
    "    else:\n",
    "        kwargs.pop(\"reasoning_effort\", None)  # Only supported by reasoning models\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            completion = await asyncio.wait_for(\n",
    "                openai_client.chat.completions.parse(model=model, **kwargs),\n",
    "                timeout=completion_timeout\n",
    "            )\n",
    "            \n",
    "            # Track usage and costs\n",
    "            if llm_usage is not None:\n",
    "                await llm_usage.add_tokens(\n",
    "                    model=model,\n",
    "                    input_tokens=completion.usage.prompt_tokens,\n",
    "                    output_tokens=completion.usage.completion_tokens,\n",
    "                    cached_tokens=completion.usage.prompt_tokens_details.cached_tokens,\n",
    "                    reasoning_tokens=completion.usage.completion_tokens_details.reasoning_tokens,\n",
    "                    label=llm_usage_label\n",
    "                )\n",
    "                \n",
    "                if web_search_bool:\n",
    "                    await llm_usage.add_web_search_usage(model, web_search_context_size)\n",
    "            \n",
    "            return completion\n",
    "            \n",
    "        except asyncio.TimeoutError:\n",
    "            print(f\"ðŸ”„ API timeout for {llm_usage_label or 'request'}, retrying...\")\n",
    "            await asyncio.sleep(base_delay)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ API error for {llm_usage_label or 'request'}: {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                break\n",
    "            await asyncio.sleep(base_delay)\n",
    "\n",
    "    print(f\"âŒ Max retries exceeded for {llm_usage_label or 'request'}\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e8d532a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OSS Insight API Helper\n",
    "async def make_oss_insight_request(\n",
    "    session: aiohttp.ClientSession,\n",
    "    url: str,\n",
    "    params: dict\n",
    ") -> Optional[dict]:\n",
    "    \"\"\"Make requests to OSS Insight API with error handling.\"\"\"\n",
    "    try:\n",
    "        async with session.get(url, params=params) as response:\n",
    "            response.raise_for_status()\n",
    "            data = await response.json()\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ OSS Insight API error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c9028c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DevOps Collections Identification\n",
    "GET_DEV_OPS_COLLECTIONS_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that identifies DevOps-related collections from a list.\n",
    "Use a broad, inclusive definition of DevOps: include any tools that developers might use for infrastructure, CI/CD, deployment, monitoring, automation, observability, or general backend workflows.\n",
    "It's better to include a collection that might not be strictly DevOps than to exclude something useful to developers.\n",
    "\"\"\"\n",
    "\n",
    "GET_DEV_OPS_COLLECTIONS_USER_PROMPT = \"\"\"\n",
    "Task: Identify DevOps-related collections from the list below.\n",
    "Use a broad definition of DevOps. Include tools for:\n",
    "- infrastructure, CI/CD, deployment, monitoring, observability, automation\n",
    "- cloud services, containerization, developer productivity (especially backend)\n",
    "- workflow tooling, build systems, scripting, package management\n",
    "\n",
    "Exclude only obviously non-relevant tools like:\n",
    "- frontend-only UI libraries (e.g., Tailwind, React)\n",
    "- general note-taking or static content tools\n",
    "\n",
    "List of collections:\n",
    "{context}\n",
    "\n",
    "Your answer must:\n",
    "1. Only contain collections from the provided list\n",
    "2. Return collections in the specified format with both id and name fields\n",
    "3. Be inclusive of any tools that developers would use for infrastructure, automation, or deployment\n",
    "\"\"\"\n",
    "\n",
    "async def get_dev_ops_collections(\n",
    "    openai_client: AsyncOpenAI,\n",
    "    collections_data: dict,\n",
    "    llm_usage: ModelUsageAsync\n",
    ") -> Optional[DevOpsCollectionsResponse]:\n",
    "    \"\"\"Identify DevOps collections using LLM analysis.\"\"\"\n",
    "    collections_rows = collections_data.get(\"data\", {}).get(\"rows\", [])\n",
    "    all_collections = [{row[\"id\"]: row[\"name\"]} for row in collections_rows]\n",
    "    \n",
    "    print(f\"ðŸ” Analyzing {len(all_collections)} collections to identify DevOps-related ones...\")\n",
    "    \n",
    "    context = \"\\n\".join(f\"{row['id']}: {row['name']}\" for row in collections_rows)\n",
    "    messages = [\n",
    "        {\"role\": \"developer\", \"content\": GET_DEV_OPS_COLLECTIONS_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": GET_DEV_OPS_COLLECTIONS_USER_PROMPT.format(context=context)}\n",
    "    ]\n",
    "\n",
    "    response = await call_openai_structured(\n",
    "        openai_client=openai_client,\n",
    "        model=\"o3\",\n",
    "        messages=messages,\n",
    "        response_format=DevOpsCollectionsResponse,\n",
    "        llm_usage=llm_usage,\n",
    "        llm_usage_label=\"dev_ops_collections_identification\",\n",
    "        reasoning_effort=\"high\"\n",
    "    )\n",
    "\n",
    "    if not response or not response.choices[0].message.parsed:\n",
    "        print(\"âŒ Failed to identify DevOps collections\")\n",
    "        return None\n",
    "    \n",
    "    parsed_response = response.choices[0].message.parsed\n",
    "\n",
    "    print(f\"âœ… Identified {len(parsed_response.collections)} DevOps collections\")\n",
    "    return parsed_response.collections\n",
    "\n",
    "# Testing get_dev_ops_collections\n",
    "# print(\"Testing get_dev_ops_collections\")\n",
    "# async with aiohttp.ClientSession() as session:\n",
    "#     collections_data = await make_oss_insight_request(\n",
    "#         session=session,\n",
    "#         url=COLLECTIONS_URL,\n",
    "#         params={}\n",
    "#     )\n",
    "\n",
    "# if collections_data:\n",
    "#     test_usage = ModelUsageAsync()\n",
    "#     dev_ops_collections = await get_dev_ops_collections(openai_client, collections_data, test_usage)\n",
    "#     if dev_ops_collections:\n",
    "#         dev_ops_collections = [{c.id: c.name} for c in dev_ops_collections]\n",
    "#         print(dev_ops_collections)\n",
    "\n",
    "#     # Display cost information\n",
    "#     total_cost = await test_usage.get_cost()\n",
    "#     total_tokens = await test_usage.get_tokens_used()\n",
    "#     print(f\"\\nðŸ’¸ Test Analysis Cost: ${total_cost:.4f}\")\n",
    "#     print(f\"ðŸ”¢ Test Total Tokens Used: {total_tokens:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b35fd5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM-based DevOps classification function loaded\n"
     ]
    }
   ],
   "source": [
    "# LLM-Based DevOps Repository Classification\n",
    "CLASSIFY_DEVOPS_REPO_SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert VC analyst specializing in DevOps and infrastructure tooling. Your task is to classify whether a GitHub repository is related to DevOps, infrastructure, or developer productivity tools.\n",
    "Be loose with the definition of DevOps.\n",
    "It's better to include a repository that might not be strictly DevOps than to exclude something useful to developers.\n",
    "Also include markup frameworks, documentation tools, and other tools that developers could use in their daily work.\n",
    "\"\"\"\n",
    "\n",
    "CLASSIFY_DEVOPS_REPO_USER_PROMPT = \"\"\"\n",
    "Repository: {repo_name}\n",
    "Description: {description}\n",
    "\n",
    "Based on the repository name and description, determine if this is a DevOps-related project.\n",
    "\n",
    "DevOps categories include:\n",
    "- Any tools that developers could use in their daily work; be loose with the definition of DevOps\n",
    "- CI/CD and deployment pipelines\n",
    "- Container technologies (Docker, Kubernetes, etc.)\n",
    "- Infrastructure as Code (Terraform, Ansible, etc.)\n",
    "- Monitoring and observability tools\n",
    "- Cloud platforms and services\n",
    "- Developer productivity tools\n",
    "- Security and compliance tools\n",
    "- Database and data infrastructure\n",
    "- API gateways and service mesh\n",
    "- Configuration management\n",
    "- Build and package management tools\n",
    "- Tools for building or deploying AI/ML models\n",
    "\n",
    "Analyze this repository and provide:\n",
    "1. **is_devops_related**: True if it's DevOps/infrastructure related\n",
    "2. **confidence_score**: Your confidence (1-5, where 5 is very confident)\n",
    "3. **reasoning**: Brief explanation of your classification\n",
    "4. **devops_categories**: List of relevant DevOps categories (empty list if not DevOps)\n",
    "\"\"\"\n",
    "\n",
    "async def classify_repository_with_llm(\n",
    "    openai_client: AsyncOpenAI,\n",
    "    repo_name: str,\n",
    "    description: str,\n",
    "    llm_usage: ModelUsageAsync\n",
    ") -> Optional[DevOpsClassification]:\n",
    "    \"\"\"Classify a repository as DevOps-related using LLM analysis.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"developer\", \"content\": CLASSIFY_DEVOPS_REPO_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": CLASSIFY_DEVOPS_REPO_USER_PROMPT.format(\n",
    "            repo_name=repo_name,\n",
    "            description=description or \"No description available\"\n",
    "        )}\n",
    "    ]\n",
    "\n",
    "    response = await call_openai_structured(\n",
    "        openai_client=openai_client,\n",
    "        model=\"o4-mini\",\n",
    "        messages=messages,\n",
    "        response_format=DevOpsClassification,\n",
    "        llm_usage=llm_usage,\n",
    "        llm_usage_label=f\"devops_classification_{repo_name}\",\n",
    "        reasoning_effort=\"medium\"\n",
    "    )\n",
    "\n",
    "    if response and response.choices:\n",
    "        return response.choices[0].message.parsed\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"âœ… LLM-based DevOps classification function loaded\")\n",
    "\n",
    "# Test classify_repository_with_llm\n",
    "# print(\"Testing classify_repository_with_llm\")\n",
    "# test_repos = [{\"name\": \"LibreSpark/LibreTV\", \"description\": \"ä¸€åˆ†é’Ÿæ­å»ºå½±è§†ç«™ï¼Œæ”¯æŒVercel/Dockerç­‰éƒ¨ç½²æ–¹å¼.\"}, {\"name\": \"srbhr/Resume-Matcher\", \"description\": \"Improve your resumes with Resume Matcher. Get insights, keyword suggestions and tune your resumes to job descriptions.\"}, {\"name\": \"maybe-finance/maybe\", \"description\": \"The personal finance app for everyone\"}, {\"name\": \"roboflow/supervision\", \"description\": \"We write your reusable computer vision tools. ðŸ’œ\"}, {\"name\": \"bluesky-social/social-app\", \"description\": \"The Bluesky Social application for Web, iOS, and Android\"}, {\"name\": \"typst/typst\", \"description\": \"A new markup-based typesetting system that is powerful and easy to learn.\"}, {\"name\": \"aidenybai/react-scan\", \"description\": \"Scan for React performance issues and eliminate slow renders in your app\"}]\n",
    "# test_usage = ModelUsageAsync()\n",
    "# for repo in test_repos:\n",
    "#     print(repo[\"name\"])\n",
    "#     print(repo[\"description\"])\n",
    "#     print(await classify_repository_with_llm(openai_client, repo[\"name\"], repo[\"description\"], test_usage))\n",
    "#     print(\"-\"*100)\n",
    "\n",
    "# # Display cost information\n",
    "# total_cost = await test_usage.get_cost()\n",
    "# total_tokens = await test_usage.get_tokens_used()\n",
    "# print(f\"\\nðŸ’¸ Test Analysis Cost: ${total_cost:.4f}\")\n",
    "# print(f\"ðŸ”¢ Test Total Tokens Used: {total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2037f193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions for VC Analysis\n",
    "def truncate_content_for_llm(content: str, max_chars: int = 15000) -> str:\n",
    "    \"\"\"Truncate content to fit within LLM context limits.\"\"\"\n",
    "    return content[:max_chars] if len(content) > max_chars else content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2ca3f467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Big tech classification function loaded\n"
     ]
    }
   ],
   "source": [
    "# Big Tech Company Classification\n",
    "BIG_TECH_CLASSIFICATION_SYSTEM_PROMPT = \"\"\"\n",
    "You are a VC analyst specializing in identifying early-stage investment opportunities. Your task is to determine if a GitHub repository belongs to a large, established company that would be unsuitable for early-stage VC investment.\n",
    "\n",
    "Focus on indicators that suggest the company is too mature/large for early-stage funding.\n",
    "\"\"\"\n",
    "\n",
    "BIG_TECH_CLASSIFICATION_USER_PROMPT = \"\"\"\n",
    "Repository: {repo_name}\n",
    "Description: {description}\n",
    "\n",
    "README Content:\n",
    "---\n",
    "{readme_content}\n",
    "---\n",
    "\n",
    "Analyze this repository to determine if it belongs to a large, established company unsuitable for early-stage VC investment.\n",
    "\n",
    "**EXCLUDE (mark as big tech) if you find:**\n",
    "- Clear corporate backing (Google, Meta, Microsoft, Amazon, Apple, Anthropic, OpenAI, etc.)\n",
    "- Enterprise-focused language (\"Enterprise\", \"at scale\", \"production-grade for large organizations\")\n",
    "- Large team indicators (many contributors, corporate structure mentions)\n",
    "- Established company features (SOC2 compliance, enterprise sales, dedicated support teams)\n",
    "- Corporate open-source projects from known large companies\n",
    "- References to IPO, public trading, or being part of Fortune 500 companies\n",
    "\n",
    "**INCLUDE (mark as NOT big tech) if:**\n",
    "- Startup/indie language (\"we're building\", \"early stage\", \"small team\")\n",
    "- Personal projects or small team indicators\n",
    "- Seeking funding, investors, or early adopters\n",
    "- Grassroots community-driven projects\n",
    "- Clear startup indicators (YC batch, seed funding, small team size)\n",
    "\n",
    "Provide:\n",
    "1. **is_big_tech**: True if unsuitable for early-stage VC (big tech/established company)\n",
    "2. **confidence_score**: Your confidence (1-5)\n",
    "3. **reasoning**: Brief explanation of your decision\n",
    "4. **company_indicators**: List of specific indicators found\n",
    "\"\"\"\n",
    "\n",
    "async def classify_big_tech_company(\n",
    "    openai_client: AsyncOpenAI,\n",
    "    github_client: Github,\n",
    "    repo_name: str,\n",
    "    description: str,\n",
    "    llm_usage: ModelUsageAsync\n",
    ") -> Optional[BigTechClassification]:\n",
    "    \"\"\"Classify if repository belongs to big tech company unsuitable for VC investment.\"\"\"\n",
    "    \n",
    "    # Fetch README content\n",
    "    try:\n",
    "        repo = await asyncio.to_thread(github_client.get_repo, repo_name)\n",
    "        readme_obj = await asyncio.to_thread(repo.get_readme)\n",
    "        readme_content = readme_obj.decoded_content.decode(\"utf-8\")\n",
    "        readme_content = truncate_content_for_llm(readme_content, max_chars=10000)  # Smaller limit for speed\n",
    "        print(f\"âœ… Fetched README for big tech classification: {repo_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not fetch README for {repo_name}: {e}\")\n",
    "        readme_content = \"README not available\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"developer\", \"content\": BIG_TECH_CLASSIFICATION_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": BIG_TECH_CLASSIFICATION_USER_PROMPT.format(\n",
    "            repo_name=repo_name,\n",
    "            description=description or \"No description available\",\n",
    "            readme_content=readme_content\n",
    "        )}\n",
    "    ]\n",
    "\n",
    "    response = await call_openai_structured(\n",
    "        openai_client=openai_client,\n",
    "        model=\"o4-mini\",  # Fast model for this check\n",
    "        messages=messages,\n",
    "        response_format=BigTechClassification,\n",
    "        llm_usage=llm_usage,\n",
    "        llm_usage_label=f\"big_tech_classification_{repo_name}\",\n",
    "        reasoning_effort=\"low\"  # Fast classification\n",
    "    )\n",
    "\n",
    "    if response and response.choices:\n",
    "        return response.choices[0].message.parsed\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"âœ… Big tech classification function loaded\")\n",
    "\n",
    "# Test classify_big_tech_company\n",
    "# print(\"Testing classify_big_tech_company\")\n",
    "# test_repos = [{\"name\": \"anthropics/claude-code\", \"description\": \"Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows - all through natural language commands.\"}, {\"name\": \"google-gemini/gemini-cli\", \"description\": \"An open-source AI agent that brings the power of Gemini directly into your terminal.\"}, {\"name\": \"maybe-finance/maybe\", \"description\": \"The personal finance app for everyone\"}, {\"name\": \"roboflow/supervision\", \"description\": \"We write your reusable computer vision tools. ðŸ’œ\"}, {\"name\": \"bluesky-social/social-app\", \"description\": \"The Bluesky Social application for Web, iOS, and Android\"}, {\"name\": \"typst/typst\", \"description\": \"A new markup-based typesetting system that is powerful and easy to learn.\"}, {\"name\": \"aidenybai/react-scan\", \"description\": \"Scan for React performance issues and eliminate slow renders in your app\"}, {\"name\": \"n8n-io/n8n\", \"description\": \"Fair-code workflow automation platform with native AI capabilities. Combine visual building with custom code, self-host or cloud, 400+ integrations.\"}]\n",
    "# test_usage = ModelUsageAsync()\n",
    "# for repo in test_repos:\n",
    "#     print(repo[\"name\"])\n",
    "#     print(repo[\"description\"])\n",
    "#     print(await classify_big_tech_company(openai_client, github_client, repo[\"name\"], repo[\"description\"], test_usage))\n",
    "#     print(\"-\"*100)\n",
    "\n",
    "# # Display cost information\n",
    "# total_cost = await test_usage.get_cost()\n",
    "# total_tokens = await test_usage.get_tokens_used()\n",
    "# print(f\"\\nðŸ’¸ Test Analysis Cost: ${total_cost:.4f}\")\n",
    "# print(f\"ðŸ”¢ Test Total Tokens Used: {total_tokens:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4134dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting files for analysis from GitHub\n",
    "async def _get_file_details(\n",
    "    repo: Repository,\n",
    "    file_basename: str,\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Fetches a specific file's content and its last commit date from a GitHub repo.\n",
    "\n",
    "    Args:\n",
    "        repo: The repository object.\n",
    "        file_basename: The basename of the file (e.g., 'CONTRIBUTING').\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the file's content and last modification date,\n",
    "        or None if the file doesn't exist.\n",
    "    \"\"\"\n",
    "    file_path_variants = [\n",
    "        f\"{file_basename.upper()}.md\",\n",
    "        f\"{file_basename.lower()}.md\",\n",
    "        f\".github/{file_basename.upper()}.md\",\n",
    "        f\".github/{file_basename.lower()}.md\",\n",
    "    ]\n",
    "    for path in file_path_variants:\n",
    "        try:\n",
    "            print(f\"ðŸ”Ž Checking for '{path}' in {repo.name}...\")\n",
    "            content_file = await asyncio.to_thread(repo.get_contents, path)\n",
    "            files_content = content_file.decoded_content.decode(\"utf-8\")\n",
    "\n",
    "            commits = await asyncio.to_thread(repo.get_commits, path=path)\n",
    "            last_commit_date = commits[0].commit.committer.date\n",
    "\n",
    "            print(f\"âœ… Found '{path}' in {repo.name}\")\n",
    "            return {\n",
    "                \"content\": files_content,\n",
    "                \"last_modified\": last_commit_date\n",
    "            }\n",
    "        except UnknownObjectException:\n",
    "            # This exception is raised when the file is not found (404)\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Could not fetch '{path}' from {repo.name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    print(f\"âŒ Could not fetch '{file_basename}' from {repo.name}\")\n",
    "    return None\n",
    "        \n",
    "# Repository Analysis Function\n",
    "async def analyze_repo_with_rubric(\n",
    "    openai_client: AsyncOpenAI,\n",
    "    github_client: Github,\n",
    "    repo_name: str,\n",
    "    llm_usage: ModelUsageAsync\n",
    ") -> Optional[RepoRubricAnalysis]:\n",
    "    \"\"\"Analyze repository README and other files for VC investment potential.\"\"\"\n",
    "    try:\n",
    "        # Get the repository\n",
    "        repo = await asyncio.to_thread(github_client.get_repo, repo_name)\n",
    "\n",
    "        # Get the README file details\n",
    "        readme_obj = await asyncio.to_thread(repo.get_readme)\n",
    "        readme_content = readme_obj.decoded_content.decode(\"utf-8\")\n",
    "        readme_content = truncate_content_for_llm(readme_content)\n",
    "        print(f\"âœ… Fetched README for {repo_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Could not fetch README for {repo_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Get the file details for CONTRIBUTING.md\n",
    "    contributing_details = await _get_file_details(\n",
    "        repo=repo,\n",
    "        file_basename=\"CONTRIBUTING\"\n",
    "    )\n",
    "    contributing_content = None\n",
    "    contributing_last_modified = None\n",
    "    if contributing_details:\n",
    "        contributing_content = contributing_details[\"content\"]\n",
    "        contributing_content = truncate_content_for_llm(\n",
    "            content=contributing_content, \n",
    "            max_chars=2000\n",
    "        )\n",
    "        contributing_last_modified = contributing_details[\"last_modified\"]\n",
    "\n",
    "    # Get the file details for CODE_OF_CONDUCT.md\n",
    "    code_of_conduct_details = await _get_file_details(\n",
    "        repo=repo,\n",
    "        file_basename=\"CODE_OF_CONDUCT\"\n",
    "    )\n",
    "    code_of_conduct_content = None\n",
    "    code_of_conduct_last_modified = None\n",
    "    if code_of_conduct_details:\n",
    "        code_of_conduct_content = code_of_conduct_details[\"content\"]\n",
    "        code_of_conduct_content = truncate_content_for_llm(\n",
    "            content=code_of_conduct_content, \n",
    "            max_chars=2000\n",
    "        )\n",
    "        code_of_conduct_last_modified = code_of_conduct_details[\"last_modified\"]\n",
    "\n",
    "    system_prompt = \"You are a sharp and discerning VC analyst specializing in DevOps and open-source software. Your task is to apply a specific rubric to evaluate a project's investment potential based on its GitHub presence.\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    Analyze the following repository: '{repo_name}'.\n",
    "\n",
    "    **Repository Data:**\n",
    "    - Found 'CONTRIBUTING.md': {contributing_content is not None}\n",
    "    - Found 'CODE_OF_CONDUCT.md': {code_of_conduct_content is not None}\n",
    "    - Last modified 'CONTRIBUTING.md': {contributing_last_modified.strftime('%Y-%m-%d') if contributing_last_modified else 'N/A'}\n",
    "    - Last modified 'CODE_OF_CONDUCT.md': {code_of_conduct_last_modified.strftime('%Y-%m-%d') if code_of_conduct_last_modified else 'N/A'}\n",
    "\n",
    "    **README Content:**\n",
    "    ---\n",
    "    {readme_content}\n",
    "    ---\n",
    "\n",
    "    **Contributing Content:**\n",
    "    ---\n",
    "    {contributing_content or \"No contributing file found\"}\n",
    "    ---\n",
    "\n",
    "    **Code of Conduct Content:**\n",
    "    ---\n",
    "    {code_of_conduct_content or \"No code of conduct file found\"}\n",
    "    ---\n",
    "\n",
    "    **Your Task:**\n",
    "    Evaluate the project strictly based on the rubric below and provide a score from 1 to 5 for each category. Do not provide explanations, only the scores.\n",
    "\n",
    "    **Rubric for Evaluation:**\n",
    "\n",
    "    **1. Problem Statement Clarity & Value Proposition (Score 1-5):**\n",
    "    - 1 (Poor): Unclear what problem it solves.\n",
    "    - 3 (Average): Describes *what* it does, but not *why* it's a compelling solution.\n",
    "    - 5 (Excellent): Clearly explains a frustrating 'painkiller' problem and presents itself as the obvious solution.\n",
    "\n",
    "    **2. Ease of Adoption / 'Time to First Wow' (Score 1-5):**\n",
    "    - 1 (Poor): No clear 'Getting Started' guide or examples.\n",
    "    - 3 (Average): Basic installation instructions are present but might be complex.\n",
    "    - 5 (Excellent): A clear 'Quickstart' section with copy-paste commands to get a user to a 'wow' moment in minutes.\n",
    "\n",
    "    **3. Project Maturity & Community Health (Score 1-5):**\n",
    "    - 1 (Poor): No `CONTRIBUTING.md` or links to a community. Last commit was over 3 months ago.\n",
    "    - 3 (Average): Has either a `CONTRIBUTING.md` or a community link. Last commit was within the last month.\n",
    "    - 5 (Excellent): Has a `CONTRIBUTING.md`, a `CODE_OF_CONDUCT.md`, and links to an active community. Last commit was within the last week.\n",
    "\n",
    "    Required Output:**\n",
    "\n",
    "    1.  **Problem Statement Clarity (Score 1-5):**\n",
    "    2.  **Ease of Adoption (Score 1-5):** \n",
    "    3.  **Project Maturity & Community Health (Score 1-5):\n",
    "    4.  **Problem Solved (One sentence):**\n",
    "        -   In a single sentence, describe the specific, frustrating problem this project solves.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = await call_openai_structured(\n",
    "        openai_client=openai_client,\n",
    "        model=\"o4-mini\",\n",
    "        messages=[{\"role\": \"developer\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}],\n",
    "        response_format=RepoRubricAnalysis,\n",
    "        llm_usage=llm_usage,\n",
    "        llm_usage_label=f\"readme_analysis_{repo_name}\",\n",
    "        reasoning_effort=\"high\"\n",
    "    )\n",
    "\n",
    "    if response and response.choices:\n",
    "        print(f\"âœ… README analysis completed for {repo_name}\")\n",
    "        return response.choices[0].message.parsed\n",
    "    \n",
    "    print(f\"âŒ README analysis failed for {repo_name}\")\n",
    "    return None\n",
    "\n",
    "# Test analyze_repo_with_rubric\n",
    "# print(\"Testing analyze_repo_with_rubric\")\n",
    "# test_usage = ModelUsageAsync()\n",
    "# test_repos = [\"langchain-ai/langchain\", \"supabase/supabase\", \"typst/typst\", \"aidenybai/million\", \"aidenybai/react-scan\"]\n",
    "# for repo in test_repos:\n",
    "#     print(f\"Testing {repo}\")\n",
    "#     print(await analyze_repo_with_rubric(openai_client, github_client, repo, test_usage))\n",
    "#     print(\"-\"*100)\n",
    "\n",
    "# # Display cost information\n",
    "# total_cost = await test_usage.get_cost()\n",
    "# total_tokens = await test_usage.get_tokens_used()\n",
    "# print(f\"\\nðŸ’¸ Test Analysis Cost: ${total_cost:.4f}\")\n",
    "# print(f\"ðŸ”¢ Test Total Tokens Used: {total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "528fda45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Community Sentiment Analysis Function\n",
    "async def analyze_community_sentiment(\n",
    "    openai_client: AsyncOpenAI,\n",
    "    exa_client: Exa,\n",
    "    repo_name: str,\n",
    "    llm_usage: ModelUsageAsync\n",
    ") -> Optional[CommunitySentimentAnalysis]:\n",
    "    \"\"\"Analyze community sentiment using Exa search and LLM analysis.\"\"\"\n",
    "    # Use Exa to find relevant discussions\n",
    "    query = f\"discussions and reviews of the GitHub repository '{repo_name}'\"\n",
    "    try:\n",
    "        search_response = exa_client.search_and_contents(\n",
    "            query,\n",
    "            num_results=5,\n",
    "            include_domains=[\"news.ycombinator.com\", \"reddit.com\"],\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        if not search_response.results:\n",
    "            print(f\"âš ï¸ No Exa search results found for {repo_name}\")\n",
    "            return None\n",
    "            \n",
    "        # Format the context for the LLM\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"URL: {res.url}\\nContent: {res.text}\" \n",
    "            for res in search_response.results\n",
    "        ])\n",
    "        print(f\"âœ… Found {len(search_response.results)} community discussions for {repo_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Exa search failed for {repo_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    system_prompt = \"You are a VC analyst specializing in developer sentiment. Your task is to analyze discussions from Hacker News and Reddit to evaluate a project's community perception using a specific rubric.\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    Project: {repo_name}\n",
    "\n",
    "    Discussion Threads:\n",
    "    ---\n",
    "    {context}\n",
    "    ---\n",
    "\n",
    "    **Your Task:**\n",
    "    Analyze the discussion threads and provide a structured analysis based on the rubric below.\n",
    "\n",
    "    **Rubric for Evaluation:**\n",
    "\n",
    "    **1. Community Excitement Score (1-5):**\n",
    "    - **1 (Dismissive):** Predominantly negative. Commenters are critical or uninterested.\n",
    "    - **3 (Curious):** Neutral to positive. Comments like \"neat idea\" or \"I'll keep an eye on this.\" No strong emotion.\n",
    "    - **5 (Evangelical):** Viral excitement. Comments like \"game-changer,\" \"I'm switching to this immediately,\" or \"finally, someone solved this.\"\n",
    "\n",
    "    **2. Problem-Solution Fit Score (1-5):**\n",
    "    - **1 (Mismatch):** Community doesn't believe the problem is real or thinks the solution is wrong. Many \"Why not just use X?\" comments.\n",
    "    - **3 (Acknowledged):** The community agrees the problem is real but may be unconvinced this is the best solution.\n",
    "    - **5 (Strong Resonance):** The community immediately validates the pain point. Comments like \"I desperately need this,\" or \"This solves a huge headache for my team.\"\n",
    "\n",
    "    **3. Credibility & Adoption Score (1-5):**\n",
    "    - **1 (Low):** Praise is superficial, or there are significant, unaddressed technical critiques. No one mentions using it.\n",
    "    - **3 (Moderate):** Positive discussion, but mainly from anonymous users. A few people mention they are \"trying it out.\"\n",
    "    - **5 (High):** Praise comes from users with stated authority (e.g., \"Principal Engineer at X\"). Multiple commenters report successful adoption in personal or professional projects.\n",
    "\n",
    "    **Required Output:**\n",
    "    Based on your analysis, provide the five required fields: `excitement_score`, `problem_solution_fit_score`, `credibility_adoption_score`, `key_praise_quote`, and `main_criticism`.\n",
    "    \"\"\"\n",
    "\n",
    "    response = await call_openai_structured(\n",
    "        openai_client=openai_client,\n",
    "        model=\"o4-mini\",\n",
    "        messages=[{\"role\": \"developer\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}],\n",
    "        response_format=CommunitySentimentAnalysis,\n",
    "        llm_usage=llm_usage,\n",
    "        llm_usage_label=f\"sentiment_analysis_{repo_name}\",\n",
    "        reasoning_effort=\"high\"\n",
    "    )\n",
    "    \n",
    "    if response and response.choices:\n",
    "        print(f\"âœ… Sentiment analysis completed for {repo_name}\")\n",
    "        return response.choices[0].message.parsed\n",
    "    \n",
    "    print(f\"âŒ Sentiment analysis failed for {repo_name}\")\n",
    "    return None\n",
    "\n",
    "    print(user_prompt)\n",
    "\n",
    "# Test analyze_community_sentiment\n",
    "# print(\"Testing analyze_community_sentiment\")\n",
    "# test_usage = ModelUsageAsync()\n",
    "# test_repos = [\"langchain-ai/langchain\", \"supabase/supabase\", \"typst/typst\", \"aidenybai/million\", \"aidenybai/react-scan\"]\n",
    "# for repo in test_repos:\n",
    "#     print(f\"Testing {repo}\")\n",
    "#     print(await analyze_community_sentiment(openai_client, exa_client, repo, test_usage))\n",
    "#     print(\"-\"*100)\n",
    "\n",
    "# # Display cost information\n",
    "# total_cost = await test_usage.get_cost()\n",
    "# total_tokens = await test_usage.get_tokens_used()\n",
    "# print(f\"\\nðŸ’¸ Test Analysis Cost: ${total_cost:.4f}\")\n",
    "# print(f\"ðŸ”¢ Test Total Tokens Used: {total_tokens:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f8246a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_final_score(analysis: 'FinalAnalysis') -> float:\n",
    "    \"\"\"\n",
    "    Calculate a final, weighted investment score based on multiple factors.\n",
    "    \n",
    "    This version uses explicit weights and normalizes each component to a 0-10 scale\n",
    "    before applying weights, making the system easier to tune.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Step 1: Define Weights ---\n",
    "    # These can be easily tuned to adjust the ranking algorithm.\n",
    "    weights = {\n",
    "        \"popularity\": 1.0,\n",
    "        \"repo_quality\": 1.5,\n",
    "        \"community_sentiment\": 1.8  # Weighs sentiment ~20% more than repo quality\n",
    "    }\n",
    "    \n",
    "    # --- Step 2: Calculate Individual Component Scores (Normalized to 0-10) ---\n",
    "    \n",
    "    # 1. Popularity Score (Log-normalized)\n",
    "    # A score of 22000 (like supabase) gives ~9.0. A score of 100 gives ~4.6.\n",
    "    # This captures order-of-magnitude popularity better.\n",
    "    total_score_raw = float(analysis.oss_insight_data.get(\"total_score\", 0))\n",
    "    # Add 1 to avoid math.log(0). The base of the log can be tuned.\n",
    "    popularity_score = math.log(total_score_raw + 1, 10) * 2.22 if total_score_raw > 0 else 0.0\n",
    "    popularity_score = min(popularity_score, 10.0) # Cap at 10\n",
    "\n",
    "    # 2. Repository Rubric Quality Score\n",
    "    repo_score = 0.0\n",
    "    if analysis.repo_rubric_analysis:\n",
    "        repo_sum = (\n",
    "            analysis.repo_rubric_analysis.problem_clarity_score +\n",
    "            analysis.repo_rubric_analysis.adoption_ease_score +\n",
    "            analysis.repo_rubric_analysis.maturity_health_score\n",
    "        )\n",
    "        # Normalize from a 3-15 scale to a 0-10 scale\n",
    "        repo_score = ((repo_sum - 3) / 12) * 10\n",
    "\n",
    "    # 3. Community Sentiment Score\n",
    "    sentiment_score = 0.0\n",
    "    if analysis.community_sentiment_analysis:\n",
    "        sentiment_sum = (\n",
    "            analysis.community_sentiment_analysis.excitement_score +\n",
    "            analysis.community_sentiment_analysis.problem_solution_fit_score +\n",
    "            analysis.community_sentiment_analysis.credibility_adoption_score\n",
    "        )\n",
    "        # Normalize from a 3-15 scale to a 0-10 scale\n",
    "        sentiment_score = ((sentiment_sum - 3) / 12) * 10\n",
    "        \n",
    "    # --- Step 3: Apply Weights and Calculate Final Score ---\n",
    "    \n",
    "    final_score = (\n",
    "        (popularity_score * weights[\"popularity\"]) +\n",
    "        (repo_score * weights[\"repo_quality\"]) +\n",
    "        (sentiment_score * weights[\"community_sentiment\"])\n",
    "    )\n",
    "    \n",
    "    # Optional: Normalize the final score to a 0-100 scale for easier reading\n",
    "    total_weight = sum(weights.values())\n",
    "    final_score_normalized = (final_score / (10 * total_weight)) * 100\n",
    "    \n",
    "    return round(final_score_normalized, 2)\n",
    "\n",
    "# Repository Analysis Pipeline\n",
    "async def analyze_repository_pipeline(\n",
    "    repo_name: str,\n",
    "    repo_data: dict,\n",
    "    openai_client: AsyncOpenAI,\n",
    "    github_client: Github,\n",
    "    exa_client: Exa,\n",
    "    llm_usage: ModelUsageAsync\n",
    ") -> FinalAnalysis:\n",
    "    \"\"\"Complete analysis pipeline for a single repository.\"\"\"\n",
    "    print(f\"ðŸ” Analyzing {repo_name}...\")\n",
    "    \n",
    "    # Parallel analysis where possible\n",
    "    repo_rubric_task = analyze_repo_with_rubric(openai_client, github_client, repo_name, llm_usage)\n",
    "    sentiment_task = analyze_community_sentiment(openai_client, exa_client, repo_name, llm_usage)\n",
    "    \n",
    "    repo_rubric_analysis, community_sentiment_analysis = await asyncio.gather(\n",
    "        repo_rubric_task,\n",
    "        sentiment_task,\n",
    "        return_exceptions=True\n",
    "    )\n",
    "    \n",
    "    # Handle exceptions\n",
    "    if isinstance(repo_rubric_analysis, Exception):\n",
    "        print(f\"âš ï¸ README analysis failed for {repo_name}: {repo_rubric_analysis}\")\n",
    "        repo_rubric_analysis = None\n",
    "    \n",
    "    if isinstance(community_sentiment_analysis, Exception):\n",
    "        print(f\"âš ï¸ Sentiment analysis failed for {repo_name}: {community_sentiment_analysis}\")\n",
    "        community_sentiment_analysis = None\n",
    "    \n",
    "    # Create final analysis\n",
    "    analysis = FinalAnalysis(\n",
    "        repo_name=repo_name,\n",
    "        oss_insight_data=repo_data,\n",
    "        repo_rubric_analysis=repo_rubric_analysis,\n",
    "        community_sentiment_analysis=community_sentiment_analysis\n",
    "    )\n",
    "    \n",
    "    # Calculate score\n",
    "    analysis.score = calculate_final_score(analysis)\n",
    "    \n",
    "    return analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "70d6b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reporting Functions\n",
    "def print_analysis_summary(analysis: FinalAnalysis) -> None:\n",
    "    \"\"\"Print formatted analysis summary.\"\"\"\n",
    "    print(f\"\\n## {analysis.repo_name} | ðŸ“Š Score: {analysis.score}\")\n",
    "    print(f\"ðŸ“ˆ Total Score: {analysis.oss_insight_data.get('total_score')} | â­ Stars: {analysis.oss_insight_data.get('stars')}\")\n",
    "    print(f\"ðŸ“ Description: {analysis.oss_insight_data.get('description')}\")\n",
    "    \n",
    "    if analysis.repo_rubric_analysis:\n",
    "        print(f\"ðŸ“– Problem Solved: {analysis.repo_rubric_analysis.problem_solved}\")\n",
    "        print(f\"ðŸŽ¯ Problem Statement Clarity: {analysis.repo_rubric_analysis.problem_clarity_score}/5\")\n",
    "        print(f\"âš¡ Ease of Adoption: {analysis.repo_rubric_analysis.adoption_ease_score}/5\")\n",
    "        print(f\"ðŸ’Š Project Maturity & Community Health: {analysis.repo_rubric_analysis.maturity_health_score}/5\")\n",
    "    \n",
    "    if analysis.community_sentiment_analysis:\n",
    "        print(f\"ðŸ˜Š Community Sentiment: {analysis.community_sentiment_analysis.excitement_score}/5\")\n",
    "        print(f\"ðŸ“ˆ Problem Solution Fit: {analysis.community_sentiment_analysis.problem_solution_fit_score}/5\")\n",
    "        print(f\"ðŸ’Š Credibility & Adoption: {analysis.community_sentiment_analysis.credibility_adoption_score}/5\")\n",
    "        if analysis.community_sentiment_analysis.key_praise_quote:\n",
    "            print(f\"ðŸ’¬ Key Praise Quote: {analysis.community_sentiment_analysis.key_praise_quote}\")\n",
    "        if analysis.community_sentiment_analysis.main_criticism:\n",
    "            print(f\"âš ï¸ Main Criticism: {analysis.community_sentiment_analysis.main_criticism}\")\n",
    "\n",
    "async def print_final_report(final_results: List[FinalAnalysis], total_usage: ModelUsageAsync) -> None:\n",
    "    \"\"\"Print the final investment report.\"\"\"\n",
    "    # Sort results by score\n",
    "    sorted_results = sorted(final_results, key=lambda x: x.score or 0, reverse=True)\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“ˆ FINAL VC INVESTMENT REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, result in enumerate(sorted_results, 1):\n",
    "        print(f\"\\nðŸ† RANK #{i}\")\n",
    "        print_analysis_summary(result)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š PIPELINE STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸ”¢ Total Repositories Analyzed: {len(final_results)}\")\n",
    "    print(f\"âœ… Successful Repo Rubric Analyses: {sum(1 for r in final_results if r.repo_rubric_analysis)}\")\n",
    "    print(f\"ðŸ˜Š Successful Community Sentiment Analyses: {sum(1 for r in final_results if r.community_sentiment_analysis)}\")\n",
    "    print(f\"ðŸ’° Analysis Completeness: {len([r for r in final_results if r.repo_rubric_analysis and r.community_sentiment_analysis])}/{len(final_results)} repositories fully analyzed\")\n",
    "    \n",
    "    # Display cost information\n",
    "    total_cost = await total_usage.get_cost()\n",
    "    total_tokens = await total_usage.get_tokens_used()\n",
    "    print(f\"\\nðŸ’¸ Total Analysis Cost: ${total_cost:.4f}\")\n",
    "    print(f\"ðŸ”¢ Total Tokens Used: {total_tokens:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d7c45630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Production-ready 3-tier filtering function loaded\n"
     ]
    }
   ],
   "source": [
    "def _is_classification_result(obj) -> TypeGuard[DevOpsClassification]:\n",
    "    return not isinstance(obj, Exception) and obj is not None\n",
    "\n",
    "# Production-Ready 3-Tier Filtering (Type-Safe)\n",
    "async def filter_devops_repositories(\n",
    "    period_trending_repos: Dict[str, dict],\n",
    "    dev_ops_collections: List[Dict[str, str]],\n",
    "    openai_client: AsyncOpenAI,\n",
    "    github_client: Github,\n",
    "    use_llm_classification: bool = True,\n",
    "    fallback_to_keywords: bool = True,\n",
    "    filter_big_tech_companies: bool = True,\n",
    "    llm_batch_size: int = 10\n",
    ") -> Dict[str, dict]:\n",
    "    \"\"\"Production-ready 3-tier filtering with proper error handling.\"\"\"\n",
    "    # Collect all repositories first\n",
    "    all_repos = []\n",
    "    for period, trending_data in period_trending_repos.items():\n",
    "        if not trending_data or not trending_data.get(\"data\"):\n",
    "            continue\n",
    "        all_repos.extend(trending_data[\"data\"][\"rows\"])\n",
    "\n",
    "    print(f\"ðŸ“Š Total repositories to process: {len(all_repos)}\")\n",
    "\n",
    "    # PHASE 0: Filter out big tech companies (PARALLEL VERSION)\n",
    "    if filter_big_tech_companies:\n",
    "        print(f\"\\nðŸ¢ Phase 0 - Big Tech Filtering...\")\n",
    "        filtered_repos_list = []\n",
    "        big_tech_usage = ModelUsageAsync()\n",
    "        \n",
    "        # Process in parallel batches for speed\n",
    "        big_tech_batch_size = 10  # Same as LLM batch size\n",
    "        total_batches = (len(all_repos) + big_tech_batch_size - 1) // big_tech_batch_size\n",
    "        \n",
    "        for batch_idx in range(0, len(all_repos), big_tech_batch_size):\n",
    "            batch = all_repos[batch_idx:batch_idx + big_tech_batch_size]\n",
    "            batch_num = (batch_idx // big_tech_batch_size) + 1\n",
    "            \n",
    "            print(f\"ðŸ”„ Processing batch {batch_num}/{total_batches} ({len(batch)} repositories)\")\n",
    "            \n",
    "            # Create parallel classification tasks\n",
    "            classification_tasks = [\n",
    "                classify_big_tech_company(\n",
    "                    openai_client=openai_client,\n",
    "                    github_client=github_client,\n",
    "                    repo_name=repo[\"repo_name\"],\n",
    "                    description=repo.get(\"description\", \"\"),\n",
    "                    llm_usage=big_tech_usage\n",
    "                )\n",
    "                for repo in batch\n",
    "            ]\n",
    "            \n",
    "            # Execute batch in parallel\n",
    "            try:\n",
    "                classifications = await asyncio.gather(*classification_tasks, return_exceptions=True)\n",
    "                \n",
    "                # Process results with explicit exception handling\n",
    "                for repo, result in zip(batch, classifications):\n",
    "                    # Handle exceptions explicitly\n",
    "                    if isinstance(result, Exception):\n",
    "                        print(f\"âš ï¸ Big tech classification failed for {repo['repo_name']}: {result}\")\n",
    "                        filtered_repos_list.append(repo)  # Include on error (conservative)\n",
    "                        continue\n",
    "                    \n",
    "                    # Process successful classification\n",
    "                    if _is_classification_result(result) and result.is_big_tech:\n",
    "                        print(f\"ðŸ¢ Filtered out big tech: {repo['repo_name']} (confidence: {result.confidence_score}/5)\")\n",
    "                        # Skip this repo (don't add to filtered_repos_list)\n",
    "                        continue\n",
    "                    else:\n",
    "                        filtered_repos_list.append(repo)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Batch processing failed: {e}\")\n",
    "                # Add all repos from failed batch (conservative approach)\n",
    "                filtered_repos_list.extend(batch)\n",
    "        \n",
    "        # Show big tech filtering results\n",
    "        big_tech_cost = await big_tech_usage.get_cost()\n",
    "        print(f\"ðŸ’° Big tech filtering cost: ${big_tech_cost:.4f}\")\n",
    "        print(f\"ðŸ“Š Filtered out {len(all_repos) - len(filtered_repos_list)} big tech repos\")\n",
    "        print(f\"ðŸ“Š Remaining repos for DevOps filtering: {len(filtered_repos_list)}\")\n",
    "        \n",
    "        # Reconstruct period_trending_repos with filtered repos for processing\n",
    "        period_trending_repos = {\n",
    "            \"filtered\": {\n",
    "                \"data\": {\n",
    "                    \"rows\": filtered_repos_list\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # TIER 1: Collection-based filtering\n",
    "    dev_ops_collection_names = {\n",
    "        name for collection_dict in dev_ops_collections \n",
    "        for name in collection_dict.values()\n",
    "    }\n",
    "    \n",
    "    potential_leads = {}\n",
    "    collection_matches = 0\n",
    "    llm_matches = 0\n",
    "    keyword_matches = 0\n",
    "    \n",
    "    # DevOps keywords for final fallback\n",
    "    devops_keywords = {\n",
    "        'docker', 'kubernetes', 'k8s', 'terraform', 'ansible', 'jenkins', \n",
    "        'ci/cd', 'cicd', 'deployment', 'infrastructure', 'devops', 'monitoring',\n",
    "        'prometheus', 'grafana', 'elasticsearch', 'nginx', 'apache', 'redis',\n",
    "        'postgresql', 'mysql', 'microservices', 'container', 'orchestration',\n",
    "        'automation', 'pipeline', 'cloud', 'aws', 'azure', 'gcp', 'helm'\n",
    "    }\n",
    "    \n",
    "    print(f\"ðŸ” 3-Tier Filtering Strategy: Collections â†’ LLM â†’ Keywords\")\n",
    "    print(f\"ðŸ“‹ Using {len(dev_ops_collection_names)} DevOps collections for Tier 1...\")\n",
    "    \n",
    "    # Collect repositories for processing\n",
    "    unmatched_repos = []\n",
    "    \n",
    "    for period, trending_data in period_trending_repos.items():\n",
    "        if not trending_data or not trending_data.get(\"data\"):\n",
    "            continue\n",
    "            \n",
    "        print(f\"ðŸ“… Processing {period}...\")\n",
    "        \n",
    "        for repo in trending_data[\"data\"][\"rows\"]:\n",
    "            repo_name = repo[\"repo_name\"]\n",
    "            repo_collections = set(repo.get(\"collection_names\", \"\").split(','))\n",
    "            repo_collections = {c.strip() for c in repo_collections if c.strip()}\n",
    "            \n",
    "            # TIER 1: Collection-based filtering\n",
    "            if not dev_ops_collection_names.isdisjoint(repo_collections):\n",
    "                if repo_name not in potential_leads:\n",
    "                    potential_leads[repo_name] = repo\n",
    "                    matching_collections = repo_collections.intersection(dev_ops_collection_names)\n",
    "                    print(f\"âœ… Found DevOps repo: {repo_name} (matches: {matching_collections})\")\n",
    "                    collection_matches += 1\n",
    "                    continue\n",
    "            \n",
    "            # Store for LLM analysis\n",
    "            if repo_name not in potential_leads:\n",
    "                unmatched_repos.append(repo)\n",
    "    \n",
    "        # TIER 2: LLM-based classification (PARALLEL for speed)\n",
    "        if use_llm_classification and unmatched_repos:\n",
    "            print(f\"\\nðŸ¤– Tier 2 - LLM Classification on {len(unmatched_repos)} repositories...\")\n",
    "            llm_usage = ModelUsageAsync()\n",
    "            \n",
    "            # Process in parallel batches for speed\n",
    "            total_batches = (len(unmatched_repos) + llm_batch_size - 1) // llm_batch_size\n",
    "            \n",
    "            for batch_idx in range(0, len(unmatched_repos), llm_batch_size):\n",
    "                batch = unmatched_repos[batch_idx:batch_idx + llm_batch_size]\n",
    "                batch_num = (batch_idx // llm_batch_size) + 1\n",
    "                \n",
    "                print(f\"ðŸ”„ Processing batch {batch_num}/{total_batches} ({len(batch)} repositories)\")\n",
    "                \n",
    "                # Create parallel classification tasks\n",
    "                classification_tasks = [\n",
    "                    classify_repository_with_llm(\n",
    "                        openai_client=openai_client,\n",
    "                        repo_name=repo[\"repo_name\"],\n",
    "                        description=repo.get(\"description\", \"\"),\n",
    "                        llm_usage=llm_usage\n",
    "                    )\n",
    "                    for repo in batch if repo[\"repo_name\"] not in potential_leads\n",
    "                ]\n",
    "                \n",
    "                # Execute batch in parallel\n",
    "                try:\n",
    "                    classifications = await asyncio.gather(*classification_tasks, return_exceptions=True)\n",
    "                    \n",
    "                    # Process results with explicit type handling\n",
    "                    repos_in_batch = [repo for repo in batch if repo[\"repo_name\"] not in potential_leads]\n",
    "                    \n",
    "                    for repo, result in zip(repos_in_batch, classifications):\n",
    "                        # Handle exceptions explicitly\n",
    "                        if isinstance(result, Exception):\n",
    "                            print(f\"âš ï¸ LLM classification failed for {repo['repo_name']}: {result}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Type-safe processing: result is now Optional[DevOpsClassification]\n",
    "                        if _is_classification_result(result):\n",
    "                            try:\n",
    "                                if result.is_devops_related:\n",
    "                                    repo_name = repo[\"repo_name\"]\n",
    "                                    potential_leads[repo_name] = repo\n",
    "                                    categories_str = ', '.join(result.devops_categories[:2]) if result.devops_categories else 'General DevOps'\n",
    "                                    print(f\"ðŸ§  Tier 2 - LLM match: {repo_name} (confidence: {result.confidence_score}/5, categories: {categories_str})\")\n",
    "                                    llm_matches += 1\n",
    "\n",
    "                            except AttributeError:\n",
    "                                # Additional safety for any attribute access issues\n",
    "                                print(f\"âš ï¸ Invalid classification result for {repo['repo_name']}\")\n",
    "                                \n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Batch processing failed: {e}\")\n",
    "            \n",
    "            # Show LLM costs\n",
    "            llm_cost = await llm_usage.get_cost()\n",
    "            print(f\"ðŸ’° LLM classification cost: ${llm_cost:.4f}\")\n",
    "    \n",
    "    # TIER 3: Keyword-based final fallback\n",
    "    if fallback_to_keywords:\n",
    "        print(f\"\\nðŸ” Tier 3 - Keyword-based fallback...\")\n",
    "        for repo in unmatched_repos:\n",
    "            repo_name = repo[\"repo_name\"]\n",
    "            if repo_name in potential_leads:\n",
    "                continue  # Already matched\n",
    "                \n",
    "            description = repo.get(\"description\", \"\").lower()\n",
    "            repo_name_lower = repo_name.lower()\n",
    "            \n",
    "            # Check for DevOps keywords\n",
    "            found_keywords = []\n",
    "            for keyword in devops_keywords:\n",
    "                if keyword in description or keyword in repo_name_lower:\n",
    "                    found_keywords.append(keyword)\n",
    "            \n",
    "            if found_keywords:\n",
    "                potential_leads[repo_name] = repo\n",
    "                print(f\"ðŸŽ¯ Tier 3 - Keyword match: {repo_name}\")\n",
    "                keyword_matches += 1\n",
    "    \n",
    "    print(f\"\\nðŸ“Š 3-Tier Filtering Results:\")\n",
    "    print(f\"   â€¢ Tier 1 (Collections): {collection_matches}\")\n",
    "    print(f\"   â€¢ Tier 2 (LLM): {llm_matches}\")\n",
    "    print(f\"   â€¢ Tier 3 (Keywords): {keyword_matches}\")\n",
    "    print(f\"   â€¢ Total unique leads: {len(potential_leads)}\")\n",
    "    \n",
    "    return potential_leads\n",
    "\n",
    "print(\"âœ… Production-ready 3-tier filtering function loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "69f39672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… VC Sourcing Pipeline functions loaded\n",
      "ðŸ“‹ Ready to execute pipeline - run the next cell to start analysis\n"
     ]
    }
   ],
   "source": [
    "# Main VC Sourcing Pipeline Execution with 3-Tier Filtering\n",
    "async def run_vc_sourcing_pipeline():\n",
    "    \"\"\"Execute the complete VC sourcing pipeline with enhanced 3-tier filtering.\"\"\"\n",
    "    print(\"ðŸš€ Starting VC Sourcing Pipeline...\")\n",
    "\n",
    "    # Fetch Trending Repositories Data\n",
    "    print(\"ðŸš€ Phase 0: Fetching trending repositories data...\")\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        period_trending_repos = {}\n",
    "        for period in PERIOD_VALUES:\n",
    "            print(f\"ðŸ“… Fetching trending repos for {period}...\")\n",
    "            trending_data = await make_oss_insight_request(\n",
    "                session=session,\n",
    "                url=TRENDING_REPOS_URL,\n",
    "                params={\"period\": period}\n",
    "            )\n",
    "            if trending_data:\n",
    "                period_trending_repos[period] = trending_data\n",
    "                repo_count = len(trending_data.get(\"data\", {}).get(\"rows\", []))\n",
    "                print(f\"âœ… Found {repo_count} trending repositories for {period}\")\n",
    "            else:\n",
    "                print(f\"âŒ Failed to fetch data for {period}\")\n",
    "\n",
    "        total_repos = sum(len(data.get(\"data\", {}).get(\"rows\", [])) for data in period_trending_repos.values())\n",
    "        print(f\"\\nðŸ“Š Total repositories fetched: {total_repos} across {len(period_trending_repos)} periods\")\n",
    "\n",
    "        # Fetch Collections and Identify DevOps Collections\n",
    "        print(\"ðŸ” Fetching and analyzing repository collections...\")\n",
    "\n",
    "        # Initialize cost tracking for DevOps collections identification\n",
    "        dev_ops_collections_usage = ModelUsageAsync()\n",
    "    \n",
    "        collections_data = await make_oss_insight_request(\n",
    "            session=session,\n",
    "            url=COLLECTIONS_URL,\n",
    "            params={}\n",
    "        )\n",
    "\n",
    "        if collections_data:\n",
    "            total_collections = len(collections_data.get(\"data\", {}).get(\"rows\", []))\n",
    "            print(f\"âœ… Found {total_collections} total collections\")\n",
    "            \n",
    "            # Identify DevOps collections using LLM\n",
    "            dev_ops_collections = await get_dev_ops_collections(\n",
    "                openai_client=openai_client,\n",
    "                collections_data=collections_data,\n",
    "                llm_usage=dev_ops_collections_usage\n",
    "            )\n",
    "            \n",
    "            if dev_ops_collections:\n",
    "                dev_ops_collections = [{c.id: c.name} for c in dev_ops_collections]\n",
    "                print(f\"ðŸ“‹ DevOps collections identified:\")\n",
    "                for collection in dev_ops_collections[:10]:  # Show first 10\n",
    "                    for id_key, name in collection.items():\n",
    "                        print(f\"  â€¢ {name} (ID: {id_key})\")\n",
    "                if len(dev_ops_collections) > 10:\n",
    "                    print(f\"  ... and {len(dev_ops_collections) - 10} more\")\n",
    "            \n",
    "            # Display cost information\n",
    "            cost = await dev_ops_collections_usage.get_cost()\n",
    "            tokens = await dev_ops_collections_usage.get_tokens_used()\n",
    "            print(f\"ðŸ’° DevOps identification cost: ${cost:.4f} ({tokens:,} tokens)\")\n",
    "        else:\n",
    "            print(\"âŒ Failed to fetch collections data\")\n",
    "            dev_ops_collections = []\n",
    "\n",
    "    # Data Collection Summary\n",
    "    print(\"ðŸ“Š Data Collection Phase Complete\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Summary of collected data\n",
    "    if 'period_trending_repos' in globals():\n",
    "        print(f\"âœ… Trending Repositories: {len(period_trending_repos)} time periods\")\n",
    "        for period, data in period_trending_repos.items():\n",
    "            if data:\n",
    "                count = len(data.get(\"data\", {}).get(\"rows\", []))\n",
    "                print(f\"   â€¢ {period}: {count} repositories\")\n",
    "\n",
    "    if 'dev_ops_collections' in globals():\n",
    "        print(f\"âœ… DevOps Collections: {len(dev_ops_collections)} identified\")\n",
    "        \n",
    "    print(\"\\nðŸŽ¯ Ready to proceed with VC sourcing pipeline!\")\n",
    "    print(\"ðŸ“‹ Data available for analysis:\")\n",
    "    print(\"   â€¢ Trending repository data across multiple time periods\")\n",
    "    print(\"   â€¢ DevOps-specific collection classifications\")\n",
    "    print(\"   â€¢ API clients configured for GitHub, Exa, and OpenAI\")\n",
    "    \n",
    "    # Phase 1: Filter DevOps repositories from trending data\n",
    "\n",
    "    # TODO: for demo, just use the repos in past month\n",
    "    past_month_repos = period_trending_repos[\"past_month\"]\n",
    "    test_repos = {\n",
    "        \"past_month\": past_month_repos\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸ” Using 3-Tier Enhanced Filtering: Collections â†’ LLM â†’ Keywords\")\n",
    "    potential_leads = await filter_devops_repositories(\n",
    "        period_trending_repos=test_repos,\n",
    "        dev_ops_collections=dev_ops_collections,\n",
    "        openai_client=openai_client,\n",
    "        github_client=github_client,\n",
    "        use_llm_classification=True,\n",
    "        fallback_to_keywords=True,\n",
    "        filter_big_tech_companies=True,\n",
    "        llm_batch_size=10,\n",
    "    )\n",
    "    \n",
    "    if not potential_leads:\n",
    "        print(\"âŒ No DevOps repositories found. Pipeline ended.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize usage tracker for the entire analysis run\n",
    "    total_usage = ModelUsageAsync()\n",
    "    final_results = []\n",
    "    \n",
    "    # TODO: for demo, just analyze first 10 repos instead of the entirety of potential_leads\n",
    "    print(f\"\\nðŸ”¬ Phase 2 & 3: Analyzing 10 repositories...\")\n",
    "    \n",
    "    # Limit analysis to first 10 repositories for demonstration\n",
    "    # Remove this limit for full analysis\n",
    "    limited_leads = dict(list(potential_leads.items())[:10])\n",
    "    print(f\"ðŸ“ Note: Analyzing first {len(limited_leads)} repositories for demonstration\")\n",
    "    \n",
    "    # Analyze each repository\n",
    "    for repo_name, repo_data in limited_leads.items():\n",
    "        try:\n",
    "            analysis_result = await analyze_repository_pipeline(\n",
    "                repo_name=repo_name,\n",
    "                repo_data=repo_data,\n",
    "                openai_client=openai_client,\n",
    "                github_client=github_client,\n",
    "                exa_client=exa_client,\n",
    "                llm_usage=total_usage\n",
    "            )\n",
    "            final_results.append(analysis_result)\n",
    "            print(f\"âœ… Completed analysis for {repo_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to analyze {repo_name}: {e}\")\n",
    "    \n",
    "    # Phase 4: Generate final report\n",
    "    print(f\"\\nðŸ Phase 4: Scoring and Reporting...\")\n",
    "    await print_final_report(final_results, total_usage)\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "print(\"âœ… VC Sourcing Pipeline functions loaded\")\n",
    "print(\"ðŸ“‹ Ready to execute pipeline - run the next cell to start analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3d51b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute with 3-Tier Enhanced Filtering\n",
    "# await run_vc_sourcing_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b46639",
   "metadata": {},
   "source": [
    "# Uploading Results to DynamoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "efe3ca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to your existing imports\n",
    "import boto3  # type: ignore\n",
    "import json\n",
    "from decimal import Decimal\n",
    "from datetime import datetime\n",
    "from typing import Any\n",
    "from botocore.exceptions import ClientError # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "86f396c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DynamoDB client\n",
    "# Create session with default profile\n",
    "session = boto3.Session(profile_name='root')  # or your profile name\n",
    "\n",
    "dynamodb = session.resource('dynamodb')\n",
    "dynamodb_client = session.client('dynamodb')\n",
    "\n",
    "# # Simplified DynamoDB Table Creation (No GSIs)\n",
    "# def create_vc_analysis_table(table_name: str = \"vc-sourcing-analysis\") -> bool:\n",
    "#     \"\"\"Create DynamoDB table for VC analysis results with composite key.\"\"\"\n",
    "#     try:\n",
    "#         table = dynamodb.create_table(\n",
    "#             TableName=table_name,\n",
    "#             KeySchema=[\n",
    "#                 {\n",
    "#                     'AttributeName': 'repo_name',\n",
    "#                     'KeyType': 'HASH'  # Primary key\n",
    "#                 },\n",
    "#                 {\n",
    "#                     'AttributeName': 'analysis_date',\n",
    "#                     'KeyType': 'RANGE'  # Sort key\n",
    "#                 }\n",
    "#             ],\n",
    "#             AttributeDefinitions=[\n",
    "#                 {\n",
    "#                     'AttributeName': 'repo_name',\n",
    "#                     'AttributeType': 'S'\n",
    "#                 },\n",
    "#                 {\n",
    "#                     'AttributeName': 'analysis_date',\n",
    "#                     'AttributeType': 'S'\n",
    "#                 }\n",
    "#             ],\n",
    "#             BillingMode='PAY_PER_REQUEST'  # On-demand pricing\n",
    "#         )\n",
    "        \n",
    "#         # Wait for table to be created\n",
    "#         print(f\"ðŸ—ï¸ Creating DynamoDB table '{table_name}'...\")\n",
    "#         table.wait_until_exists()\n",
    "#         print(f\"âœ… Table '{table_name}' created successfully!\")\n",
    "#         return True\n",
    "        \n",
    "#     except ClientError as e:\n",
    "#         if e.response['Error']['Code'] == 'ResourceInUseException':\n",
    "#             print(f\"ðŸ“‹ Table '{table_name}' already exists\")\n",
    "#             return True\n",
    "#         else:\n",
    "#             print(f\"âŒ Error creating table: {e}\")\n",
    "#             return False\n",
    "\n",
    "# create_vc_analysis_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa605d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DynamoDB integration functions loaded\n"
     ]
    }
   ],
   "source": [
    "def convert_to_dynamodb_format(obj: Any) -> Any:  # Changed return type\n",
    "    \"\"\"Convert Python objects to DynamoDB compatible format.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_dynamodb_format(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_dynamodb_format(item) for item in obj]\n",
    "    elif isinstance(obj, float):\n",
    "        return Decimal(str(obj))\n",
    "    elif isinstance(obj, int):\n",
    "        return Decimal(str(obj))\n",
    "    elif obj is None:\n",
    "        return None\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def serialize_analysis_for_dynamodb(analysis: FinalAnalysis) -> dict[str, Any]:  # More specific return type\n",
    "    \"\"\"Convert FinalAnalysis object to DynamoDB-compatible format.\"\"\"\n",
    "    \n",
    "    # Convert Pydantic models to dictionaries\n",
    "    item: dict[str, Any] = {  # Explicit type annotation\n",
    "        'repo_name': analysis.repo_name,\n",
    "        'analysis_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "        'final_score': Decimal(str(analysis.score)) if analysis.score else Decimal('0'),\n",
    "        \n",
    "        # OSS Insight Data\n",
    "        'oss_insight_data': convert_to_dynamodb_format(analysis.oss_insight_data),\n",
    "        \n",
    "        # Repository Analysis\n",
    "        'repo_analysis': None,\n",
    "        'community_analysis': None\n",
    "    }\n",
    "    \n",
    "    # Add repo rubric analysis if available\n",
    "    if analysis.repo_rubric_analysis:\n",
    "        item['repo_analysis'] = {\n",
    "            'problem_clarity_score': Decimal(str(analysis.repo_rubric_analysis.problem_clarity_score)),\n",
    "            'adoption_ease_score': Decimal(str(analysis.repo_rubric_analysis.adoption_ease_score)),\n",
    "            'maturity_health_score': Decimal(str(analysis.repo_rubric_analysis.maturity_health_score)),\n",
    "            'problem_solved': analysis.repo_rubric_analysis.problem_solved\n",
    "        }\n",
    "    \n",
    "    # Add community sentiment analysis if available  \n",
    "    if analysis.community_sentiment_analysis:\n",
    "        item['community_analysis'] = {\n",
    "            'excitement_score': Decimal(str(analysis.community_sentiment_analysis.excitement_score)),\n",
    "            'problem_solution_fit_score': Decimal(str(analysis.community_sentiment_analysis.problem_solution_fit_score)),\n",
    "            'credibility_adoption_score': Decimal(str(analysis.community_sentiment_analysis.credibility_adoption_score)),\n",
    "            'key_praise_quote': analysis.community_sentiment_analysis.key_praise_quote,\n",
    "            'main_criticism': analysis.community_sentiment_analysis.main_criticism\n",
    "        }\n",
    "    \n",
    "    return item\n",
    "\n",
    "async def upload_analysis_to_dynamodb(\n",
    "    final_results: List[FinalAnalysis], \n",
    "    table_name: str = \"vc-sourcing-analysis\"\n",
    ") -> bool:\n",
    "    \"\"\"Upload analysis results to DynamoDB table.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        table = dynamodb.Table(table_name)\n",
    "        \n",
    "        print(f\"ðŸ“¤ Uploading {len(final_results)} analysis results to DynamoDB...\")\n",
    "        \n",
    "        # Upload each analysis result\n",
    "        successful_uploads = 0\n",
    "        failed_uploads = 0\n",
    "        \n",
    "        for analysis in final_results:\n",
    "            try:\n",
    "                item = serialize_analysis_for_dynamodb(analysis)\n",
    "                \n",
    "                # Put item in DynamoDB\n",
    "                table.put_item(Item=item)\n",
    "                \n",
    "                print(f\"âœ… Uploaded: {analysis.repo_name} (Score: {analysis.score})\")\n",
    "                successful_uploads += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Failed to upload {analysis.repo_name}: {e}\")\n",
    "                failed_uploads += 1\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Upload Summary:\")\n",
    "        print(f\"   âœ… Successful: {successful_uploads}\")\n",
    "        print(f\"   âŒ Failed: {failed_uploads}\")\n",
    "        print(f\"   ðŸ“‹ Total: {len(final_results)}\")\n",
    "        \n",
    "        return failed_uploads == 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ DynamoDB upload error: {e}\")\n",
    "        return False\n",
    "\n",
    "def query_top_repos_by_score(table_name: str = \"vc-sourcing-analysis\", limit: int = 10) -> List[dict[str, Any]]:\n",
    "    \"\"\"Query top repositories by score from DynamoDB.\"\"\"\n",
    "    try:\n",
    "        table = dynamodb.Table(table_name)\n",
    "        \n",
    "        # Scan table and sort by score (since we can't easily query by score range)\n",
    "        response = table.scan()\n",
    "        \n",
    "        items = response['Items']\n",
    "        \n",
    "        # Sort by final_score in descending order\n",
    "        sorted_items = sorted(items, key=lambda x: float(x.get('final_score', 0)), reverse=True)\n",
    "        \n",
    "        return sorted_items[:limit]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error querying DynamoDB: {e}\")\n",
    "        return []\n",
    "\n",
    "def print_dynamodb_results(items: List[dict[str, Any]]) -> None:\n",
    "    \"\"\"Print formatted results from DynamoDB query.\"\"\"\n",
    "    print(\"\\nðŸ† TOP REPOSITORIES FROM DYNAMODB:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, item in enumerate(items, 1):\n",
    "        repo_name = item.get('repo_name', 'Unknown')\n",
    "        score = float(item.get('final_score', 0))\n",
    "        analysis_date = item.get('analysis_date', 'Unknown')\n",
    "        \n",
    "        # Get OSS Insight data\n",
    "        oss_data = item.get('oss_insight_data', {})\n",
    "        total_score = oss_data.get('total_score', 'N/A')\n",
    "        stars = oss_data.get('stars', 'N/A')\n",
    "        description = oss_data.get('description', 'No description available')\n",
    "        \n",
    "        print(f\"\\n## {repo_name} | ðŸ“Š Score: {score}\")\n",
    "        print(f\"ðŸ“ˆ Total Score: {total_score} | â­ Stars: {stars}\")\n",
    "        print(f\"ðŸ“ Description: {description}\")\n",
    "        \n",
    "        # Show repo analysis details if available\n",
    "        if item.get('repo_analysis'):\n",
    "            repo_analysis = item['repo_analysis']\n",
    "            problem_solved = repo_analysis.get('problem_solved', 'Problem description not available')\n",
    "            print(f\"ðŸ“– Problem Solved: {problem_solved}\")\n",
    "            print(f\"ðŸŽ¯ Problem Statement Clarity: {repo_analysis.get('problem_clarity_score', 'N/A')}/5\")\n",
    "            print(f\"âš¡ Ease of Adoption: {repo_analysis.get('adoption_ease_score', 'N/A')}/5\")\n",
    "            print(f\"ðŸ’Š Project Maturity & Community Health: {repo_analysis.get('maturity_health_score', 'N/A')}/5\")\n",
    "        \n",
    "        # Show community sentiment details if available\n",
    "        if item.get('community_analysis'):\n",
    "            community = item['community_analysis']\n",
    "            print(f\"ðŸ˜Š Community Sentiment: {community.get('excitement_score', 'N/A')}/5\")\n",
    "            print(f\"ðŸ“ˆ Problem Solution Fit: {community.get('problem_solution_fit_score', 'N/A')}/5\")\n",
    "            print(f\"ðŸ’Š Credibility & Adoption: {community.get('credibility_adoption_score', 'N/A')}/5\")\n",
    "            \n",
    "            # Add quotes if available\n",
    "            key_praise = community.get('key_praise_quote')\n",
    "            main_criticism = community.get('main_criticism')\n",
    "            \n",
    "            if key_praise:\n",
    "                print(f\"ðŸ’¬ Key Praise Quote: {key_praise}\")\n",
    "            if main_criticism:\n",
    "                print(f\"âš ï¸ Main Criticism: {main_criticism}\")\n",
    "\n",
    "print(\"âœ… Updated DynamoDB results display function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2689e0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Updated pipeline with DynamoDB integration loaded\n"
     ]
    }
   ],
   "source": [
    "# Updated Main Pipeline with DynamoDB Integration\n",
    "async def run_vc_sourcing_pipeline_with_dynamodb(table_name: str = \"vc-sourcing-analysis\"):\n",
    "    \"\"\"Execute VC sourcing pipeline and upload results to DynamoDB.\"\"\"\n",
    "    \n",
    "    # Run your existing pipeline\n",
    "    final_results = await run_vc_sourcing_pipeline()\n",
    "    \n",
    "    if not final_results:\n",
    "        print(\"âŒ No analysis results to upload\")\n",
    "        return\n",
    "    \n",
    "    # Upload results to DynamoDB\n",
    "    print(f\"\\nðŸ’¾ Phase 5: Uploading results to DynamoDB...\")\n",
    "    upload_success = await upload_analysis_to_dynamodb(final_results, table_name)\n",
    "    \n",
    "    if upload_success:\n",
    "        print(f\"âœ… All results uploaded successfully to '{table_name}'\")\n",
    "        \n",
    "        # Query and display top results from DynamoDB\n",
    "        print(f\"\\nðŸ” Querying top results from DynamoDB...\")\n",
    "        top_repos = query_top_repos_by_score(table_name, limit=5)\n",
    "        print_dynamodb_results(top_repos)\n",
    "        \n",
    "    else:\n",
    "        print(f\"âš ï¸ Some uploads failed. Check logs above.\")\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "print(\"âœ… Updated pipeline with DynamoDB integration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c2d46656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting VC Sourcing Pipeline...\n",
      "ðŸš€ Phase 0: Fetching trending repositories data...\n",
      "ðŸ“… Fetching trending repos for past_24_hours...\n",
      "âœ… Found 100 trending repositories for past_24_hours\n",
      "ðŸ“… Fetching trending repos for past_week...\n",
      "âœ… Found 100 trending repositories for past_week\n",
      "ðŸ“… Fetching trending repos for past_month...\n",
      "âœ… Found 100 trending repositories for past_month\n",
      "ðŸ“… Fetching trending repos for past_3_months...\n",
      "âœ… Found 100 trending repositories for past_3_months\n",
      "\n",
      "ðŸ“Š Total repositories fetched: 400 across 4 periods\n",
      "ðŸ” Fetching and analyzing repository collections...\n",
      "âœ… Found 95 total collections\n",
      "ðŸ” Analyzing 95 collections to identify DevOps-related ones...\n",
      "âœ… Identified 62 DevOps collections\n",
      "ðŸ“‹ DevOps collections identified:\n",
      "  â€¢ Static Site Generator (ID: 1)\n",
      "  â€¢ Open Source Database (ID: 2)\n",
      "  â€¢ Web Framework (ID: 10004)\n",
      "  â€¢ Business Intelligence (ID: 10006)\n",
      "  â€¢ Time Series Database (ID: 10007)\n",
      "  â€¢ Graph Database (ID: 10008)\n",
      "  â€¢ Github Alternative (ID: 10009)\n",
      "  â€¢ Artificial Intelligence (ID: 10010)\n",
      "  â€¢ Headless CMS (ID: 10012)\n",
      "  â€¢ Text Editor (ID: 10015)\n",
      "  ... and 52 more\n",
      "ðŸ’° DevOps identification cost: $0.0368 (5,332 tokens)\n",
      "ðŸ“Š Data Collection Phase Complete\n",
      "==================================================\n",
      "âœ… Trending Repositories: 4 time periods\n",
      "   â€¢ past_24_hours: 100 repositories\n",
      "   â€¢ past_week: 100 repositories\n",
      "   â€¢ past_month: 100 repositories\n",
      "   â€¢ past_3_months: 100 repositories\n",
      "âœ… DevOps Collections: 62 identified\n",
      "\n",
      "ðŸŽ¯ Ready to proceed with VC sourcing pipeline!\n",
      "ðŸ“‹ Data available for analysis:\n",
      "   â€¢ Trending repository data across multiple time periods\n",
      "   â€¢ DevOps-specific collection classifications\n",
      "   â€¢ API clients configured for GitHub, Exa, and OpenAI\n",
      "ðŸ” Using 3-Tier Enhanced Filtering: Collections â†’ LLM â†’ Keywords\n",
      "ðŸ“Š Total repositories to process: 100\n",
      "\n",
      "ðŸ¢ Phase 0 - Big Tech Filtering...\n",
      "ðŸ”„ Processing batch 1/10 (10 repositories)\n",
      "âœ… Fetched README for big tech classification: HW-whistleblower/True-Story-of-Pangu\n",
      "âœ… Fetched README for big tech classification: OpenCut-app/OpenCut\n",
      "âœ… Fetched README for big tech classification: permissionlesstech/bitchat\n",
      "âœ… Fetched README for big tech classification: langflow-ai/langflow\n",
      "âœ… Fetched README for big tech classification: n8n-io/n8n\n",
      "âœ… Fetched README for big tech classification: codecrafters-io/build-your-own-x\n",
      "âœ… Fetched README for big tech classification: zama-ai/fhevm\n",
      "âœ… Fetched README for big tech classification: google-gemini/gemini-cli\n",
      "âœ… Fetched README for big tech classification: sindresorhus/awesome\n",
      "âœ… Fetched README for big tech classification: zama-ai/bounty-program\n",
      "ðŸ¢ Filtered out big tech: google-gemini/gemini-cli (confidence: 5/5)\n",
      "ðŸ¢ Filtered out big tech: HW-whistleblower/True-Story-of-Pangu (confidence: 5/5)\n",
      "ðŸ”„ Processing batch 2/10 (10 repositories)\n",
      "âœ… Fetched README for big tech classification: senshinya/MoonTV\n",
      "âœ… Fetched README for big tech classification: NanmiCoder/MediaCrawler\n",
      "âœ… Fetched README for big tech classification: SuperClaude-Org/SuperClaude_Framework\n",
      "âœ… Fetched README for big tech classification: x1xhlol/system-prompts-and-models-of-ai-tools\n",
      "âœ… Fetched README for big tech classification: sst/opencode\n",
      "âœ… Fetched README for big tech classification: DigitalPlatDev/FreeDomain\n",
      "âœ… Fetched README for big tech classification: TauricResearch/TradingAgents\n",
      "âœ… Fetched README for big tech classification: anthropics/claude-code\n",
      "âœ… Fetched README for big tech classification: Zie619/n8n-workflows\n",
      "âœ… Fetched README for big tech classification: microsoft/markitdown\n",
      "ðŸ¢ Filtered out big tech: anthropics/claude-code (confidence: 5/5)\n",
      "ðŸ¢ Filtered out big tech: microsoft/markitdown (confidence: 5/5)\n",
      "ðŸ”„ Processing batch 3/10 (10 repositories)\n",
      "âœ… Fetched README for big tech classification: octra-labs/wallet-gen\n",
      "âœ… Fetched README for big tech classification: MoonshotAI/Kimi-K2\n",
      "âœ… Fetched README for big tech classification: coleam00/context-engineering-intro\n",
      "âœ… Fetched README for big tech classification: getAsterisk/claudia\n",
      "âœ… Fetched README for big tech classification: LibreSpark/LibreTV\n",
      "âœ… Fetched README for big tech classification: datawhalechina/happy-llm\n",
      "âœ… Fetched README for big tech classification: bytedance/trae-agent\n",
      "âœ… Fetched README for big tech classification: ripienaar/free-for-dev\n",
      "âœ… Fetched README for big tech classification: microsoft/generative-ai-for-beginners\n",
      "âœ… Fetched README for big tech classification: microsoft/vscode-copilot-chat\n",
      "ðŸ¢ Filtered out big tech: bytedance/trae-agent (confidence: 5/5)\n",
      "ðŸ¢ Filtered out big tech: microsoft/generative-ai-for-beginners (confidence: 5/5)\n",
      "ðŸ¢ Filtered out big tech: microsoft/vscode-copilot-chat (confidence: 5/5)\n",
      "ðŸ”„ Processing batch 4/10 (10 repositories)\n",
      "âœ… Fetched README for big tech classification: rasbt/LLMs-from-scratch\n",
      "âœ… Fetched README for big tech classification: Shubhamsaboo/awesome-llm-apps\n",
      "âœ… Fetched README for big tech classification: musistudio/claude-code-router\n",
      "âœ… Fetched README for big tech classification: unionlabs/union\n",
      "âœ… Fetched README for big tech classification: rustfs/rustfs\n",
      "âœ… Fetched README for big tech classification: punkpeye/awesome-mcp-servers\n",
      "âœ… Fetched README for big tech classification: upstash/context7\n",
      "âœ… Fetched README for big tech classification: GraphiteEditor/Graphite\n",
      "âœ… Fetched README for big tech classification: modelcontextprotocol/servers\n",
      "âœ… Fetched README for big tech classification: googleapis/genai-toolbox\n",
      "ðŸ¢ Filtered out big tech: googleapis/genai-toolbox (confidence: 5/5)\n",
      "ðŸ”„ Processing batch 5/10 (10 repositories)\n",
      "âœ… Fetched README for big tech classification: TapXWorld/ChinaTextbook\n",
      "âœ… Fetched README for big tech classification: NirDiamant/agents-towards-production\n",
      "âœ… Fetched README for big tech classification: shareAI-lab/analysis_claude_code\n",
      "âœ… Fetched README for big tech classification: humanlayer/12-factor-agents\n",
      "âœ… Fetched README for big tech classification: pickle-com/glass\n",
      "âœ… Fetched README for big tech classification: donnemartin/system-design-primer\n",
      "âœ… Fetched README for big tech classification: opendatalab/MinerU\n",
      "âœ… Fetched README for big tech classification: twentyhq/twenty\n",
      "âœ… Fetched README for big tech classification: awesome-selfhosted/awesome-selfhosted\n",
      "âœ… Fetched README for big tech classification: microsoft/ai-agents-for-beginners\n",
      "ðŸ¢ Filtered out big tech: microsoft/ai-agents-for-beginners (confidence: 5/5)\n",
      "ðŸ”„ Processing batch 6/10 (10 repositories)\n",
      "âœ… Fetched README for big tech classification: stan-smith/FossFLOW\n",
      "âœ… Fetched README for big tech classification: GyulyVGC/sniffnet\n",
      "âœ… Fetched README for big tech classification: clash-verge-rev/clash-verge-rev\n",
      "âœ… Fetched README for big tech classification: Alibaba-NLP/WebAgent\n",
      "âœ… Fetched README for big tech classification: czlonkowski/n8n-mcp\n",
      "âœ… Fetched README for big tech classification: open-webui/open-webui\n",
      "âœ… Fetched README for big tech classification: vinta/awesome-python\n",
      "âœ… Fetched README for big tech classification: OpenListTeam/OpenList\n",
      "âœ… Fetched README for big tech classification: langgenius/dify\n",
      "âœ… Fetched README for big tech classification: opencode-ai/opencode\n",
      "ðŸ¢ Filtered out big tech: Alibaba-NLP/WebAgent (confidence: 5/5)\n",
      "ðŸ”„ Processing batch 7/10 (10 repositories)\n",
      "âœ… Fetched README for big tech classification: DavidHDev/react-bits\n",
      "âœ… Fetched README for big tech classification: th-ch/youtube-music\n",
      "âœ… Fetched README for big tech classification: hesreallyhim/awesome-claude-code\n",
      "âœ… Fetched README for big tech classification: hsliuping/TradingAgents-CN\n",
      "âœ… Fetched README for big tech classification: Kilo-Org/kilocode\n",
      "âœ… Fetched README for big tech classification: dockur/macos\n",
      "âœ… Fetched README for big tech classification: bmadcode/BMAD-METHOD\n",
      "âœ… Fetched README for big tech classification: openai/openai-cs-agents-demo\n",
      "âœ… Fetched README for big tech classification: yt-dlp/yt-dlp\n",
      "âœ… Fetched README for big tech classification: sdmg15/Best-websites-a-programmer-should-visit\n",
      "ðŸ¢ Filtered out big tech: openai/openai-cs-agents-demo (confidence: 5/5)\n",
      "ðŸ”„ Processing batch 8/10 (10 repositories)\n",
      "âœ… Fetched README for big tech classification: davidkimai/Context-Engineering\n",
      "âœ… Fetched README for big tech classification: 521xueweihan/HelloGitHub\n",
      "âœ… Fetched README for big tech classification: massgravel/Microsoft-Activation-Scripts\n",
      "âœ… Fetched README for big tech classification: RSSNext/Folo\n",
      "âœ… Fetched README for big tech classification: maybe-finance/maybe\n",
      "âœ… Fetched README for big tech classification: DataExpert-io/data-engineer-handbook\n",
      "âœ… Fetched README for big tech classification: sohzm/cheating-daddy\n",
      "âœ… Fetched README for big tech classification: astral-sh/uv\n",
      "âœ… Fetched README for big tech classification: patchy631/ai-engineering-hub\n",
      "âœ… Fetched README for big tech classification: github/awesome-copilot\n",
      "ðŸ¢ Filtered out big tech: github/awesome-copilot (confidence: 5/5)\n",
      "ðŸ”„ Processing batch 9/10 (10 repositories)\n",
      "âœ… Fetched README for big tech classification: hangwin/mcp-chrome\n",
      "âœ… Fetched README for big tech classification: ryoppippi/ccusage\n",
      "âœ… Fetched README for big tech classification: eyaltoledano/claude-task-master\n",
      "âœ… Fetched README for big tech classification: infiniflow/ragflow\n",
      "âœ… Fetched README for big tech classification: comfyanonymous/ComfyUI\n",
      "âœ… Fetched README for big tech classification: fosrl/pangolin\n",
      "âœ… Fetched README for big tech classification: anoma/anoma\n",
      "âœ… Fetched README for big tech classification: getzep/graphiti\n",
      "âœ… Fetched README for big tech classification: BloopAI/vibe-kanban\n",
      "âœ… Fetched README for big tech classification: ollama/ollama\n",
      "ðŸ”„ Processing batch 10/10 (10 repositories)\n",
      "âœ… Fetched README for big tech classification: GeeeekExplorer/nano-vllm\n",
      "âœ… Fetched README for big tech classification: VectorSpaceLab/OmniGen2\n",
      "âœ… Fetched README for big tech classification: siteboon/claudecodeui\n",
      "âœ… Fetched README for big tech classification: linshenkx/prompt-optimizer\n",
      "âœ… Fetched README for big tech classification: mendableai/firecrawl\n",
      "âœ… Fetched README for big tech classification: OpenPipe/ART\n",
      "âœ… Fetched README for big tech classification: vllm-project/vllm\n",
      "âœ… Fetched README for big tech classification: enescingoz/awesome-n8n-templates\n",
      "âœ… Fetched README for big tech classification: MrLesk/Backlog.md\n",
      "âœ… Fetched README for big tech classification: microsoft/ML-For-Beginners\n",
      "ðŸ¢ Filtered out big tech: microsoft/ML-For-Beginners (confidence: 5/5)\n",
      "ðŸ’° Big tech filtering cost: $0.2110\n",
      "ðŸ“Š Filtered out 13 big tech repos\n",
      "ðŸ“Š Remaining repos for DevOps filtering: 87\n",
      "ðŸ” 3-Tier Filtering Strategy: Collections â†’ LLM â†’ Keywords\n",
      "ðŸ“‹ Using 62 DevOps collections for Tier 1...\n",
      "ðŸ“… Processing filtered...\n",
      "âœ… Found DevOps repo: n8n-io/n8n (matches: {'Zapier Alternatives'})\n",
      "âœ… Found DevOps repo: langgenius/dify (matches: {'LLM Tools'})\n",
      "\n",
      "ðŸ¤– Tier 2 - LLM Classification on 85 repositories...\n",
      "ðŸ”„ Processing batch 1/9 (10 repositories)\n",
      "ðŸ§  Tier 2 - LLM match: langflow-ai/langflow (confidence: 4/5, categories: Developer productivity tools, Tools for building or deploying AI/ML models)\n",
      "ðŸ§  Tier 2 - LLM match: Zie619/n8n-workflows (confidence: 4/5, categories: Developer productivity tools)\n",
      "ðŸ”„ Processing batch 2/9 (10 repositories)\n",
      "ðŸ§  Tier 2 - LLM match: sst/opencode (confidence: 4/5, categories: Developer productivity tools)\n",
      "ðŸ§  Tier 2 - LLM match: SuperClaude-Org/SuperClaude_Framework (confidence: 3/5, categories: Developer productivity tools)\n",
      "ðŸ§  Tier 2 - LLM match: LibreSpark/LibreTV (confidence: 3/5, categories: Container technologies, Cloud platforms and services)\n",
      "ðŸ§  Tier 2 - LLM match: ripienaar/free-for-dev (confidence: 5/5, categories: Developer productivity tools, Cloud platforms and services)\n",
      "ðŸ§  Tier 2 - LLM match: getAsterisk/claudia (confidence: 4/5, categories: Developer productivity tools, Tools for building or deploying AI/ML models)\n",
      "ðŸ”„ Processing batch 3/9 (10 repositories)\n",
      "ðŸ§  Tier 2 - LLM match: upstash/context7 (confidence: 3/5, categories: Developer productivity tools)\n",
      "ðŸ§  Tier 2 - LLM match: rustfs/rustfs (confidence: 5/5, categories: Database and data infrastructure, Cloud platforms and services)\n",
      "ðŸ§  Tier 2 - LLM match: modelcontextprotocol/servers (confidence: 3/5, categories: Tools for building or deploying AI/ML models)\n",
      "ðŸ”„ Processing batch 4/9 (10 repositories)\n",
      "ðŸ§  Tier 2 - LLM match: musistudio/claude-code-router (confidence: 4/5, categories: Developer productivity tools, Tools for building or deploying AI/ML models)\n",
      "ðŸ§  Tier 2 - LLM match: NirDiamant/agents-towards-production (confidence: 4/5, categories: Tools for building or deploying AI/ML models, CI/CD and deployment pipelines)\n",
      "ðŸ§  Tier 2 - LLM match: awesome-selfhosted/awesome-selfhosted (confidence: 5/5, categories: Developer productivity tools)\n",
      "ðŸ§  Tier 2 - LLM match: opendatalab/MinerU (confidence: 5/5, categories: Developer productivity tools)\n",
      "ðŸ”„ Processing batch 5/9 (10 repositories)\n",
      "ðŸ§  Tier 2 - LLM match: stan-smith/FossFLOW (confidence: 4/5, categories: Developer productivity tools, Documentation tools)\n",
      "ðŸ§  Tier 2 - LLM match: GyulyVGC/sniffnet (confidence: 4/5, categories: Monitoring and observability tools, Security and compliance tools)\n",
      "ðŸ§  Tier 2 - LLM match: opencode-ai/opencode (confidence: 4/5, categories: Developer productivity tools)\n",
      "ðŸ§  Tier 2 - LLM match: czlonkowski/n8n-mcp (confidence: 3/5, categories: Developer productivity tools)\n",
      "ðŸ§  Tier 2 - LLM match: dockur/macos (confidence: 5/5, categories: Container technologies)\n",
      "ðŸ”„ Processing batch 6/9 (10 repositories)\n",
      "ðŸ§  Tier 2 - LLM match: Kilo-Org/kilocode (confidence: 4/5, categories: Developer productivity tools)\n",
      "ðŸ§  Tier 2 - LLM match: DavidHDev/react-bits (confidence: 4/5, categories: Developer productivity tools)\n",
      "ðŸ§  Tier 2 - LLM match: hesreallyhim/awesome-claude-code (confidence: 3/5, categories: Developer productivity tools)\n",
      "ðŸ”„ Processing batch 7/9 (10 repositories)\n",
      "ðŸ§  Tier 2 - LLM match: astral-sh/uv (confidence: 5/5, categories: Developer productivity tools, Build and package management tools)\n",
      "ðŸ§  Tier 2 - LLM match: BloopAI/vibe-kanban (confidence: 4/5, categories: Developer productivity tools)\n",
      "ðŸ§  Tier 2 - LLM match: infiniflow/ragflow (confidence: 4/5, categories: Tools for building or deploying AI/ML models, Developer productivity tools)\n",
      "ðŸ§  Tier 2 - LLM match: fosrl/pangolin (confidence: 4/5, categories: API gateways and service mesh, Security and compliance tools)\n",
      "ðŸ”„ Processing batch 8/9 (10 repositories)\n",
      "ðŸ§  Tier 2 - LLM match: ryoppippi/ccusage (confidence: 4/5, categories: Developer productivity tools)\n",
      "ðŸ§  Tier 2 - LLM match: eyaltoledano/claude-task-master (confidence: 4/5, categories: Developer productivity tools)\n",
      "ðŸ§  Tier 2 - LLM match: ollama/ollama (confidence: 3/5, categories: Developer productivity tools, Tools for building or deploying AI/ML models)\n",
      "ðŸ§  Tier 2 - LLM match: OpenPipe/ART (confidence: 4/5, categories: Tools for building or deploying AI/ML models)\n",
      "ðŸ§  Tier 2 - LLM match: enescingoz/awesome-n8n-templates (confidence: 4/5, categories: Developer productivity tools)\n",
      "ðŸ§  Tier 2 - LLM match: linshenkx/prompt-optimizer (confidence: 4/5, categories: Developer productivity tools)\n",
      "ðŸ”„ Processing batch 9/9 (5 repositories)\n",
      "ðŸ§  Tier 2 - LLM match: MrLesk/Backlog.md (confidence: 4/5, categories: Developer productivity tools)\n",
      "ðŸ§  Tier 2 - LLM match: vllm-project/vllm (confidence: 4/5, categories: Tools for building or deploying AI/ML models, Developer productivity tools)\n",
      "ðŸ§  Tier 2 - LLM match: GeeeekExplorer/nano-vllm (confidence: 3/5, categories: Tools for building or deploying AI/ML models, Developer productivity tools)\n",
      "ðŸ’° LLM classification cost: $0.1449\n",
      "\n",
      "ðŸ” Tier 3 - Keyword-based fallback...\n",
      "ðŸŽ¯ Tier 3 - Keyword match: davidkimai/Context-Engineering\n",
      "ðŸŽ¯ Tier 3 - Keyword match: hangwin/mcp-chrome\n",
      "\n",
      "ðŸ“Š 3-Tier Filtering Results:\n",
      "   â€¢ Tier 1 (Collections): 2\n",
      "   â€¢ Tier 2 (LLM): 35\n",
      "   â€¢ Tier 3 (Keywords): 2\n",
      "   â€¢ Total unique leads: 39\n",
      "\n",
      "ðŸ”¬ Phase 2 & 3: Analyzing 10 repositories...\n",
      "ðŸ“ Note: Analyzing first 10 repositories for demonstration\n",
      "ðŸ” Analyzing n8n-io/n8n...\n",
      "âœ… Found 5 community discussions for n8n-io/n8n\n",
      "âœ… Fetched README for n8n-io/n8n\n",
      "ðŸ”Ž Checking for 'CONTRIBUTING.md' in n8n...\n",
      "âœ… Found 'CONTRIBUTING.md' in n8n\n",
      "ðŸ”Ž Checking for 'CODE_OF_CONDUCT.md' in n8n...\n",
      "âœ… Found 'CODE_OF_CONDUCT.md' in n8n\n",
      "âœ… README analysis completed for n8n-io/n8n\n",
      "âœ… Sentiment analysis completed for n8n-io/n8n\n",
      "âœ… Completed analysis for n8n-io/n8n\n",
      "ðŸ” Analyzing langgenius/dify...\n",
      "âœ… Found 5 community discussions for langgenius/dify\n",
      "âœ… Fetched README for langgenius/dify\n",
      "ðŸ”Ž Checking for 'CONTRIBUTING.md' in dify...\n",
      "âœ… Found 'CONTRIBUTING.md' in dify\n",
      "ðŸ”Ž Checking for 'CODE_OF_CONDUCT.md' in dify...\n",
      "ðŸ”Ž Checking for 'code_of_conduct.md' in dify...\n",
      "ðŸ”Ž Checking for '.github/CODE_OF_CONDUCT.md' in dify...\n",
      "âœ… Found '.github/CODE_OF_CONDUCT.md' in dify\n",
      "âœ… Sentiment analysis completed for langgenius/dify\n",
      "âœ… README analysis completed for langgenius/dify\n",
      "âœ… Completed analysis for langgenius/dify\n",
      "ðŸ” Analyzing langflow-ai/langflow...\n",
      "âœ… Found 5 community discussions for langflow-ai/langflow\n",
      "âœ… Fetched README for langflow-ai/langflow\n",
      "ðŸ”Ž Checking for 'CONTRIBUTING.md' in langflow...\n",
      "âœ… Found 'CONTRIBUTING.md' in langflow\n",
      "ðŸ”Ž Checking for 'CODE_OF_CONDUCT.md' in langflow...\n",
      "âœ… Found 'CODE_OF_CONDUCT.md' in langflow\n",
      "âœ… README analysis completed for langflow-ai/langflow\n",
      "âœ… Sentiment analysis completed for langflow-ai/langflow\n",
      "âœ… Completed analysis for langflow-ai/langflow\n",
      "ðŸ” Analyzing Zie619/n8n-workflows...\n",
      "âœ… Found 5 community discussions for Zie619/n8n-workflows\n",
      "âœ… Fetched README for Zie619/n8n-workflows\n",
      "ðŸ”Ž Checking for 'CONTRIBUTING.md' in n8n-workflows...\n",
      "ðŸ”Ž Checking for 'contributing.md' in n8n-workflows...\n",
      "ðŸ”Ž Checking for '.github/CONTRIBUTING.md' in n8n-workflows...\n",
      "ðŸ”Ž Checking for '.github/contributing.md' in n8n-workflows...\n",
      "âŒ Could not fetch 'CONTRIBUTING' from n8n-workflows\n",
      "ðŸ”Ž Checking for 'CODE_OF_CONDUCT.md' in n8n-workflows...\n",
      "ðŸ”Ž Checking for 'code_of_conduct.md' in n8n-workflows...\n",
      "ðŸ”Ž Checking for '.github/CODE_OF_CONDUCT.md' in n8n-workflows...\n",
      "ðŸ”Ž Checking for '.github/code_of_conduct.md' in n8n-workflows...\n",
      "âŒ Could not fetch 'CODE_OF_CONDUCT' from n8n-workflows\n",
      "âœ… README analysis completed for Zie619/n8n-workflows\n",
      "âœ… Sentiment analysis completed for Zie619/n8n-workflows\n",
      "âœ… Completed analysis for Zie619/n8n-workflows\n",
      "ðŸ” Analyzing sst/opencode...\n",
      "âœ… Found 5 community discussions for sst/opencode\n",
      "âœ… Fetched README for sst/opencode\n",
      "ðŸ”Ž Checking for 'CONTRIBUTING.md' in opencode...\n",
      "ðŸ”Ž Checking for 'contributing.md' in opencode...\n",
      "ðŸ”Ž Checking for '.github/CONTRIBUTING.md' in opencode...\n",
      "ðŸ”Ž Checking for '.github/contributing.md' in opencode...\n",
      "âŒ Could not fetch 'CONTRIBUTING' from opencode\n",
      "ðŸ”Ž Checking for 'CODE_OF_CONDUCT.md' in opencode...\n",
      "ðŸ”Ž Checking for 'code_of_conduct.md' in opencode...\n",
      "ðŸ”Ž Checking for '.github/CODE_OF_CONDUCT.md' in opencode...\n",
      "ðŸ”Ž Checking for '.github/code_of_conduct.md' in opencode...\n",
      "âŒ Could not fetch 'CODE_OF_CONDUCT' from opencode\n",
      "âœ… README analysis completed for sst/opencode\n",
      "âœ… Sentiment analysis completed for sst/opencode\n",
      "âœ… Completed analysis for sst/opencode\n",
      "ðŸ” Analyzing SuperClaude-Org/SuperClaude_Framework...\n",
      "âœ… Found 5 community discussions for SuperClaude-Org/SuperClaude_Framework\n",
      "âœ… Fetched README for SuperClaude-Org/SuperClaude_Framework\n",
      "ðŸ”Ž Checking for 'CONTRIBUTING.md' in SuperClaude_Framework...\n",
      "âœ… Found 'CONTRIBUTING.md' in SuperClaude_Framework\n",
      "ðŸ”Ž Checking for 'CODE_OF_CONDUCT.md' in SuperClaude_Framework...\n",
      "âœ… Found 'CODE_OF_CONDUCT.md' in SuperClaude_Framework\n",
      "âœ… README analysis completed for SuperClaude-Org/SuperClaude_Framework\n",
      "âœ… Sentiment analysis completed for SuperClaude-Org/SuperClaude_Framework\n",
      "âœ… Completed analysis for SuperClaude-Org/SuperClaude_Framework\n",
      "ðŸ” Analyzing LibreSpark/LibreTV...\n",
      "âœ… Found 5 community discussions for LibreSpark/LibreTV\n",
      "âœ… Fetched README for LibreSpark/LibreTV\n",
      "ðŸ”Ž Checking for 'CONTRIBUTING.md' in LibreTV...\n",
      "âœ… Found 'CONTRIBUTING.md' in LibreTV\n",
      "ðŸ”Ž Checking for 'CODE_OF_CONDUCT.md' in LibreTV...\n",
      "ðŸ”Ž Checking for 'code_of_conduct.md' in LibreTV...\n",
      "ðŸ”Ž Checking for '.github/CODE_OF_CONDUCT.md' in LibreTV...\n",
      "ðŸ”Ž Checking for '.github/code_of_conduct.md' in LibreTV...\n",
      "âŒ Could not fetch 'CODE_OF_CONDUCT' from LibreTV\n",
      "âœ… README analysis completed for LibreSpark/LibreTV\n",
      "âœ… Sentiment analysis completed for LibreSpark/LibreTV\n",
      "âœ… Completed analysis for LibreSpark/LibreTV\n",
      "ðŸ” Analyzing ripienaar/free-for-dev...\n",
      "âœ… Found 5 community discussions for ripienaar/free-for-dev\n",
      "âœ… Fetched README for ripienaar/free-for-dev\n",
      "ðŸ”Ž Checking for 'CONTRIBUTING.md' in free-for-dev...\n",
      "ðŸ”Ž Checking for 'contributing.md' in free-for-dev...\n",
      "ðŸ”Ž Checking for '.github/CONTRIBUTING.md' in free-for-dev...\n",
      "ðŸ”Ž Checking for '.github/contributing.md' in free-for-dev...\n",
      "âŒ Could not fetch 'CONTRIBUTING' from free-for-dev\n",
      "ðŸ”Ž Checking for 'CODE_OF_CONDUCT.md' in free-for-dev...\n",
      "ðŸ”Ž Checking for 'code_of_conduct.md' in free-for-dev...\n",
      "ðŸ”Ž Checking for '.github/CODE_OF_CONDUCT.md' in free-for-dev...\n",
      "ðŸ”Ž Checking for '.github/code_of_conduct.md' in free-for-dev...\n",
      "âŒ Could not fetch 'CODE_OF_CONDUCT' from free-for-dev\n",
      "âœ… Sentiment analysis completed for ripienaar/free-for-dev\n",
      "âœ… README analysis completed for ripienaar/free-for-dev\n",
      "âœ… Completed analysis for ripienaar/free-for-dev\n",
      "ðŸ” Analyzing getAsterisk/claudia...\n",
      "âœ… Found 5 community discussions for getAsterisk/claudia\n",
      "âœ… Fetched README for getAsterisk/claudia\n",
      "ðŸ”Ž Checking for 'CONTRIBUTING.md' in claudia...\n",
      "âœ… Found 'CONTRIBUTING.md' in claudia\n",
      "ðŸ”Ž Checking for 'CODE_OF_CONDUCT.md' in claudia...\n",
      "ðŸ”Ž Checking for 'code_of_conduct.md' in claudia...\n",
      "ðŸ”Ž Checking for '.github/CODE_OF_CONDUCT.md' in claudia...\n",
      "ðŸ”Ž Checking for '.github/code_of_conduct.md' in claudia...\n",
      "âŒ Could not fetch 'CODE_OF_CONDUCT' from claudia\n",
      "âœ… README analysis completed for getAsterisk/claudia\n",
      "âœ… Sentiment analysis completed for getAsterisk/claudia\n",
      "âœ… Completed analysis for getAsterisk/claudia\n",
      "ðŸ” Analyzing upstash/context7...\n",
      "âœ… Found 5 community discussions for upstash/context7\n",
      "âœ… Fetched README for upstash/context7\n",
      "ðŸ”Ž Checking for 'CONTRIBUTING.md' in context7...\n",
      "ðŸ”Ž Checking for 'contributing.md' in context7...\n",
      "ðŸ”Ž Checking for '.github/CONTRIBUTING.md' in context7...\n",
      "ðŸ”Ž Checking for '.github/contributing.md' in context7...\n",
      "âŒ Could not fetch 'CONTRIBUTING' from context7\n",
      "ðŸ”Ž Checking for 'CODE_OF_CONDUCT.md' in context7...\n",
      "ðŸ”Ž Checking for 'code_of_conduct.md' in context7...\n",
      "ðŸ”Ž Checking for '.github/CODE_OF_CONDUCT.md' in context7...\n",
      "ðŸ”Ž Checking for '.github/code_of_conduct.md' in context7...\n",
      "âŒ Could not fetch 'CODE_OF_CONDUCT' from context7\n",
      "âœ… README analysis completed for upstash/context7\n",
      "âœ… Sentiment analysis completed for upstash/context7\n",
      "âœ… Completed analysis for upstash/context7\n",
      "\n",
      "ðŸ Phase 4: Scoring and Reporting...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ðŸ“ˆ FINAL VC INVESTMENT REPORT\n",
      "================================================================================\n",
      "\n",
      "ðŸ† RANK #1\n",
      "\n",
      "## n8n-io/n8n | ðŸ“Š Score: 93.02\n",
      "ðŸ“ˆ Total Score: 34714.0041 | â­ Stars: 7482\n",
      "ðŸ“ Description: Fair-code workflow automation platform with native AI capabilities. Combine visual building with custom code, self-host or cloud, 400+ integrations.\n",
      "ðŸ“– Problem Solved: It removes the friction of building and managing complex integrations by providing a self-hostable, code-extensible no-code platform with hundreds of prebuilt connectors.\n",
      "ðŸŽ¯ Problem Statement Clarity: 5/5\n",
      "âš¡ Ease of Adoption: 5/5\n",
      "ðŸ’Š Project Maturity & Community Health: 5/5\n",
      "ðŸ˜Š Community Sentiment: 4/5\n",
      "ðŸ“ˆ Problem Solution Fit: 5/5\n",
      "ðŸ’Š Credibility & Adoption: 4/5\n",
      "ðŸ’¬ Key Praise Quote: It took 20 minutes to have a workflow which runs every hour that calls Miniflux to get my RSS feed data, Mealie to get my recipes, and then upload those files to Dropboxâ€”complete with logging and monitoring out of the box.\n",
      "âš ï¸ Main Criticism: The source-available licenseâ€™s commercial-use restrictions and lack of true open-source status undermine trust and hinder broader adoption.\n",
      "\n",
      "ðŸ† RANK #2\n",
      "\n",
      "## langflow-ai/langflow | ðŸ“Š Score: 79.34\n",
      "ðŸ“ˆ Total Score: 27782.9219 | â­ Stars: 7787\n",
      "ðŸ“ Description: Langflow is a powerful tool for building and deploying AI-powered agents and workflows.\n",
      "ðŸ“– Problem Solved: Langflow eliminates the complexity of building and deploying AI-powered workflows by offering an intuitive visual builder and turnkey integration with major LLMs and tools.\n",
      "ðŸŽ¯ Problem Statement Clarity: 4/5\n",
      "âš¡ Ease of Adoption: 5/5\n",
      "ðŸ’Š Project Maturity & Community Health: 5/5\n",
      "ðŸ˜Š Community Sentiment: 3/5\n",
      "ðŸ“ˆ Problem Solution Fit: 4/5\n",
      "ðŸ’Š Credibility & Adoption: 3/5\n",
      "ðŸ’¬ Key Praise Quote: Langflow is as close as it gets to no code + agentic.\n",
      "âš ï¸ Main Criticism: Documentation is sparse and the tool can feel immature and buggy, requiring custom workarounds.\n",
      "\n",
      "ðŸ† RANK #3\n",
      "\n",
      "## Zie619/n8n-workflows | ðŸ“Š Score: 78.73\n",
      "ðŸ“ˆ Total Score: 27476.6953 | â­ Stars: 6630\n",
      "ðŸ“ Description: all of the workflows of n8n i could find (also from the site itself)\n",
      "ðŸ“– Problem Solved: It solves the slow, bulky, and hard-to-search nature of large n8n workflow collections by providing a high-performance, instantly searchable documentation system.\n",
      "ðŸŽ¯ Problem Statement Clarity: 3/5\n",
      "âš¡ Ease of Adoption: 5/5\n",
      "ðŸ’Š Project Maturity & Community Health: 1/5\n",
      "ðŸ˜Š Community Sentiment: 4/5\n",
      "ðŸ“ˆ Problem Solution Fit: 5/5\n",
      "ðŸ’Š Credibility & Adoption: 5/5\n",
      "ðŸ’¬ Key Praise Quote: Weâ€™ve been using N8N for about 2 years, at scale, in production, self-hosted on a VM with docker compose. Itâ€™s phenomenal.\n",
      "âš ï¸ Main Criticism: Its source-available license restricts commercial use and forces users to pay steep fees for production deployments.\n",
      "\n",
      "ðŸ† RANK #4\n",
      "\n",
      "## sst/opencode | ðŸ“Š Score: 77.06\n",
      "ðŸ“ˆ Total Score: 21944.1901 | â­ Stars: 6247\n",
      "ðŸ“ Description: AI coding agent, built for the terminal.\n",
      "ðŸ“– Problem Solved: It addresses the pain of context-switching to external web-based AI coding tools by offering a fully open-source, terminal-based AI coding assistant.\n",
      "ðŸŽ¯ Problem Statement Clarity: 3/5\n",
      "âš¡ Ease of Adoption: 5/5\n",
      "ðŸ’Š Project Maturity & Community Health: 3/5\n",
      "ðŸ˜Š Community Sentiment: 4/5\n",
      "ðŸ“ˆ Problem Solution Fit: 4/5\n",
      "ðŸ’Š Credibility & Adoption: 4/5\n",
      "ðŸ’¬ Key Praise Quote: It was the first time I felt like I could write up a large prompt, walk away from my laptop, and come back to a lot of work having been done.\n",
      "âš ï¸ Main Criticism: Users praise its agentic capabilities but most criticize its immature feature setâ€”missing documentation, opaque contextâ€window management, and limited control over what the agent sees and does.\n",
      "\n",
      "ðŸ† RANK #5\n",
      "\n",
      "## langgenius/dify | ðŸ“Š Score: 74.13\n",
      "ðŸ“ˆ Total Score: 9935.9114 | â­ Stars: 2439\n",
      "ðŸ“ Description: Production-ready platform for agentic workflow development.\n",
      "ðŸ“– Problem Solved: It solves the headache of fragmented, labor-intensive development and deployment of LLM applications by providing a unified, self-hostable platform with visual workflows, RAG pipelines, agent tooling, model management, and observability features.\n",
      "ðŸŽ¯ Problem Statement Clarity: 3/5\n",
      "âš¡ Ease of Adoption: 5/5\n",
      "ðŸ’Š Project Maturity & Community Health: 5/5\n",
      "ðŸ˜Š Community Sentiment: 3/5\n",
      "ðŸ“ˆ Problem Solution Fit: 4/5\n",
      "ðŸ’Š Credibility & Adoption: 3/5\n",
      "ðŸ’¬ Key Praise Quote: This is the easiest one out there to use imo. Has UI. Easy docker install. You can be up in minutes.\n",
      "âš ï¸ Main Criticism: Users report that Difyâ€™s configuration is convoluted and its documentation sparse, complicating customization and non-default setups.\n",
      "\n",
      "ðŸ† RANK #6\n",
      "\n",
      "## getAsterisk/claudia | ðŸ“Š Score: 73.74\n",
      "ðŸ“ˆ Total Score: 18201.7106 | â­ Stars: 5273\n",
      "ðŸ“ Description: A powerful GUI app and Toolkit for Claude Code - Create custom agents, manage interactive Claude Code sessions, run secure background agents, and more.\n",
      "ðŸ“– Problem Solved: It eliminates the friction of using the Claude Code CLI by providing an intuitive GUI to manage sessions, custom agents, and usage analytics.\n",
      "ðŸŽ¯ Problem Statement Clarity: 4/5\n",
      "âš¡ Ease of Adoption: 3/5\n",
      "ðŸ’Š Project Maturity & Community Health: 3/5\n",
      "ðŸ˜Š Community Sentiment: 4/5\n",
      "ðŸ“ˆ Problem Solution Fit: 4/5\n",
      "ðŸ’Š Credibility & Adoption: 4/5\n",
      "ðŸ’¬ Key Praise Quote: It's amazing this project is still alive and kicking â€“ Asterisk allowed us to completely automate that process, thus reducing the work of our callcenter tremendously and saving incredible amounts of money.\n",
      "âš ï¸ Main Criticism: Asteriskâ€™s dialplan and configuration system is notoriously complex and painful to work with, driving some users toward simpler or newer alternatives.\n",
      "\n",
      "ðŸ† RANK #7\n",
      "\n",
      "## SuperClaude-Org/SuperClaude_Framework | ðŸ“Š Score: 72.88\n",
      "ðŸ“ˆ Total Score: 20859.8127 | â­ Stars: 5611\n",
      "ðŸ“ Description: A configuration framework that enhances Claude Code with specialized commands, cognitive personas, and development methodologies.\n",
      "ðŸ“– Problem Solved: It eliminates the pain of generic AI coding responses by providing specialized commands, expert personas, and integrations to streamline development workflows in Claude Code.\n",
      "ðŸŽ¯ Problem Statement Clarity: 3/5\n",
      "âš¡ Ease of Adoption: 4/5\n",
      "ðŸ’Š Project Maturity & Community Health: 5/5\n",
      "ðŸ˜Š Community Sentiment: 3/5\n",
      "ðŸ“ˆ Problem Solution Fit: 4/5\n",
      "ðŸ’Š Credibility & Adoption: 3/5\n",
      "ðŸ’¬ Key Praise Quote: â€œClaude Code is intentionally low-level and unopinionated, providing close to raw model access without forcing specific workflows. This design philosophy creates a flexible, customizable, scriptable, and safe power tool.â€\n",
      "âš ï¸ Main Criticism: The frameworkâ€™s low-level, unopinionated design comes with a steep learning curve that can slow down engineers new to agentic coding tools.\n",
      "\n",
      "ðŸ† RANK #8\n",
      "\n",
      "## LibreSpark/LibreTV | ðŸ“Š Score: 62.92\n",
      "ðŸ“ˆ Total Score: 20179.5674 | â­ Stars: 1777\n",
      "ðŸ“ Description: ä¸€åˆ†é’Ÿæ­å»ºå½±è§†ç«™ï¼Œæ”¯æŒVercel/Dockerç­‰éƒ¨ç½²æ–¹å¼\n",
      "ðŸ“– Problem Solved: Users often face fragmented, ad-laden video services requiring registration and subscriptions; LibreTV offers a lightweight, free, no-signup, one-click-deploy solution to search and stream videos from multiple sources.\n",
      "ðŸŽ¯ Problem Statement Clarity: 3/5\n",
      "âš¡ Ease of Adoption: 5/5\n",
      "ðŸ’Š Project Maturity & Community Health: 3/5\n",
      "ðŸ˜Š Community Sentiment: 3/5\n",
      "ðŸ“ˆ Problem Solution Fit: 3/5\n",
      "ðŸ’Š Credibility & Adoption: 2/5\n",
      "ðŸ’¬ Key Praise Quote: No, it's not the slickest interface. And no, it's not the most full-featured. But it doesn't spy on me and it doesn't try to show me ads either. That's a win!\n",
      "âš ï¸ Main Criticism: Critics argue the laptop-strapped hack is overkill and that a small, quiet mini-PC or Raspberry Pi on a VESA mount would achieve the same ad-free, privacy-focused setup more cleanly and affordably.\n",
      "\n",
      "ðŸ† RANK #9\n",
      "\n",
      "## upstash/context7 | ðŸ“Š Score: 62.33\n",
      "ðŸ“ˆ Total Score: 15506.3525 | â­ Stars: 4343\n",
      "ðŸ“ Description: Context7 MCP Server -- Up-to-date code documentation for LLMs and AI code editors\n",
      "ðŸ“– Problem Solved: It ensures LLMs receive accurate, up-to-date code documentation and examples in prompts, eliminating outdated or hallucinated API information.\n",
      "ðŸŽ¯ Problem Statement Clarity: 5/5\n",
      "âš¡ Ease of Adoption: 5/5\n",
      "ðŸ’Š Project Maturity & Community Health: 1/5\n",
      "ðŸ˜Š Community Sentiment: 3/5\n",
      "ðŸ“ˆ Problem Solution Fit: 3/5\n",
      "ðŸ’Š Credibility & Adoption: 2/5\n",
      "ðŸ’¬ Key Praise Quote: Just this week I came across the Context7 mcp and have since been using it together with Copilot with great results.\n",
      "âš ï¸ Main Criticism: Critics note that Context7 is neither fully open-source nor free and has limited integration support (e.g., only works in agent mode), making free alternatives like GitMCP more appealing.\n",
      "\n",
      "ðŸ† RANK #10\n",
      "\n",
      "## ripienaar/free-for-dev | ðŸ“Š Score: 58.17\n",
      "ðŸ“ˆ Total Score: 19261.2312 | â­ Stars: 5159\n",
      "ðŸ“ Description: A list of SaaS, PaaS and IaaS offerings that have free tiers of interest to devops and infradev\n",
      "ðŸ“– Problem Solved: It solves the time-consuming challenge of discovering and evaluating free-tier SaaS, PaaS, and IaaS services relevant to DevOps and infrastructure developers.\n",
      "ðŸŽ¯ Problem Statement Clarity: 5/5\n",
      "âš¡ Ease of Adoption: 1/5\n",
      "ðŸ’Š Project Maturity & Community Health: 1/5\n",
      "ðŸ˜Š Community Sentiment: 3/5\n",
      "ðŸ“ˆ Problem Solution Fit: 4/5\n",
      "ðŸ’Š Credibility & Adoption: 3/5\n",
      "ðŸ’¬ Key Praise Quote: This is incredible. Thank you!\n",
      "âš ï¸ Main Criticism: The list doesnâ€™t indicate which free tiers require credit cards and some worry providers like Oracle may pull a bait-and-switch.\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š PIPELINE STATISTICS\n",
      "================================================================================\n",
      "ðŸ”¢ Total Repositories Analyzed: 10\n",
      "âœ… Successful Repo Rubric Analyses: 10\n",
      "ðŸ˜Š Successful Community Sentiment Analyses: 10\n",
      "ðŸ’° Analysis Completeness: 10/10 repositories fully analyzed\n",
      "\n",
      "ðŸ’¸ Total Analysis Cost: $0.6096\n",
      "ðŸ”¢ Total Tokens Used: 451,333\n",
      "\n",
      "ðŸ’¾ Phase 5: Uploading results to DynamoDB...\n",
      "ðŸ“¤ Uploading 10 analysis results to DynamoDB...\n",
      "âœ… Uploaded: n8n-io/n8n (Score: 93.02)\n",
      "âœ… Uploaded: langgenius/dify (Score: 74.13)\n",
      "âœ… Uploaded: langflow-ai/langflow (Score: 79.34)\n",
      "âœ… Uploaded: Zie619/n8n-workflows (Score: 78.73)\n",
      "âœ… Uploaded: sst/opencode (Score: 77.06)\n",
      "âœ… Uploaded: SuperClaude-Org/SuperClaude_Framework (Score: 72.88)\n",
      "âœ… Uploaded: LibreSpark/LibreTV (Score: 62.92)\n",
      "âœ… Uploaded: ripienaar/free-for-dev (Score: 58.17)\n",
      "âœ… Uploaded: getAsterisk/claudia (Score: 73.74)\n",
      "âœ… Uploaded: upstash/context7 (Score: 62.33)\n",
      "\n",
      "ðŸ“Š Upload Summary:\n",
      "   âœ… Successful: 10\n",
      "   âŒ Failed: 0\n",
      "   ðŸ“‹ Total: 10\n",
      "âœ… All results uploaded successfully to 'vc-sourcing-analysis'\n",
      "\n",
      "ðŸ” Querying top results from DynamoDB...\n",
      "\n",
      "ðŸ† TOP REPOSITORIES FROM DYNAMODB:\n",
      "============================================================\n",
      "\n",
      "#1 - n8n-io/n8n\n",
      "   ðŸ“Š Score: 93.02\n",
      "   ðŸ“… Analyzed: 2025-07-20\n",
      "   ðŸ“– Problem Clarity: 5/5\n",
      "   âš¡ Adoption Ease: 5/5\n",
      "   ðŸ’Š Maturity: 5/5\n",
      "   ðŸ˜Š Community Excitement: 4/5\n",
      "   ðŸŽ¯ Problem-Solution Fit: 5/5\n",
      "\n",
      "#2 - langflow-ai/langflow\n",
      "   ðŸ“Š Score: 79.34\n",
      "   ðŸ“… Analyzed: 2025-07-20\n",
      "   ðŸ“– Problem Clarity: 4/5\n",
      "   âš¡ Adoption Ease: 5/5\n",
      "   ðŸ’Š Maturity: 5/5\n",
      "   ðŸ˜Š Community Excitement: 3/5\n",
      "   ðŸŽ¯ Problem-Solution Fit: 4/5\n",
      "\n",
      "#3 - Zie619/n8n-workflows\n",
      "   ðŸ“Š Score: 78.73\n",
      "   ðŸ“… Analyzed: 2025-07-20\n",
      "   ðŸ“– Problem Clarity: 3/5\n",
      "   âš¡ Adoption Ease: 5/5\n",
      "   ðŸ’Š Maturity: 1/5\n",
      "   ðŸ˜Š Community Excitement: 4/5\n",
      "   ðŸŽ¯ Problem-Solution Fit: 5/5\n",
      "\n",
      "#4 - sst/opencode\n",
      "   ðŸ“Š Score: 77.06\n",
      "   ðŸ“… Analyzed: 2025-07-20\n",
      "   ðŸ“– Problem Clarity: 3/5\n",
      "   âš¡ Adoption Ease: 5/5\n",
      "   ðŸ’Š Maturity: 3/5\n",
      "   ðŸ˜Š Community Excitement: 4/5\n",
      "   ðŸŽ¯ Problem-Solution Fit: 4/5\n",
      "\n",
      "#5 - langgenius/dify\n",
      "   ðŸ“Š Score: 74.13\n",
      "   ðŸ“… Analyzed: 2025-07-20\n",
      "   ðŸ“– Problem Clarity: 3/5\n",
      "   âš¡ Adoption Ease: 5/5\n",
      "   ðŸ’Š Maturity: 5/5\n",
      "   ðŸ˜Š Community Excitement: 3/5\n",
      "   ðŸŽ¯ Problem-Solution Fit: 4/5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[FinalAnalysis(repo_name='n8n-io/n8n', oss_insight_data={'repo_id': '193215554', 'repo_name': 'n8n-io/n8n', 'primary_language': 'TypeScript', 'description': 'Fair-code workflow automation platform with native AI capabilities. Combine visual building with custom code, self-host or cloud, 400+ integrations.', 'stars': '7482', 'forks': '3321', 'pull_requests': '242', 'pushes': '1001', 'total_score': '34714.0041', 'contributor_logins': 'shortstacked,tomi,ivov,mutdmour,autologie', 'collection_names': 'Low Code Development Tool,Zapier Alternatives'}, repo_rubric_analysis=RepoRubricAnalysis(problem_clarity_score=5, adoption_ease_score=5, maturity_health_score=5, problem_solved='It removes the friction of building and managing complex integrations by providing a self-hostable, code-extensible no-code platform with hundreds of prebuilt connectors.'), community_sentiment_analysis=CommunitySentimentAnalysis(excitement_score=4, problem_solution_fit_score=5, credibility_adoption_score=4, key_praise_quote='It took 20 minutes to have a workflow which runs every hour that calls Miniflux to get my RSS feed data, Mealie to get my recipes, and then upload those files to Dropboxâ€”complete with logging and monitoring out of the box.', main_criticism='The source-available licenseâ€™s commercial-use restrictions and lack of true open-source status undermine trust and hinder broader adoption.'), score=93.02),\n",
       " FinalAnalysis(repo_name='langgenius/dify', oss_insight_data={'repo_id': '626805178', 'repo_name': 'langgenius/dify', 'primary_language': 'TypeScript', 'description': 'Production-ready platform for agentic workflow development.', 'stars': '2439', 'forks': '496', 'pull_requests': '237', 'pushes': '674', 'total_score': '9935.9114', 'contributor_logins': 'crazywoola,WTW0313,iamjoel,laipz8200,Copilot', 'collection_names': 'LLM Tools'}, repo_rubric_analysis=RepoRubricAnalysis(problem_clarity_score=3, adoption_ease_score=5, maturity_health_score=5, problem_solved='It solves the headache of fragmented, labor-intensive development and deployment of LLM applications by providing a unified, self-hostable platform with visual workflows, RAG pipelines, agent tooling, model management, and observability features.'), community_sentiment_analysis=CommunitySentimentAnalysis(excitement_score=3, problem_solution_fit_score=4, credibility_adoption_score=3, key_praise_quote='This is the easiest one out there to use imo. Has UI. Easy docker install. You can be up in minutes.', main_criticism='Users report that Difyâ€™s configuration is convoluted and its documentation sparse, complicating customization and non-default setups.'), score=74.13),\n",
       " FinalAnalysis(repo_name='langflow-ai/langflow', oss_insight_data={'repo_id': '599320067', 'repo_name': 'langflow-ai/langflow', 'primary_language': 'Python', 'description': 'Langflow is a powerful tool for building and deploying AI-powered agents and workflows.', 'stars': '7787', 'forks': '192', 'pull_requests': '174', 'pushes': '760', 'total_score': '27782.9219', 'contributor_logins': 'mendonk,ogabrielluiz,edwinjosechittilappilly,aimurphy,erichare', 'collection_names': 'ChatGPT Apps'}, repo_rubric_analysis=RepoRubricAnalysis(problem_clarity_score=4, adoption_ease_score=5, maturity_health_score=5, problem_solved='Langflow eliminates the complexity of building and deploying AI-powered workflows by offering an intuitive visual builder and turnkey integration with major LLMs and tools.'), community_sentiment_analysis=CommunitySentimentAnalysis(excitement_score=3, problem_solution_fit_score=4, credibility_adoption_score=3, key_praise_quote='Langflow is as close as it gets to no code + agentic.', main_criticism='Documentation is sparse and the tool can feel immature and buggy, requiring custom workarounds.'), score=79.34),\n",
       " FinalAnalysis(repo_name='Zie619/n8n-workflows', oss_insight_data={'repo_id': '983345476', 'repo_name': 'Zie619/n8n-workflows', 'primary_language': 'HTML', 'description': 'all of the workflows of n8n i could find (also from the site itself)', 'stars': '6630', 'forks': '2114', 'pull_requests': '13', 'pushes': '8', 'total_score': '27476.6953', 'contributor_logins': 'Zie619,EnricoLibutti,console-1,sapiensenpai,abd9777', 'collection_names': ''}, repo_rubric_analysis=RepoRubricAnalysis(problem_clarity_score=3, adoption_ease_score=5, maturity_health_score=1, problem_solved='It solves the slow, bulky, and hard-to-search nature of large n8n workflow collections by providing a high-performance, instantly searchable documentation system.'), community_sentiment_analysis=CommunitySentimentAnalysis(excitement_score=4, problem_solution_fit_score=5, credibility_adoption_score=5, key_praise_quote='Weâ€™ve been using N8N for about 2 years, at scale, in production, self-hosted on a VM with docker compose. Itâ€™s phenomenal.', main_criticism='Its source-available license restricts commercial use and forces users to pay steep fees for production deployments.'), score=78.73),\n",
       " FinalAnalysis(repo_name='sst/opencode', oss_insight_data={'repo_id': '975734319', 'repo_name': 'sst/opencode', 'primary_language': 'Go', 'description': 'AI coding agent, built for the terminal.', 'stars': '6247', 'forks': '384', 'pull_requests': '167', 'pushes': '395', 'total_score': '21944.1901', 'contributor_logins': 'thdxr,adamdotdevin,jayair,rekram1-node,timoclsn', 'collection_names': ''}, repo_rubric_analysis=RepoRubricAnalysis(problem_clarity_score=3, adoption_ease_score=5, maturity_health_score=3, problem_solved='It addresses the pain of context-switching to external web-based AI coding tools by offering a fully open-source, terminal-based AI coding assistant.'), community_sentiment_analysis=CommunitySentimentAnalysis(excitement_score=4, problem_solution_fit_score=4, credibility_adoption_score=4, key_praise_quote='It was the first time I felt like I could write up a large prompt, walk away from my laptop, and come back to a lot of work having been done.', main_criticism='Users praise its agentic capabilities but most criticize its immature feature setâ€”missing documentation, opaque contextâ€window management, and limited control over what the agent sees and does.'), score=77.06),\n",
       " FinalAnalysis(repo_name='SuperClaude-Org/SuperClaude_Framework', oss_insight_data={'repo_id': '1006496207', 'repo_name': 'SuperClaude-Org/SuperClaude_Framework', 'primary_language': 'Python', 'description': 'A configuration framework that enhances Claude Code with specialized commands, cognitive personas, and development methodologies.', 'stars': '5611', 'forks': '524', 'pull_requests': '31', 'pushes': '66', 'total_score': '20859.8127', 'contributor_logins': 'NomenAK,mithun50,lmontanares,AlwaysBluer,lightningRalf', 'collection_names': ''}, repo_rubric_analysis=RepoRubricAnalysis(problem_clarity_score=3, adoption_ease_score=4, maturity_health_score=5, problem_solved='It eliminates the pain of generic AI coding responses by providing specialized commands, expert personas, and integrations to streamline development workflows in Claude Code.'), community_sentiment_analysis=CommunitySentimentAnalysis(excitement_score=3, problem_solution_fit_score=4, credibility_adoption_score=3, key_praise_quote='â€œClaude Code is intentionally low-level and unopinionated, providing close to raw model access without forcing specific workflows. This design philosophy creates a flexible, customizable, scriptable, and safe power tool.â€', main_criticism='The frameworkâ€™s low-level, unopinionated design comes with a steep learning curve that can slow down engineers new to agentic coding tools.'), score=72.88),\n",
       " FinalAnalysis(repo_name='LibreSpark/LibreTV', oss_insight_data={'repo_id': '961357433', 'repo_name': 'LibreSpark/LibreTV', 'primary_language': 'JavaScript', 'description': 'ä¸€åˆ†é’Ÿæ­å»ºå½±è§†ç«™ï¼Œæ”¯æŒVercel/Dockerç­‰éƒ¨ç½²æ–¹å¼', 'stars': '1777', 'forks': '5263', 'pull_requests': '34', 'pushes': '6', 'total_score': '20179.5674', 'contributor_logins': 'bestZwei,fuguangshuai,jiankeyan,mmm0926493492,zmw0823', 'collection_names': ''}, repo_rubric_analysis=RepoRubricAnalysis(problem_clarity_score=3, adoption_ease_score=5, maturity_health_score=3, problem_solved='Users often face fragmented, ad-laden video services requiring registration and subscriptions; LibreTV offers a lightweight, free, no-signup, one-click-deploy solution to search and stream videos from multiple sources.'), community_sentiment_analysis=CommunitySentimentAnalysis(excitement_score=3, problem_solution_fit_score=3, credibility_adoption_score=2, key_praise_quote=\"No, it's not the slickest interface. And no, it's not the most full-featured. But it doesn't spy on me and it doesn't try to show me ads either. That's a win!\", main_criticism='Critics argue the laptop-strapped hack is overkill and that a small, quiet mini-PC or Raspberry Pi on a VESA mount would achieve the same ad-free, privacy-focused setup more cleanly and affordably.'), score=62.92),\n",
       " FinalAnalysis(repo_name='ripienaar/free-for-dev', oss_insight_data={'repo_id': '32484381', 'repo_name': 'ripienaar/free-for-dev', 'primary_language': 'HTML', 'description': 'A list of SaaS, PaaS and IaaS offerings that have free tiers of interest to devops and infradev', 'stars': '5159', 'forks': '287', 'pull_requests': '17', 'pushes': '10', 'total_score': '19261.2312', 'contributor_logins': 'ripienaar,iammuhammedabdAllah,rajnishsubedi0,gniting,IDKn1', 'collection_names': ''}, repo_rubric_analysis=RepoRubricAnalysis(problem_clarity_score=5, adoption_ease_score=1, maturity_health_score=1, problem_solved='It solves the time-consuming challenge of discovering and evaluating free-tier SaaS, PaaS, and IaaS services relevant to DevOps and infrastructure developers.'), community_sentiment_analysis=CommunitySentimentAnalysis(excitement_score=3, problem_solution_fit_score=4, credibility_adoption_score=3, key_praise_quote='This is incredible. Thank you!', main_criticism='The list doesnâ€™t indicate which free tiers require credit cards and some worry providers like Oracle may pull a bait-and-switch.'), score=58.17),\n",
       " FinalAnalysis(repo_name='getAsterisk/claudia', oss_insight_data={'repo_id': '1004995740', 'repo_name': 'getAsterisk/claudia', 'primary_language': 'TypeScript', 'description': 'A powerful GUI app and Toolkit for Claude Code - Create custom agents, manage interactive Claude Code sessions, run secure background agents, and more.', 'stars': '5273', 'forks': '416', 'pull_requests': '41', 'pushes': '40', 'total_score': '18201.7106', 'contributor_logins': 'mufeedvh,123vivekr,Copilot,ursisterbtw,alexiokay', 'collection_names': ''}, repo_rubric_analysis=RepoRubricAnalysis(problem_clarity_score=4, adoption_ease_score=3, maturity_health_score=3, problem_solved='It eliminates the friction of using the Claude Code CLI by providing an intuitive GUI to manage sessions, custom agents, and usage analytics.'), community_sentiment_analysis=CommunitySentimentAnalysis(excitement_score=4, problem_solution_fit_score=4, credibility_adoption_score=4, key_praise_quote=\"It's amazing this project is still alive and kicking â€“ Asterisk allowed us to completely automate that process, thus reducing the work of our callcenter tremendously and saving incredible amounts of money.\", main_criticism='Asteriskâ€™s dialplan and configuration system is notoriously complex and painful to work with, driving some users toward simpler or newer alternatives.'), score=73.74),\n",
       " FinalAnalysis(repo_name='upstash/context7', oss_insight_data={'repo_id': '955620917', 'repo_name': 'upstash/context7', 'primary_language': 'JavaScript', 'description': 'Context7 MCP Server -- Up-to-date code documentation for LLMs and AI code editors', 'stars': '4343', 'forks': '230', 'pull_requests': '14', 'pushes': '19', 'total_score': '15506.3525', 'contributor_logins': 'enesgules,buggyhunter,ooopus,ruslanlap,yu-iskw', 'collection_names': ''}, repo_rubric_analysis=RepoRubricAnalysis(problem_clarity_score=5, adoption_ease_score=5, maturity_health_score=1, problem_solved='It ensures LLMs receive accurate, up-to-date code documentation and examples in prompts, eliminating outdated or hallucinated API information.'), community_sentiment_analysis=CommunitySentimentAnalysis(excitement_score=3, problem_solution_fit_score=3, credibility_adoption_score=2, key_praise_quote='Just this week I came across the Context7 mcp and have since been using it together with Copilot with great results.', main_criticism='Critics note that Context7 is neither fully open-source nor free and has limited integration support (e.g., only works in agent mode), making free alternatives like GitMCP more appealing.'), score=62.33)]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await run_vc_sourcing_pipeline_with_dynamodb()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oss_sourcer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
