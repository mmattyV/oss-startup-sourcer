{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea038603",
   "metadata": {},
   "source": [
    "# Script to analyze GitHub trend data and upload results to Dynamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4a51efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ VC Sourcing Pipeline initialized successfully\n",
      "üîë API clients configured for OpenAI, Exa, and GitHub\n",
      "üìä Ready to analyze DevOps repositories across 4 time periods\n"
     ]
    }
   ],
   "source": [
    "# VC Sourcing Pipeline - Complete Setup and Configuration\n",
    "import aiohttp  # type: ignore\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import traceback\n",
    "from typing import Dict, List, Optional, Any\n",
    "from dotenv import load_dotenv  # type: ignore\n",
    "from openai import AsyncOpenAI, ChatCompletion  # type: ignore\n",
    "from pydantic import BaseModel  # type: ignore\n",
    "from exa_py import Exa  # type: ignore\n",
    "from github import Github  # type: ignore\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API Configuration\n",
    "BASE_OSS_INSIGHT_URL = \"https://api.ossinsight.io/v1\"\n",
    "TRENDING_REPOS_URL = f\"{BASE_OSS_INSIGHT_URL}/trends/repos\"\n",
    "COLLECTIONS_URL = f\"{BASE_OSS_INSIGHT_URL}/collections\"\n",
    "PERIOD_VALUES = [\"past_24_hours\", \"past_week\", \"past_month\", \"past_3_months\"]\n",
    "\n",
    "# Initialize API Clients\n",
    "openai_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_PROJECT_KEY\"))\n",
    "exa_client = Exa(api_key=os.getenv(\"EXA_API_KEY\"))\n",
    "github_client = Github(os.getenv(\"GITHUB_TOKEN\"))\n",
    "\n",
    "# LLM Model Configuration\n",
    "REASONING_MODELS = [\n",
    "    \"o1\", \"o1-2024-12-17\", \"o3-mini-2025-01-31\", \"o3-mini\",\n",
    "    \"o4-mini\", \"o4-mini-2025-04-16\", \"o3\", \"o3-2025-04-16\"\n",
    "]\n",
    "\n",
    "LLM_MODEL_COSTS = {\n",
    "    \"o3\": {\"input\": 2 / 10 ** 6, \"cached_input\": 0.6 / 10 ** 6, \"output\": 8 / 10 ** 6},\n",
    "    \"o3-2025-04-16\": {\"input\": 10.0 / 10 ** 6, \"cached_input\": 2.5 / 10 ** 6, \"output\": 40.0 / 10 ** 6},\n",
    "    \"o4-mini\": {\"input\": 1.1 / 10 ** 6, \"cached_input\": 0.275 / 10 ** 6, \"output\": 4.4 / 10 ** 6},\n",
    "    \"o4-mini-2025-04-16\": {\"input\": 1.1 / 10 ** 6, \"cached_input\": 0.275 / 10 ** 6, \"output\": 4.4 / 10 ** 6},\n",
    "    \"gpt-4.1-mini\": {\"input\": 0.40 / 10 ** 6, \"cached_input\": 0.10 / 10 ** 6, \"output\": 1.60 / 10 ** 6},\n",
    "    \"gpt-4.1\": {\"input\": 2.00 / 10 ** 6, \"cached_input\": 0.50 / 10 ** 6, \"output\": 8.00 / 10 ** 6},\n",
    "    \"gpt-4.1-nano\": {\"input\": 0.10 / 10 ** 6, \"cached_input\": 0.025 / 10 ** 6, \"output\": 0.40 / 10 ** 6},\n",
    "    \"gpt-4o\": {\"input\": 2.5 / 10 ** 6, \"cached_input\": 1.25 / 10 ** 6, \"output\": 10 / 10 ** 6},\n",
    "    \"gpt-4o-2024-11-20\": {\"input\": 2.5 / 10 ** 6, \"cached_input\": 1.25 / 10 ** 6, \"output\": 10 / 10 ** 6},\n",
    "    \"gpt-4o-2024-08-06\": {\"input\": 2.5 / 10 ** 6, \"cached_input\": 1.25 / 10 ** 6, \"output\": 10 / 10 ** 6},\n",
    "    \"gpt-4o-mini\": {\"input\": 0.15 / 10 ** 6, \"cached_input\": 0.075 / 10 ** 6, \"output\": 0.6 / 10 ** 6},\n",
    "    \"text-embedding-3-small\": {\"input\": 0.02 / 10 ** 6},\n",
    "    \"text-embedding-3-large\": {\"input\": 0.13 / 10 ** 6},\n",
    "    \"text-embedding-ada-002\": {\"input\": 0.1 / 10 ** 6}\n",
    "}\n",
    "\n",
    "# Pydantic Models for VC Analysis\n",
    "class Collection(BaseModel):\n",
    "    id: str\n",
    "    name: str\n",
    "\n",
    "class DevOpsCollectionsResponse(BaseModel):\n",
    "    collections: List[Collection]\n",
    "\n",
    "class READMEAnalysis(BaseModel):\n",
    "    clarity_score: int  # Score from 1-5 on how clear the README is\n",
    "    problem_solved: str  # A summary of the problem the repo solves\n",
    "    time_to_wow_estimate: str  # e.g., \"Minutes\", \"Hours\", \"Days\"\n",
    "    is_painkiller: bool  # Does it solve a common, frustrating problem?\n",
    "\n",
    "class SentimentAnalysis(BaseModel):\n",
    "    overall_sentiment: str  # e.g., \"Positive\", \"Neutral\", \"Mixed\", \"Negative\"\n",
    "    sentiment_score: int  # Score from 1-5\n",
    "    key_quotes: List[str]  # Positive or insightful quotes from discussions\n",
    "    common_criticisms: List[str]  # Common complaints or concerns\n",
    "\n",
    "class DevOpsClassification(BaseModel):\n",
    "    is_devops_related: bool  # True if this repository is DevOps/infrastructure related\n",
    "    confidence_score: int  # Score from 1-5, how confident the classification is\n",
    "    reasoning: str  # Brief explanation of why it's classified as DevOps or not\n",
    "    devops_categories: List[str]  # Categories like \"CI/CD\", \"Container\", \"Monitoring\", etc.\n",
    "\n",
    "class FinalAnalysis(BaseModel):\n",
    "    repo_name: str\n",
    "    oss_insight_data: dict\n",
    "    readme_analysis: Optional[READMEAnalysis] = None\n",
    "    sentiment_analysis: Optional[SentimentAnalysis] = None\n",
    "    score: Optional[float] = None  # Calculated final score\n",
    "\n",
    "print(\"‚úÖ VC Sourcing Pipeline initialized successfully\")\n",
    "print(f\"üîë API clients configured for OpenAI, Exa, and GitHub\")\n",
    "print(f\"üìä Ready to analyze DevOps repositories across {len(PERIOD_VALUES)} time periods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5e1fdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost Tracking for LLM Usage\n",
    "class ModelUsageAsync:\n",
    "    \"\"\"Track LLM API costs throughout the VC analysis pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_costs=LLM_MODEL_COSTS, web_search_costs=None):\n",
    "        self.token_usage = {}\n",
    "        self.web_search_usage = {}\n",
    "        self.model_costs = model_costs\n",
    "        self.web_search_costs = web_search_costs\n",
    "        self.logs = []\n",
    "        self.lock = asyncio.Lock()\n",
    "\n",
    "    async def add_tokens(\n",
    "        self, \n",
    "        model: str, \n",
    "        input_tokens: int = 0, \n",
    "        output_tokens: int = 0, \n",
    "        cached_tokens: int = 0, \n",
    "        reasoning_tokens: int = 0, \n",
    "        label: Optional[str] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Add token usage for cost tracking.\"\"\"\n",
    "        async with self.lock:\n",
    "            if model not in self.token_usage:\n",
    "                self.token_usage[model] = {\"input\": 0, \"output\": 0, \"cached\": 0, \"reasoning\": 0}\n",
    "            \n",
    "            self.token_usage[model][\"input\"] += input_tokens\n",
    "            self.token_usage[model][\"output\"] += output_tokens\n",
    "            self.token_usage[model][\"cached\"] += cached_tokens\n",
    "            self.token_usage[model][\"reasoning\"] += reasoning_tokens\n",
    "            \n",
    "            if label is not None:\n",
    "                self.logs.append({\n",
    "                    \"model\": model, \"input_tokens\": input_tokens, \"output_tokens\": output_tokens,\n",
    "                    \"cached_tokens\": cached_tokens, \"reasoning_tokens\": reasoning_tokens, \"label\": label\n",
    "                })\n",
    "\n",
    "    async def add_web_search_usage(self, model: str, search_context_size: str) -> None:\n",
    "        \"\"\"Add web search usage for cost tracking.\"\"\"\n",
    "        if self.web_search_costs is not None:\n",
    "            if model not in self.web_search_usage:\n",
    "                self.web_search_usage[model] = {}\n",
    "            if search_context_size not in self.web_search_usage[model]:\n",
    "                self.web_search_usage[model][search_context_size] = 0\n",
    "            self.web_search_usage[model][search_context_size] += 1\n",
    "        \n",
    "    async def get_cost(self) -> float:\n",
    "        \"\"\"Calculate total cost of LLM usage.\"\"\"\n",
    "        async with self.lock:\n",
    "            for model in self.token_usage:\n",
    "                if model not in self.model_costs:\n",
    "                    raise ValueError(f\"Model {model} is not supported.\")\n",
    "            \n",
    "            cost_of_input_output = sum([\n",
    "                (self.token_usage[model][\"input\"] - self.token_usage[model][\"cached\"]) * self.model_costs[model].get(\"input\", 0) +\n",
    "                self.token_usage[model][\"output\"] * self.model_costs[model].get(\"output\", 0) +\n",
    "                self.token_usage[model][\"cached\"] * self.model_costs[model].get(\"cached_input\", 0)\n",
    "                for model in self.token_usage \n",
    "            ]) \n",
    "\n",
    "            cost_of_web_searches = 0\n",
    "            if self.web_search_costs is not None:\n",
    "                for model in self.web_search_usage:\n",
    "                    for search_context_size in self.web_search_usage[model]:\n",
    "                        cost_of_web_searches += self.web_search_usage[model][search_context_size] * self.web_search_costs[model].get(search_context_size, 0)\n",
    "\n",
    "            return cost_of_input_output + cost_of_web_searches\n",
    "\n",
    "    async def get_tokens_used(self) -> int:\n",
    "        \"\"\"Get total number of tokens used.\"\"\"\n",
    "        async with self.lock:\n",
    "            return sum([\n",
    "                self.token_usage[model][\"input\"] + self.token_usage[model][\"output\"]\n",
    "                for model in self.token_usage\n",
    "            ])\n",
    "\n",
    "    async def get_web_searches_performed(self) -> int:\n",
    "        \"\"\"Get total number of web searches performed.\"\"\"\n",
    "        async with self.lock:\n",
    "            total_searches = 0\n",
    "            for model in self.web_search_usage:\n",
    "                for search_context_size in self.web_search_usage[model]:\n",
    "                    total_searches += self.web_search_usage[model][search_context_size]\n",
    "            return total_searches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b485e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API Helper with Structured Output\n",
    "async def call_openai_structured(\n",
    "    openai_client: AsyncOpenAI,\n",
    "    model: str,\n",
    "    llm_usage: Optional[ModelUsageAsync] = None,\n",
    "    llm_usage_label: Optional[str] = None,\n",
    "    max_retries: int = 3,\n",
    "    base_delay: int = 5,\n",
    "    completion_timeout: int = 90,\n",
    "    web_search_bool: bool = False,\n",
    "    web_search_context_size: str = \"high\",\n",
    "    **kwargs\n",
    ") -> Optional[ChatCompletion]:\n",
    "    \"\"\"Call OpenAI API with structured output and cost tracking.\"\"\"\n",
    "    \n",
    "    # Handle model-specific parameters\n",
    "    if model in REASONING_MODELS:\n",
    "        kwargs.pop(\"temperature\", None)  # Reasoning models don't support temperature\n",
    "    else:\n",
    "        kwargs.pop(\"reasoning_effort\", None)  # Only supported by reasoning models\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            completion = await asyncio.wait_for(\n",
    "                openai_client.chat.completions.parse(model=model, **kwargs),\n",
    "                timeout=completion_timeout\n",
    "            )\n",
    "            \n",
    "            # Track usage and costs\n",
    "            if llm_usage is not None:\n",
    "                await llm_usage.add_tokens(\n",
    "                    model=model,\n",
    "                    input_tokens=completion.usage.prompt_tokens,\n",
    "                    output_tokens=completion.usage.completion_tokens,\n",
    "                    cached_tokens=completion.usage.prompt_tokens_details.cached_tokens,\n",
    "                    reasoning_tokens=completion.usage.completion_tokens_details.reasoning_tokens,\n",
    "                    label=llm_usage_label\n",
    "                )\n",
    "                \n",
    "                if web_search_bool:\n",
    "                    await llm_usage.add_web_search_usage(model, web_search_context_size)\n",
    "            \n",
    "            return completion\n",
    "            \n",
    "        except asyncio.TimeoutError:\n",
    "            print(f\"üîÑ API timeout for {llm_usage_label or 'request'}, retrying...\")\n",
    "            await asyncio.sleep(base_delay)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå API error for {llm_usage_label or 'request'}: {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                break\n",
    "            await asyncio.sleep(base_delay)\n",
    "\n",
    "    print(f\"‚ùå Max retries exceeded for {llm_usage_label or 'request'}\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8d532a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OSS Insight API Helper\n",
    "async def make_oss_insight_request(\n",
    "    session: aiohttp.ClientSession,\n",
    "    url: str,\n",
    "    params: dict\n",
    ") -> Optional[dict]:\n",
    "    \"\"\"Make requests to OSS Insight API with error handling.\"\"\"\n",
    "    try:\n",
    "        async with session.get(url, params=params) as response:\n",
    "            response.raise_for_status()\n",
    "            data = await response.json()\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå OSS Insight API error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9028c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DevOps Collections Identification\n",
    "GET_DEV_OPS_COLLECTIONS_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that identifies DevOps collections from a list of collections.\n",
    "\"\"\"\n",
    "\n",
    "GET_DEV_OPS_COLLECTIONS_USER_PROMPT = \"\"\"\n",
    "Task: Identify the DevOps collections from the list of collections.\n",
    "Return a structured response with collections related to DevOps, CI/CD, infrastructure, monitoring, deployment, or developer tools.\n",
    "\n",
    "List of collections:\n",
    "{context}\n",
    "\n",
    "Your answer must:\n",
    "1. Only contain collections from the provided list\n",
    "2. Return collections in the specified format with both id and name fields\n",
    "3. Be inclusive of any tools that developers would use for infrastructure, automation, or deployment\n",
    "\"\"\"\n",
    "\n",
    "async def get_dev_ops_collections(\n",
    "    openai_client: AsyncOpenAI,\n",
    "    collections_data: dict,\n",
    "    llm_usage: ModelUsageAsync\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"Identify DevOps collections using LLM analysis.\"\"\"\n",
    "    collections_rows = collections_data.get(\"data\", {}).get(\"rows\", [])\n",
    "    all_collections = [{row[\"id\"]: row[\"name\"]} for row in collections_rows]\n",
    "    \n",
    "    print(f\"üîç Analyzing {len(all_collections)} collections to identify DevOps-related ones...\")\n",
    "    \n",
    "    context = str(all_collections)\n",
    "    messages = [\n",
    "        {\"role\": \"developer\", \"content\": GET_DEV_OPS_COLLECTIONS_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": GET_DEV_OPS_COLLECTIONS_USER_PROMPT.format(context=context)}\n",
    "    ]\n",
    "\n",
    "    response = await call_openai_structured(\n",
    "        openai_client=openai_client,\n",
    "        model=\"o4-mini\",\n",
    "        messages=messages,\n",
    "        response_format=DevOpsCollectionsResponse,\n",
    "        llm_usage=llm_usage,\n",
    "        llm_usage_label=\"dev_ops_collections_identification\",\n",
    "        reasoning_effort=\"high\"\n",
    "    )\n",
    "\n",
    "    if not response or not response.choices[0].message.parsed:\n",
    "        print(\"‚ùå Failed to identify DevOps collections\")\n",
    "        return []\n",
    "    \n",
    "    parsed_response = response.choices[0].message.parsed\n",
    "    dev_ops_collections = [{c.id: c.name} for c in parsed_response.collections]\n",
    "    \n",
    "    print(f\"‚úÖ Identified {len(dev_ops_collections)} DevOps collections\")\n",
    "    return dev_ops_collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74d96b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Phase 0: Fetching trending repositories data...\n",
      "üìÖ Fetching trending repos for past_24_hours...\n",
      "‚úÖ Found 100 trending repositories for past_24_hours\n",
      "üìÖ Fetching trending repos for past_week...\n",
      "‚úÖ Found 100 trending repositories for past_week\n",
      "üìÖ Fetching trending repos for past_month...\n",
      "‚úÖ Found 100 trending repositories for past_month\n",
      "üìÖ Fetching trending repos for past_3_months...\n",
      "‚úÖ Found 100 trending repositories for past_3_months\n",
      "\n",
      "üìä Total repositories fetched: 400 across 4 periods\n"
     ]
    }
   ],
   "source": [
    "# Fetch Trending Repositories Data\n",
    "print(\"üöÄ Phase 0: Fetching trending repositories data...\")\n",
    "\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    period_trending_repos = {}\n",
    "    for period in PERIOD_VALUES:\n",
    "        print(f\"üìÖ Fetching trending repos for {period}...\")\n",
    "        trending_data = await make_oss_insight_request(\n",
    "            session=session,\n",
    "            url=TRENDING_REPOS_URL,\n",
    "            params={\"period\": period}\n",
    "        )\n",
    "        if trending_data:\n",
    "            period_trending_repos[period] = trending_data\n",
    "            repo_count = len(trending_data.get(\"data\", {}).get(\"rows\", []))\n",
    "            print(f\"‚úÖ Found {repo_count} trending repositories for {period}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to fetch data for {period}\")\n",
    "\n",
    "total_repos = sum(len(data.get(\"data\", {}).get(\"rows\", [])) for data in period_trending_repos.values())\n",
    "print(f\"\\nüìä Total repositories fetched: {total_repos} across {len(period_trending_repos)} periods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f283f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Fetching and analyzing repository collections...\n",
      "‚úÖ Found 95 total collections\n",
      "üîç Analyzing 95 collections to identify DevOps-related ones...\n",
      "‚úÖ Identified 27 DevOps collections\n",
      "üìã DevOps collections identified:\n",
      "  ‚Ä¢ Static Site Generator (ID: 1)\n",
      "  ‚Ä¢ Low Code Development Tool (ID: 10003)\n",
      "  ‚Ä¢ Github Alternative (ID: 10009)\n",
      "  ‚Ä¢ Text Editor (ID: 10015)\n",
      "  ‚Ä¢ Chaos Engineering (ID: 10017)\n",
      "  ‚Ä¢ APM Tool (ID: 10018)\n",
      "  ‚Ä¢ CICD (ID: 10020)\n",
      "  ‚Ä¢ API Gateway (ID: 10021)\n",
      "  ‚Ä¢ Distributed File Storage (ID: 10025)\n",
      "  ‚Ä¢ Testing Tools (ID: 10027)\n",
      "  ... and 17 more\n",
      "üí∞ DevOps identification cost: $0.0268 (7,077 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Fetch Collections and Identify DevOps Collections\n",
    "print(\"üîç Fetching and analyzing repository collections...\")\n",
    "\n",
    "# Initialize cost tracking for DevOps collections identification\n",
    "dev_ops_collections_usage = ModelUsageAsync()\n",
    "\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    collections_data = await make_oss_insight_request(\n",
    "        session=session,\n",
    "        url=COLLECTIONS_URL,\n",
    "        params={}\n",
    "    )\n",
    "\n",
    "if collections_data:\n",
    "    total_collections = len(collections_data.get(\"data\", {}).get(\"rows\", []))\n",
    "    print(f\"‚úÖ Found {total_collections} total collections\")\n",
    "    \n",
    "    # Identify DevOps collections using LLM\n",
    "    dev_ops_collections = await get_dev_ops_collections(\n",
    "        openai_client=openai_client,\n",
    "        collections_data=collections_data,\n",
    "        llm_usage=dev_ops_collections_usage\n",
    "    )\n",
    "    \n",
    "    if dev_ops_collections:\n",
    "        print(f\"üìã DevOps collections identified:\")\n",
    "        for collection in dev_ops_collections[:10]:  # Show first 10\n",
    "            for id_key, name in collection.items():\n",
    "                print(f\"  ‚Ä¢ {name} (ID: {id_key})\")\n",
    "        if len(dev_ops_collections) > 10:\n",
    "            print(f\"  ... and {len(dev_ops_collections) - 10} more\")\n",
    "    \n",
    "    # Display cost information\n",
    "    cost = await dev_ops_collections_usage.get_cost()\n",
    "    tokens = await dev_ops_collections_usage.get_tokens_used()\n",
    "    print(f\"üí∞ DevOps identification cost: ${cost:.4f} ({tokens:,} tokens)\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to fetch collections data\")\n",
    "    dev_ops_collections = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0e74e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Data Collection Phase Complete\n",
      "==================================================\n",
      "‚úÖ Trending Repositories: 4 time periods\n",
      "   ‚Ä¢ past_24_hours: 100 repositories\n",
      "   ‚Ä¢ past_week: 100 repositories\n",
      "   ‚Ä¢ past_month: 100 repositories\n",
      "   ‚Ä¢ past_3_months: 100 repositories\n",
      "‚úÖ DevOps Collections: 27 identified\n",
      "\n",
      "üéØ Ready to proceed with VC sourcing pipeline!\n",
      "üìã Data available for analysis:\n",
      "   ‚Ä¢ Trending repository data across multiple time periods\n",
      "   ‚Ä¢ DevOps-specific collection classifications\n",
      "   ‚Ä¢ API clients configured for GitHub, Exa, and OpenAI\n"
     ]
    }
   ],
   "source": [
    "# Data Collection Summary\n",
    "print(\"üìä Data Collection Phase Complete\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Summary of collected data\n",
    "if 'period_trending_repos' in globals():\n",
    "    print(f\"‚úÖ Trending Repositories: {len(period_trending_repos)} time periods\")\n",
    "    for period, data in period_trending_repos.items():\n",
    "        if data:\n",
    "            count = len(data.get(\"data\", {}).get(\"rows\", []))\n",
    "            print(f\"   ‚Ä¢ {period}: {count} repositories\")\n",
    "\n",
    "if 'dev_ops_collections' in globals():\n",
    "    print(f\"‚úÖ DevOps Collections: {len(dev_ops_collections)} identified\")\n",
    "    \n",
    "print(\"\\nüéØ Ready to proceed with VC sourcing pipeline!\")\n",
    "print(\"üìã Data available for analysis:\")\n",
    "print(\"   ‚Ä¢ Trending repository data across multiple time periods\")\n",
    "print(\"   ‚Ä¢ DevOps-specific collection classifications\")\n",
    "print(\"   ‚Ä¢ API clients configured for GitHub, Exa, and OpenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b35fd5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM-based DevOps classification function loaded\n"
     ]
    }
   ],
   "source": [
    "# LLM-Based DevOps Repository Classification\n",
    "CLASSIFY_DEVOPS_REPO_SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert VC analyst specializing in DevOps and infrastructure tooling. Your task is to classify whether a GitHub repository is related to DevOps, infrastructure, or developer productivity tools.\n",
    "\"\"\"\n",
    "\n",
    "CLASSIFY_DEVOPS_REPO_USER_PROMPT = \"\"\"\n",
    "Repository: {repo_name}\n",
    "Description: {description}\n",
    "\n",
    "Based on the repository name and description, determine if this is a DevOps-related project.\n",
    "\n",
    "DevOps categories include:\n",
    "- CI/CD and deployment pipelines\n",
    "- Container technologies (Docker, Kubernetes, etc.)\n",
    "- Infrastructure as Code (Terraform, Ansible, etc.)\n",
    "- Monitoring and observability tools\n",
    "- Cloud platforms and services\n",
    "- Developer productivity tools\n",
    "- Security and compliance tools\n",
    "- Database and data infrastructure\n",
    "- API gateways and service mesh\n",
    "- Configuration management\n",
    "- Build and package management tools\n",
    "\n",
    "Analyze this repository and provide:\n",
    "1. **is_devops_related**: True if it's DevOps/infrastructure related\n",
    "2. **confidence_score**: Your confidence (1-5, where 5 is very confident)\n",
    "3. **reasoning**: Brief explanation of your classification\n",
    "4. **devops_categories**: List of relevant DevOps categories (empty list if not DevOps)\n",
    "\"\"\"\n",
    "\n",
    "async def classify_repository_with_llm(\n",
    "    openai_client: AsyncOpenAI,\n",
    "    repo_name: str,\n",
    "    description: str,\n",
    "    llm_usage: ModelUsageAsync\n",
    ") -> Optional[DevOpsClassification]:\n",
    "    \"\"\"Classify a repository as DevOps-related using LLM analysis.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"developer\", \"content\": CLASSIFY_DEVOPS_REPO_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": CLASSIFY_DEVOPS_REPO_USER_PROMPT.format(\n",
    "            repo_name=repo_name,\n",
    "            description=description or \"No description available\"\n",
    "        )}\n",
    "    ]\n",
    "\n",
    "    response = await call_openai_structured(\n",
    "        openai_client=openai_client,\n",
    "        model=\"o4-mini\",\n",
    "        messages=messages,\n",
    "        response_format=DevOpsClassification,\n",
    "        llm_usage=llm_usage,\n",
    "        llm_usage_label=f\"devops_classification_{repo_name}\",\n",
    "        reasoning_effort=\"medium\"\n",
    "    )\n",
    "\n",
    "    if response and response.choices:\n",
    "        return response.choices[0].message.parsed\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"‚úÖ LLM-based DevOps classification function loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "589ef244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions for VC Analysis\n",
    "def truncate_content_for_llm(content: str, max_chars: int = 15000) -> str:\n",
    "    \"\"\"Truncate content to fit within LLM context limits.\"\"\"\n",
    "    return content[:max_chars] if len(content) > max_chars else content\n",
    "\n",
    "# Repository Filtering\n",
    "def filter_devops_repositories(\n",
    "    period_trending_repos: Dict[str, dict],\n",
    "    dev_ops_collections: List[Dict[str, str]]\n",
    ") -> Dict[str, dict]:\n",
    "    \"\"\"Filter repositories that match DevOps collections.\"\"\"\n",
    "    # Extract collection names from the DevOps collections\n",
    "    dev_ops_collection_names = {\n",
    "        name for collection_dict in dev_ops_collections \n",
    "        for name in collection_dict.values()\n",
    "    }\n",
    "    \n",
    "    potential_leads = {}\n",
    "    \n",
    "    print(f\"üîç Filtering repositories using {len(dev_ops_collection_names)} DevOps collections...\")\n",
    "    \n",
    "    for period, trending_data in period_trending_repos.items():\n",
    "        if not trending_data or not trending_data.get(\"data\"):\n",
    "            continue\n",
    "            \n",
    "        print(f\"üìÖ Processing {period}...\")\n",
    "        \n",
    "        for repo in trending_data[\"data\"][\"rows\"]:\n",
    "            repo_collections = set(repo.get(\"collection_names\", \"\").split(','))\n",
    "            repo_collections = {c.strip() for c in repo_collections if c.strip()}  # Clean up\n",
    "            \n",
    "            # Check for intersection between repo's collections and our DevOps list\n",
    "            if not dev_ops_collection_names.isdisjoint(repo_collections):\n",
    "                repo_name = repo[\"repo_name\"]\n",
    "                if repo_name not in potential_leads:\n",
    "                    potential_leads[repo_name] = repo\n",
    "                    matching_collections = repo_collections.intersection(dev_ops_collection_names)\n",
    "                    print(f\"‚úÖ Found DevOps repo: {repo_name} (matches: {matching_collections})\")\n",
    "    \n",
    "    print(f\"\\nüéØ Found {len(potential_leads)} potential DevOps leads across all periods.\")\n",
    "    return potential_leads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4134dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# README Analysis Function\n",
    "async def analyze_readme(\n",
    "    openai_client: AsyncOpenAI,\n",
    "    github_client: Github,\n",
    "    repo_name: str,\n",
    "    llm_usage: ModelUsageAsync\n",
    ") -> Optional[READMEAnalysis]:\n",
    "    \"\"\"Analyze repository README for VC investment potential.\"\"\"\n",
    "    try:\n",
    "        repo = github_client.get_repo(repo_name)\n",
    "        readme_content = repo.get_readme().decoded_content.decode(\"utf-8\")\n",
    "        print(f\"‚úÖ Fetched README for {repo_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not fetch README for {repo_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Truncate README to fit context window\n",
    "    readme_content = truncate_content_for_llm(readme_content)\n",
    "\n",
    "    system_prompt = \"You are an expert VC analyst. Your task is to analyze a project's README file to assess its investment potential based on clarity and the problem it solves.\"\n",
    "    user_prompt = f\"\"\"\n",
    "    Analyze the following README.md content for the repository '{repo_name}'.\n",
    "    \n",
    "    README Content:\n",
    "    ---\n",
    "    {readme_content}\n",
    "    ---\n",
    "    \n",
    "    Based on the content, please provide the following analysis:\n",
    "    1. **Clarity Score (1-5):** How clearly and concisely does it state the problem and solution? 5 is extremely clear.\n",
    "    2. **Problem Solved:** In one sentence, what specific problem does this project solve?\n",
    "    3. **Time to 'Wow' Estimate:** Based on the 'Getting Started' guide, estimate the time for a developer to get a valuable outcome (e.g., 'Minutes', 'Hours', 'Days').\n",
    "    4. **Is Painkiller:** Does this project seem to solve an acute, frustrating 'painkiller' problem, or is it a 'vitamin' (nice-to-have)?\n",
    "    \"\"\"\n",
    "    \n",
    "    response = await call_openai_structured(\n",
    "        openai_client=openai_client,\n",
    "        model=\"o4-mini\",\n",
    "        messages=[{\"role\": \"developer\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}],\n",
    "        response_format=READMEAnalysis,\n",
    "        llm_usage=llm_usage,\n",
    "        llm_usage_label=f\"readme_analysis_{repo_name}\",\n",
    "        reasoning_effort=\"high\"\n",
    "    )\n",
    "\n",
    "    if response and response.choices:\n",
    "        print(f\"‚úÖ README analysis completed for {repo_name}\")\n",
    "        return response.choices[0].message.parsed\n",
    "    \n",
    "    print(f\"‚ùå README analysis failed for {repo_name}\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "528fda45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Community Sentiment Analysis Function\n",
    "async def analyze_community_sentiment(\n",
    "    openai_client: AsyncOpenAI,\n",
    "    exa_client: Exa,\n",
    "    repo_name: str,\n",
    "    llm_usage: ModelUsageAsync\n",
    ") -> Optional[SentimentAnalysis]:\n",
    "    \"\"\"Analyze community sentiment using Exa search and LLM analysis.\"\"\"\n",
    "    # Use Exa to find relevant discussions\n",
    "    query = f\"discussions and reviews of the GitHub repository '{repo_name}'\"\n",
    "    try:\n",
    "        search_response = exa_client.search_and_contents(\n",
    "            query,\n",
    "            num_results=5,\n",
    "            include_domains=[\"news.ycombinator.com\", \"reddit.com\"],\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        if not search_response.results:\n",
    "            print(f\"‚ö†Ô∏è No Exa search results found for {repo_name}\")\n",
    "            return None\n",
    "            \n",
    "        # Format the context for the LLM\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"URL: {res.url}\\nContent: {res.text}\" \n",
    "            for res in search_response.results\n",
    "        ])\n",
    "        print(f\"‚úÖ Found {len(search_response.results)} community discussions for {repo_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exa search failed for {repo_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    system_prompt = \"You are a VC analyst specializing in developer sentiment. Analyze the following search results from Hacker News and Reddit to gauge community perception of a software project.\"\n",
    "    user_prompt = f\"\"\"\n",
    "    Project: {repo_name}\n",
    "\n",
    "    Discussion Threads:\n",
    "    ---\n",
    "    {context}\n",
    "    ---\n",
    "\n",
    "    Based on these discussions, please provide a sentiment analysis:\n",
    "    1. **Overall Sentiment:** What is the general feeling (Positive, Neutral, Mixed, Negative)?\n",
    "    2. **Sentiment Score (1-5):** How excited is the community? 5 is viral excitement.\n",
    "    3. **Key Quotes:** List up to 3 direct quotes that capture the essence of the community's praise or key insights.\n",
    "    4. **Common Criticisms:** List up to 2 common criticisms or concerns raised by the community.\n",
    "    \"\"\"\n",
    "\n",
    "    response = await call_openai_structured(\n",
    "        openai_client=openai_client,\n",
    "        model=\"o4-mini\",\n",
    "        messages=[{\"role\": \"developer\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}],\n",
    "        response_format=SentimentAnalysis,\n",
    "        llm_usage=llm_usage,\n",
    "        llm_usage_label=f\"sentiment_analysis_{repo_name}\",\n",
    "        reasoning_effort=\"high\"\n",
    "    )\n",
    "    \n",
    "    if response and response.choices:\n",
    "        print(f\"‚úÖ Sentiment analysis completed for {repo_name}\")\n",
    "        return response.choices[0].message.parsed\n",
    "    \n",
    "    print(f\"‚ùå Sentiment analysis failed for {repo_name}\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f8246a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring System and Pipeline Orchestration\n",
    "def calculate_final_score(analysis: FinalAnalysis) -> float:\n",
    "    \"\"\"Calculate final investment score based on multiple factors.\"\"\"\n",
    "    score = 0.0\n",
    "    \n",
    "    # 1. Popularity Score (from OSS Insight total_score)\n",
    "    total_score = float(analysis.oss_insight_data.get(\"total_score\", 0))\n",
    "    score += min(total_score / 1000, 5.0)  # Normalize to max 5 points\n",
    "    \n",
    "    # 2. README Quality Score\n",
    "    if analysis.readme_analysis:\n",
    "        score += analysis.readme_analysis.clarity_score\n",
    "        if analysis.readme_analysis.is_painkiller:\n",
    "            score += 2.0  # Bonus for solving painful problems\n",
    "        \n",
    "        # Time to wow bonus\n",
    "        time_to_wow = analysis.readme_analysis.time_to_wow_estimate.lower()\n",
    "        if \"minute\" in time_to_wow:\n",
    "            score += 2.0\n",
    "        elif \"hour\" in time_to_wow:\n",
    "            score += 1.0\n",
    "    \n",
    "    # 3. Community Sentiment Score\n",
    "    if analysis.sentiment_analysis:\n",
    "        score += analysis.sentiment_analysis.sentiment_score\n",
    "        \n",
    "        # Bonus for positive sentiment\n",
    "        if analysis.sentiment_analysis.overall_sentiment.lower() == \"positive\":\n",
    "            score += 1.0\n",
    "    \n",
    "    return round(score, 2)\n",
    "\n",
    "# Repository Analysis Pipeline\n",
    "async def analyze_repository_pipeline(\n",
    "    repo_name: str,\n",
    "    repo_data: dict,\n",
    "    openai_client: AsyncOpenAI,\n",
    "    github_client: Github,\n",
    "    exa_client: Exa,\n",
    "    llm_usage: ModelUsageAsync\n",
    ") -> FinalAnalysis:\n",
    "    \"\"\"Complete analysis pipeline for a single repository.\"\"\"\n",
    "    print(f\"üîç Analyzing {repo_name}...\")\n",
    "    \n",
    "    # Parallel analysis where possible\n",
    "    readme_task = analyze_readme(openai_client, github_client, repo_name, llm_usage)\n",
    "    sentiment_task = analyze_community_sentiment(openai_client, exa_client, repo_name, llm_usage)\n",
    "    \n",
    "    readme_analysis, sentiment_analysis = await asyncio.gather(\n",
    "        readme_task,\n",
    "        sentiment_task,\n",
    "        return_exceptions=True\n",
    "    )\n",
    "    \n",
    "    # Handle exceptions\n",
    "    if isinstance(readme_analysis, Exception):\n",
    "        print(f\"‚ö†Ô∏è README analysis failed for {repo_name}: {readme_analysis}\")\n",
    "        readme_analysis = None\n",
    "    \n",
    "    if isinstance(sentiment_analysis, Exception):\n",
    "        print(f\"‚ö†Ô∏è Sentiment analysis failed for {repo_name}: {sentiment_analysis}\")\n",
    "        sentiment_analysis = None\n",
    "    \n",
    "    # Create final analysis\n",
    "    analysis = FinalAnalysis(\n",
    "        repo_name=repo_name,\n",
    "        oss_insight_data=repo_data,\n",
    "        readme_analysis=readme_analysis,\n",
    "        sentiment_analysis=sentiment_analysis\n",
    "    )\n",
    "    \n",
    "    # Calculate score\n",
    "    analysis.score = calculate_final_score(analysis)\n",
    "    \n",
    "    return analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70d6b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reporting Functions\n",
    "def print_analysis_summary(analysis: FinalAnalysis) -> None:\n",
    "    \"\"\"Print formatted analysis summary.\"\"\"\n",
    "    print(f\"\\n## {analysis.repo_name} | üìä Score: {analysis.score}\")\n",
    "    print(f\"üìà Total Score: {analysis.oss_insight_data.get('total_score')} | ‚≠ê Stars: {analysis.oss_insight_data.get('stars')}\")\n",
    "    print(f\"üìù Description: {analysis.oss_insight_data.get('description')}\")\n",
    "    \n",
    "    if analysis.readme_analysis:\n",
    "        print(f\"üìñ Problem Solved: {analysis.readme_analysis.problem_solved}\")\n",
    "        print(f\"üéØ Clarity Score: {analysis.readme_analysis.clarity_score}/5\")\n",
    "        print(f\"‚ö° Time to Wow: {analysis.readme_analysis.time_to_wow_estimate}\")\n",
    "        print(f\"üíä Is Painkiller: {'Yes' if analysis.readme_analysis.is_painkiller else 'No'}\")\n",
    "    \n",
    "    if analysis.sentiment_analysis:\n",
    "        print(f\"üòä Community Sentiment: {analysis.sentiment_analysis.overall_sentiment}\")\n",
    "        print(f\"üìà Sentiment Score: {analysis.sentiment_analysis.sentiment_score}/5\")\n",
    "        if analysis.sentiment_analysis.key_quotes:\n",
    "            print(f\"üí¨ Key Quotes: {analysis.sentiment_analysis.key_quotes}\")\n",
    "        if analysis.sentiment_analysis.common_criticisms:\n",
    "            print(f\"‚ö†Ô∏è Common Criticisms: {analysis.sentiment_analysis.common_criticisms}\")\n",
    "\n",
    "def print_final_report(final_results: List[FinalAnalysis], total_usage: ModelUsageAsync) -> None:\n",
    "    \"\"\"Print the final investment report.\"\"\"\n",
    "    # Sort results by score\n",
    "    sorted_results = sorted(final_results, key=lambda x: x.score or 0, reverse=True)\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"üìà FINAL VC INVESTMENT REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, result in enumerate(sorted_results, 1):\n",
    "        print(f\"\\nüèÜ RANK #{i}\")\n",
    "        print_analysis_summary(result)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä PIPELINE STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üî¢ Total Repositories Analyzed: {len(final_results)}\")\n",
    "    print(f\"‚úÖ Successful README Analyses: {sum(1 for r in final_results if r.readme_analysis)}\")\n",
    "    print(f\"üòä Successful Sentiment Analyses: {sum(1 for r in final_results if r.sentiment_analysis)}\")\n",
    "    print(f\"üí∞ Analysis Completeness: {len([r for r in final_results if r.readme_analysis and r.sentiment_analysis])}/{len(final_results)} repositories fully analyzed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7c45630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Production-ready 3-tier filtering function loaded\n"
     ]
    }
   ],
   "source": [
    "# Production-Ready 3-Tier Filtering (Type-Safe)\n",
    "async def filter_devops_repositories_production(\n",
    "    period_trending_repos: Dict[str, dict],\n",
    "    dev_ops_collections: List[Dict[str, str]],\n",
    "    openai_client: AsyncOpenAI,\n",
    "    use_llm_classification: bool = True,\n",
    "    fallback_to_keywords: bool = True,\n",
    "    llm_batch_size: int = 10\n",
    ") -> Dict[str, dict]:\n",
    "    \"\"\"Production-ready 3-tier filtering with proper error handling.\"\"\"\n",
    "    \n",
    "    # TIER 1: Collection-based filtering\n",
    "    dev_ops_collection_names = {\n",
    "        name for collection_dict in dev_ops_collections \n",
    "        for name in collection_dict.values()\n",
    "    }\n",
    "    \n",
    "    potential_leads = {}\n",
    "    collection_matches = 0\n",
    "    llm_matches = 0\n",
    "    keyword_matches = 0\n",
    "    \n",
    "    # DevOps keywords for final fallback\n",
    "    devops_keywords = {\n",
    "        'docker', 'kubernetes', 'k8s', 'terraform', 'ansible', 'jenkins', \n",
    "        'ci/cd', 'cicd', 'deployment', 'infrastructure', 'devops', 'monitoring',\n",
    "        'prometheus', 'grafana', 'elasticsearch', 'nginx', 'apache', 'redis',\n",
    "        'postgresql', 'mysql', 'microservices', 'container', 'orchestration',\n",
    "        'automation', 'pipeline', 'cloud', 'aws', 'azure', 'gcp', 'helm'\n",
    "    }\n",
    "    \n",
    "    print(f\"üîç 3-Tier Filtering Strategy: Collections ‚Üí LLM ‚Üí Keywords\")\n",
    "    print(f\"üìã Using {len(dev_ops_collection_names)} DevOps collections for Tier 1...\")\n",
    "    \n",
    "    # Collect repositories for processing\n",
    "    unmatched_repos = []\n",
    "    \n",
    "    for period, trending_data in period_trending_repos.items():\n",
    "        if not trending_data or not trending_data.get(\"data\"):\n",
    "            continue\n",
    "            \n",
    "        print(f\"üìÖ Processing {period}...\")\n",
    "        \n",
    "        for repo in trending_data[\"data\"][\"rows\"]:\n",
    "            repo_name = repo[\"repo_name\"]\n",
    "            repo_collections = set(repo.get(\"collection_names\", \"\").split(','))\n",
    "            repo_collections = {c.strip() for c in repo_collections if c.strip()}\n",
    "            \n",
    "            # TIER 1: Collection-based filtering\n",
    "            if not dev_ops_collection_names.isdisjoint(repo_collections):\n",
    "                if repo_name not in potential_leads:\n",
    "                    potential_leads[repo_name] = repo\n",
    "                    matching_collections = repo_collections.intersection(dev_ops_collection_names)\n",
    "                    print(f\"‚úÖ Tier 1 - Collection match: {repo_name}\")\n",
    "                    collection_matches += 1\n",
    "                    continue\n",
    "            \n",
    "            # Store for LLM analysis\n",
    "            if repo_name not in potential_leads:\n",
    "                unmatched_repos.append(repo)\n",
    "    \n",
    "        # TIER 2: LLM-based classification (PARALLEL for speed)\n",
    "        if use_llm_classification and unmatched_repos:\n",
    "            print(f\"\\nü§ñ Tier 2 - LLM Classification on {len(unmatched_repos)} repositories...\")\n",
    "            llm_usage = ModelUsageAsync()\n",
    "            \n",
    "            # Process in parallel batches for speed\n",
    "            total_batches = (len(unmatched_repos) + llm_batch_size - 1) // llm_batch_size\n",
    "            \n",
    "            for batch_idx in range(0, len(unmatched_repos), llm_batch_size):\n",
    "                batch = unmatched_repos[batch_idx:batch_idx + llm_batch_size]\n",
    "                batch_num = (batch_idx // llm_batch_size) + 1\n",
    "                \n",
    "                print(f\"üîÑ Processing batch {batch_num}/{total_batches} ({len(batch)} repositories)\")\n",
    "                \n",
    "                # Create parallel classification tasks\n",
    "                classification_tasks = [\n",
    "                    classify_repository_with_llm(\n",
    "                        openai_client=openai_client,\n",
    "                        repo_name=repo[\"repo_name\"],\n",
    "                        description=repo.get(\"description\", \"\"),\n",
    "                        llm_usage=llm_usage\n",
    "                    )\n",
    "                    for repo in batch if repo[\"repo_name\"] not in potential_leads\n",
    "                ]\n",
    "                \n",
    "                # Execute batch in parallel\n",
    "                try:\n",
    "                    classifications = await asyncio.gather(*classification_tasks, return_exceptions=True)\n",
    "                    \n",
    "                    # Process results with explicit type handling\n",
    "                    repos_in_batch = [repo for repo in batch if repo[\"repo_name\"] not in potential_leads]\n",
    "                    \n",
    "                    for repo, result in zip(repos_in_batch, classifications):\n",
    "                        # Handle exceptions explicitly\n",
    "                        if isinstance(result, Exception):\n",
    "                            print(f\"‚ö†Ô∏è LLM classification failed for {repo['repo_name']}: {result}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Type-safe processing: result is now Optional[DevOpsClassification]\n",
    "                        if result is not None:\n",
    "                            try:\n",
    "                                if result.is_devops_related:\n",
    "                                    repo_name = repo[\"repo_name\"]\n",
    "                                    potential_leads[repo_name] = repo\n",
    "                                    categories_str = ', '.join(result.devops_categories[:2]) if result.devops_categories else 'General DevOps'\n",
    "                                    print(f\"üß† Tier 2 - LLM match: {repo_name} (confidence: {result.confidence_score}/5)\")\n",
    "                                    llm_matches += 1\n",
    "                            except AttributeError:\n",
    "                                # Additional safety for any attribute access issues\n",
    "                                print(f\"‚ö†Ô∏è Invalid classification result for {repo['repo_name']}\")\n",
    "                                \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Batch processing failed: {e}\")\n",
    "            \n",
    "            # Show LLM costs\n",
    "            llm_cost = await llm_usage.get_cost()\n",
    "            print(f\"üí∞ LLM classification cost: ${llm_cost:.4f}\")\n",
    "    \n",
    "    # TIER 3: Keyword-based final fallback\n",
    "    if fallback_to_keywords:\n",
    "        print(f\"\\nüîç Tier 3 - Keyword-based fallback...\")\n",
    "        for repo in unmatched_repos:\n",
    "            repo_name = repo[\"repo_name\"]\n",
    "            if repo_name in potential_leads:\n",
    "                continue  # Already matched\n",
    "                \n",
    "            description = repo.get(\"description\", \"\").lower()\n",
    "            repo_name_lower = repo_name.lower()\n",
    "            \n",
    "            # Check for DevOps keywords\n",
    "            found_keywords = []\n",
    "            for keyword in devops_keywords:\n",
    "                if keyword in description or keyword in repo_name_lower:\n",
    "                    found_keywords.append(keyword)\n",
    "            \n",
    "            if found_keywords:\n",
    "                potential_leads[repo_name] = repo\n",
    "                print(f\"üéØ Tier 3 - Keyword match: {repo_name}\")\n",
    "                keyword_matches += 1\n",
    "    \n",
    "    print(f\"\\nüìä 3-Tier Filtering Results:\")\n",
    "    print(f\"   ‚Ä¢ Tier 1 (Collections): {collection_matches}\")\n",
    "    print(f\"   ‚Ä¢ Tier 2 (LLM): {llm_matches}\")\n",
    "    print(f\"   ‚Ä¢ Tier 3 (Keywords): {keyword_matches}\")\n",
    "    print(f\"   ‚Ä¢ Total unique leads: {len(potential_leads)}\")\n",
    "    \n",
    "    return potential_leads\n",
    "\n",
    "print(\"‚úÖ Production-ready 3-tier filtering function loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f39672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ VC Sourcing Pipeline functions loaded\n",
      "üìã Ready to execute pipeline - run the next cell to start analysis\n"
     ]
    }
   ],
   "source": [
    "# Main VC Sourcing Pipeline Execution with 3-Tier Filtering\n",
    "async def run_vc_sourcing_pipeline(use_3tier_filtering: bool = True):\n",
    "    \"\"\"Execute the complete VC sourcing pipeline with enhanced 3-tier filtering.\"\"\"\n",
    "    print(\"üöÄ Starting VC Sourcing Pipeline...\")\n",
    "    \n",
    "    # Phase 1: Filter DevOps repositories from trending data\n",
    "    if 'period_trending_repos' not in globals() or 'dev_ops_collections' not in globals():\n",
    "        print(\"‚ùå Missing required data. Please run the previous cells first.\")\n",
    "        return\n",
    "\n",
    "    # TODO: for demo, just use the repos in past month\n",
    "    past_month_repos = period_trending_repos[\"past_month\"]\n",
    "    test_repos = {\n",
    "        \"past_month\": past_month_repos\n",
    "    }\n",
    "    \n",
    "    if use_3tier_filtering:\n",
    "        print(\"üîç Using 3-Tier Enhanced Filtering: Collections ‚Üí LLM ‚Üí Keywords\")\n",
    "        potential_leads = await filter_devops_repositories_production(\n",
    "            period_trending_repos=test_repos,\n",
    "            dev_ops_collections=dev_ops_collections,\n",
    "            openai_client=openai_client,\n",
    "            use_llm_classification=True,\n",
    "            fallback_to_keywords=True,\n",
    "            llm_batch_size=10\n",
    "        )\n",
    "    else:\n",
    "        print(\"üîç Using Original Collection-Only Filtering\")\n",
    "        potential_leads = filter_devops_repositories(test_repos, dev_ops_collections)\n",
    "    \n",
    "    if not potential_leads:\n",
    "        print(\"‚ùå No DevOps repositories found. Pipeline ended.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize usage tracker for the entire analysis run\n",
    "    total_usage = ModelUsageAsync()\n",
    "    final_results = []\n",
    "    \n",
    "    # TODO: for demo, just analyze first 5 repos instead of the entirety of potential_leads\n",
    "    print(f\"\\nüî¨ Phase 2 & 3: Analyzing 5 repositories...\")\n",
    "    \n",
    "    # Limit analysis to first 5 repositories for demonstration\n",
    "    # Remove this limit for full analysis\n",
    "    limited_leads = dict(list(potential_leads.items())[:5])\n",
    "    print(f\"üìù Note: Analyzing first {len(limited_leads)} repositories for demonstration\")\n",
    "    \n",
    "    # Analyze each repository\n",
    "    for repo_name, repo_data in limited_leads.items():\n",
    "        try:\n",
    "            analysis_result = await analyze_repository_pipeline(\n",
    "                repo_name=repo_name,\n",
    "                repo_data=repo_data,\n",
    "                openai_client=openai_client,\n",
    "                github_client=github_client,\n",
    "                exa_client=exa_client,\n",
    "                llm_usage=total_usage\n",
    "            )\n",
    "            final_results.append(analysis_result)\n",
    "            print(f\"‚úÖ Completed analysis for {repo_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to analyze {repo_name}: {e}\")\n",
    "    \n",
    "    # Phase 4: Generate final report\n",
    "    print(f\"\\nüèÅ Phase 4: Scoring and Reporting...\")\n",
    "    print_final_report(final_results, total_usage)\n",
    "    \n",
    "    # Display cost information\n",
    "    total_cost = await total_usage.get_cost()\n",
    "    total_tokens = await total_usage.get_tokens_used()\n",
    "    print(f\"\\nüí∏ Total Analysis Cost: ${total_cost:.4f}\")\n",
    "    print(f\"üî¢ Total Tokens Used: {total_tokens:,}\")\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "print(\"‚úÖ VC Sourcing Pipeline functions loaded\")\n",
    "print(\"üìã Ready to execute pipeline - run the next cell to start analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d51b4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting VC Sourcing Pipeline...\n",
      "üîç Using 3-Tier Enhanced Filtering: Collections ‚Üí LLM ‚Üí Keywords\n",
      "üîç 3-Tier Filtering Strategy: Collections ‚Üí LLM ‚Üí Keywords\n",
      "üìã Using 27 DevOps collections for Tier 1...\n",
      "üìÖ Processing past_month...\n",
      "‚úÖ Tier 1 - Collection match: n8n-io/n8n\n",
      "\n",
      "ü§ñ Tier 2 - LLM Classification on 99 repositories...\n",
      "üîÑ Processing batch 1/10 (10 repositories)\n",
      "üîÑ Processing batch 2/10 (10 repositories)\n",
      "üß† Tier 2 - LLM match: anthropics/claude-code (confidence: 4/5)\n",
      "üß† Tier 2 - LLM match: sst/opencode (confidence: 3/5)\n",
      "üß† Tier 2 - LLM match: bytedance/trae-agent (confidence: 4/5)\n",
      "üîÑ Processing batch 3/10 (10 repositories)\n",
      "üß† Tier 2 - LLM match: getAsterisk/claudia (confidence: 3/5)\n",
      "üß† Tier 2 - LLM match: ripienaar/free-for-dev (confidence: 5/5)\n",
      "üß† Tier 2 - LLM match: microsoft/vscode-copilot-chat (confidence: 5/5)\n",
      "üß† Tier 2 - LLM match: upstash/context7 (confidence: 2/5)\n",
      "üß† Tier 2 - LLM match: NirDiamant/agents-towards-production (confidence: 3/5)\n",
      "üîÑ Processing batch 4/10 (10 repositories)\n",
      "üß† Tier 2 - LLM match: rustfs/rustfs (confidence: 4/5)\n",
      "üîÑ Processing batch 5/10 (10 repositories)\n",
      "üß† Tier 2 - LLM match: stan-smith/FossFLOW (confidence: 3/5)\n",
      "üîÑ Processing batch 6/10 (10 repositories)\n",
      "üß† Tier 2 - LLM match: dockur/macos (confidence: 5/5)\n",
      "üß† Tier 2 - LLM match: GyulyVGC/sniffnet (confidence: 4/5)\n",
      "üß† Tier 2 - LLM match: opencode-ai/opencode (confidence: 4/5)\n",
      "üîÑ Processing batch 7/10 (10 repositories)\n",
      "üß† Tier 2 - LLM match: Kilo-Org/kilocode (confidence: 4/5)\n",
      "üîÑ Processing batch 8/10 (10 repositories)\n",
      "üß† Tier 2 - LLM match: czlonkowski/n8n-mcp (confidence: 3/5)\n",
      "üß† Tier 2 - LLM match: astral-sh/uv (confidence: 4/5)\n",
      "üîÑ Processing batch 9/10 (10 repositories)\n",
      "üß† Tier 2 - LLM match: github/awesome-copilot (confidence: 4/5)\n",
      "üß† Tier 2 - LLM match: enescingoz/awesome-n8n-templates (confidence: 4/5)\n",
      "üß† Tier 2 - LLM match: MrLesk/Backlog.md (confidence: 3/5)\n",
      "üîÑ Processing batch 10/10 (9 repositories)\n",
      "üß† Tier 2 - LLM match: fosrl/pangolin (confidence: 4/5)\n",
      "üß† Tier 2 - LLM match: microsoft/playwright-mcp (confidence: 3/5)\n",
      "üí∞ LLM classification cost: $0.1554\n",
      "\n",
      "üîç Tier 3 - Keyword-based fallback...\n",
      "üéØ Tier 3 - Keyword match: LibreSpark/LibreTV\n",
      "üéØ Tier 3 - Keyword match: musistudio/claude-code-router\n",
      "üéØ Tier 3 - Keyword match: hangwin/mcp-chrome\n",
      "\n",
      "üìä 3-Tier Filtering Results:\n",
      "   ‚Ä¢ Tier 1 (Collections): 1\n",
      "   ‚Ä¢ Tier 2 (LLM): 21\n",
      "   ‚Ä¢ Tier 3 (Keywords): 3\n",
      "   ‚Ä¢ Total unique leads: 25\n",
      "\n",
      "üî¨ Phase 2 & 3: Analyzing 25 repositories...\n",
      "üìù Note: Analyzing first 5 repositories for demonstration\n",
      "üîç Analyzing n8n-io/n8n...\n",
      "‚úÖ Fetched README for n8n-io/n8n\n",
      "‚úÖ Found 5 community discussions for n8n-io/n8n\n",
      "‚úÖ README analysis completed for n8n-io/n8n\n",
      "‚úÖ Sentiment analysis completed for n8n-io/n8n\n",
      "‚úÖ Completed analysis for n8n-io/n8n\n",
      "üîç Analyzing anthropics/claude-code...\n",
      "‚úÖ Fetched README for anthropics/claude-code\n",
      "‚úÖ Found 5 community discussions for anthropics/claude-code\n",
      "‚úÖ README analysis completed for anthropics/claude-code\n",
      "‚úÖ Sentiment analysis completed for anthropics/claude-code\n",
      "‚úÖ Completed analysis for anthropics/claude-code\n",
      "üîç Analyzing sst/opencode...\n",
      "‚úÖ Fetched README for sst/opencode\n",
      "‚úÖ Found 5 community discussions for sst/opencode\n",
      "‚úÖ README analysis completed for sst/opencode\n",
      "‚úÖ Sentiment analysis completed for sst/opencode\n",
      "‚úÖ Completed analysis for sst/opencode\n",
      "üîç Analyzing bytedance/trae-agent...\n",
      "‚úÖ Fetched README for bytedance/trae-agent\n",
      "‚úÖ Found 5 community discussions for bytedance/trae-agent\n",
      "‚úÖ README analysis completed for bytedance/trae-agent\n",
      "‚úÖ Sentiment analysis completed for bytedance/trae-agent\n",
      "‚úÖ Completed analysis for bytedance/trae-agent\n",
      "üîç Analyzing getAsterisk/claudia...\n",
      "‚úÖ Fetched README for getAsterisk/claudia\n",
      "‚úÖ Found 5 community discussions for getAsterisk/claudia\n"
     ]
    }
   ],
   "source": [
    "# Execute with 3-Tier Enhanced Filtering\n",
    "await run_vc_sourcing_pipeline(use_3tier_filtering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a695df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oss_sourcer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
